{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7df9318-3492-43a9-9f19-5a589a6e57f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0951f3ec-0243-4b64-8da3-2c1494e51e29",
   "metadata": {},
   "source": [
    "## Testing recognition + detection from python - THERMAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "173c1378-756a-4b9a-a22b-e4e9de93dd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Set paths for CenterNet files\n",
    "CENTERNET_LIB_PATH = \"../Thermal/CenterNet-ThermalPose-master/CenterNet/src/lib/\"\n",
    "CENTERNET_SRC_PATH = \"../Thermal/CenterNet-ThermalPose-master/CenterNet/src/\"\n",
    "sys.path.insert(0, CENTERNET_LIB_PATH)\n",
    "sys.path.insert(1, CENTERNET_SRC_PATH)\n",
    "\n",
    "# Import libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from opts import opts\n",
    "from detectors.detector_factory import detector_factory\n",
    "\n",
    "import moviepy.editor as mpy\n",
    "\n",
    "# Supported image extensions\n",
    "image_ext = ['jpg', 'jpeg', 'png', 'webp']\n",
    "# Supported video extensions\n",
    "video_ext = ['mp4', 'mov', 'avi', 'mkv']\n",
    "# Name of time stats prints\n",
    "time_stats = ['tot', 'load', 'pre', 'net', 'dec', 'post', 'merge']\n",
    "\n",
    "# Colors for located keypoints\n",
    "colors_hp = [(255, 0, 255), (255, 0, 0), (0, 0, 255), \n",
    "\t\t\t(255, 0, 0), (0, 0, 255), (255, 0, 0), (0, 0, 255),\n",
    "\t\t\t(255, 0, 0), (0, 0, 255), (255, 0, 0), (0, 0, 255),\n",
    "\t\t\t(255, 0, 0), (0, 0, 255), (255, 0, 0), (0, 0, 255),\n",
    "\t\t\t(255, 0, 0), (0, 0, 255)]\n",
    "\n",
    "# Colors for skeleton\n",
    "ec = [(255, 0, 0), (0, 0, 255), (255, 0, 0), (0, 0, 255), \n",
    "\t\t(255, 0, 0), (0, 0, 255), (255, 0, 255),\n",
    "\t\t(255, 0, 0), (255, 0, 0), (0, 0, 255), (0, 0, 255),\n",
    "\t\t(255, 0, 0), (0, 0, 255), (255, 0, 255),\n",
    "\t\t(255, 0, 0), (255, 0, 0), (0, 0, 255), (0, 0, 255)]\n",
    "\n",
    "# Links between keypoints, for skeleton construction\n",
    "edges = [[0, 1], [0, 2], [1, 3], [2, 4], \n",
    "\t\t[3, 5], [4, 6], [5, 6], \n",
    "\t\t[5, 7], [7, 9], [6, 8], [8, 10], \n",
    "\t\t[5, 11], [6, 12], [11, 12], \n",
    "\t\t[11, 13], [13, 15], [12, 14], [14, 16]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16a41062-a3c3-454f-add7-367666358d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_coco_bbox(img,bbox, conf=1, show_txt=True):\n",
    "\t\"\"\"\n",
    "\tdraws bounding box over img\n",
    "\t-----\n",
    "\tParams\n",
    "\t-----\n",
    "\timg: np.array\n",
    "\t\tinput image\n",
    "\tbbox: list\n",
    "\t\tbounding box coordinates\n",
    "\tconf: float\n",
    "\t\tconfidence in detection\n",
    "\tshow_txt: bool\n",
    "\t\tshow text with confidence score and category over bbox\n",
    "\t------\n",
    "\tReturns\n",
    "\t\tImage with bounding box drawn over\n",
    "\t\"\"\"\n",
    "\tbbox = np.array(bbox, dtype=np.int32)\n",
    "\tc = [0,255,0] # bbox color\n",
    "\ttxt = '{}{:.1f}'.format(\"person\", conf) # text to display\n",
    "\tfont = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\tcat_size = cv2.getTextSize(txt, font, 0.5, 2)[0]\n",
    "\t# Creates rectangle over image\n",
    "\tcv2.rectangle(\n",
    "\t\timg, (bbox[0], bbox[1]), (bbox[2], bbox[3]), c, 5)\n",
    "\t# Draws text over image, if requested\n",
    "\tif show_txt:\n",
    "\t  cv2.rectangle(img,\n",
    "\t                (bbox[0], bbox[1] - cat_size[1] - 2),\n",
    "\t                (bbox[0] + cat_size[0], bbox[1] - 2), c, -1)\n",
    "\t  cv2.putText(img, txt, (bbox[0], bbox[1] - 2), \n",
    "\t              font, 0.5, (0, 0, 0), thickness=1, lineType=cv2.LINE_AA)\n",
    "\treturn img\n",
    "\n",
    "def add_coco_hp(img,points): \n",
    "\t\"\"\"\n",
    "\tdraws detected keypoints and skeleton over input image\n",
    "\t-----\n",
    "\tParams\n",
    "\t-----\n",
    "\timg: np.array\n",
    "\t\tinput image\n",
    "\tpoints: list\n",
    "\t\tkeypoint coordinates\n",
    "\t------\n",
    "\tReturns\n",
    "\t\tImage with keypoints and skeletons drawn over\n",
    "\t\"\"\"\n",
    "\tpoints = np.array(points, dtype=np.int32).reshape(17, 2)\n",
    "\t# Draws each keypoint over image\n",
    "\tfor j in range(17):\n",
    "\t\tcv2.circle(img,\n",
    "\t\t\t\t(points[j, 0], points[j, 1]), 8, colors_hp[j], -1)\n",
    "\t# Draws lines joining keypoints, to form skeletons\n",
    "\tfor j, e in enumerate(edges):\n",
    "\t\tif points[e].min() > 0:\n",
    "\t\t\tcv2.line(img, (points[e[0], 0], points[e[0], 1]),\n",
    "\t\t\t(points[e[1], 0], points[e[1], 1]), ec[j], 5,\n",
    "\t\t\tlineType=cv2.LINE_AA)\n",
    "\treturn img\n",
    "\n",
    "def draw_detection(img,results,min_confidence):\n",
    "\t\"\"\"\n",
    "\tdraws detected bounding box and keypoints over image\n",
    "\t-----\n",
    "\tParams\n",
    "\t-----\n",
    "\timg: np.array\n",
    "\t\tinput image\n",
    "\tresults: dict\n",
    "\t\tdictionary with keypoint and bounding box detections\n",
    "\tmin_confidence: float\n",
    "\t\tminimum confidence in detection for displaying\n",
    "\t------\n",
    "\tReturns\n",
    "\t\tImage with keypoints and bounding boxes drawn over\n",
    "\t\"\"\"\n",
    "\tfor bbox in results[1]:\n",
    "\t\t# Verifies if detection is over threshold\n",
    "\t\tif bbox[4] > min_confidence:\n",
    "\t\t\tret_img = add_coco_bbox(img,bbox[:4], bbox[4]) # draws bbox\n",
    "\t\t\tret_img = add_coco_hp(ret_img,bbox[5:39])         # draws kpts\n",
    "\t\t\t# ret_img = add_coco_hp(img,bbox[5:39])         # draws kpts\n",
    "\t\t# If detection is not over confidence threshold, returns original image\n",
    "\t\telse:\n",
    "\t\t\tret_img = img\n",
    "\treturn ret_img\n",
    "\n",
    "def org_detections(results,min_confidence):\n",
    "\t\"\"\"\n",
    "\treturns detection results as an structured dataframe\n",
    "\t-----\n",
    "\tParams\n",
    "\t-----\n",
    "\tresults: dict\n",
    "\t\tdictionary with keypoint and bounding box detections\n",
    "\tmin_confidence: float\n",
    "\t\tminimum confidence in detection for displaying\n",
    "\t------\n",
    "\tReturns\n",
    "\t\tDataframe with bounding box coordinates, score and keypoint locations\n",
    "\t\"\"\"\n",
    "\tcolumns = [\"topleft_bbox\",\"botright_bbox\",\"score\",\"nose\",\"left_eye\",\"right_eye\",\"left_ear\",\"right_ear\",\"left_shoulder\",\n",
    "\t\t\t   \"right_shoulder\",\"left_elbow\",\"right_elbow\",\"left_wrist\",\"right_wrist\",\"left_hip\",\"right_hip\",\"left_knee\",\n",
    "\t\t\t   \"right_knee\",\"left_ankle\",\"right_ankle\"]\n",
    "\tdf = pd.DataFrame(columns=columns)\n",
    "\tdet_idx = 0\n",
    "\tfor bbox in results[1]:\n",
    "\t\t# Only saves detections over threshold confidence\n",
    "\t\tif bbox[4] > min_confidence:\n",
    "\t\t\t# Boundix box coordinates\n",
    "\t\t\ttopleft_bbox = [(bbox[0],bbox[1])]\n",
    "\t\t\tbotright_bbox = [(bbox[2],bbox[3])]\n",
    "\t\t\t# Detection score\n",
    "\t\t\tscore = [bbox[4]]\n",
    "\t\t\t# Keypoints coordinates\n",
    "\t\t\tx_kpts = bbox[5:39:2]\n",
    "\t\t\ty_kpts = bbox[6:39:2]\n",
    "\t\t\txy_kpts = list(zip(x_kpts,y_kpts))\n",
    "\t\t\tdet = topleft_bbox + botright_bbox + score + xy_kpts\n",
    "\t\t\t# Appends detection info to dataframe\n",
    "\t\t\tdf.loc[det_idx] = det\n",
    "\t\t\tdet_idx += 1\n",
    "\treturn df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eee7bfc-b140-469f-9fb1-1e11b380e7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = '../0516_10s_12r.mp4'  # path to image/ image folders/ video.\n",
    "pause = False  # whether to pause between detections\n",
    "arch = 'dla'  # model architecture. Currently tested || dla | hourglass | hrnet ||\n",
    "min_confidence = 0.3  # minimum confidence for visualization\n",
    "show_fps = False  # show fps of detection in visualization\n",
    "output_dir = '../Thermal/CenterNet-ThermalPose-master/osd/'  # output directory for detections\n",
    "save_img = True  # store images with detections\n",
    "save_csv = False  # save csv files with detected joints and bboxes\n",
    "visualize = 0  # wheter to visualize outputs\n",
    "input_fps = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27e8bc59-1d2b-441c-b4c7-08a26db40bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fix size testing.\n",
      "training chunk_sizes: [32]\n",
      "The output will be saved to  /home/aldy/Documents/skripsi/posec3d-video-structuralize/../Thermal/CenterNet-ThermalPose-master/CenterNet/src/lib/../../exp/multi_pose/default\n",
      "heads {'hm': 1, 'wh': 2, 'hps': 34, 'reg': 2, 'hm_hp': 17, 'hp_offset': 2}\n",
      "Creating model...\n",
      "loaded ../Thermal/CenterNet-ThermalPose-master/CenterNet/models/multi_pose_dla_3x_gray_384_0frz.pth, epoch 50\n",
      "tot 0.288s |load 0.000s |pre 0.012s |net 0.181s |dec 0.089s |post 0.007s |merge 0.000s |\n",
      "tot 0.073s |load 0.000s |pre 0.010s |net 0.055s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.072s |load 0.000s |pre 0.009s |net 0.055s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.071s |load 0.000s |pre 0.009s |net 0.053s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.065s |load 0.000s |pre 0.010s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.009s |net 0.045s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.010s |net 0.045s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.010s |net 0.045s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.066s |load 0.000s |pre 0.010s |net 0.047s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.010s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.062s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.072s |load 0.000s |pre 0.009s |net 0.046s |dec 0.003s |post 0.013s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.011s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.010s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.067s |load 0.000s |pre 0.013s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.010s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.067s |load 0.000s |pre 0.009s |net 0.047s |dec 0.002s |post 0.009s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.065s |load 0.000s |pre 0.010s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.010s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.010s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.009s |net 0.047s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.066s |load 0.000s |pre 0.012s |net 0.046s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.068s |load 0.000s |pre 0.009s |net 0.046s |dec 0.003s |post 0.009s |merge 0.000s |\n",
      "tot 0.065s |load 0.000s |pre 0.011s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.062s |load 0.000s |pre 0.009s |net 0.046s |dec 0.001s |post 0.007s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.010s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.009s |net 0.047s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.010s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.010s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.067s |load 0.000s |pre 0.010s |net 0.047s |dec 0.003s |post 0.007s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.066s |load 0.000s |pre 0.010s |net 0.046s |dec 0.003s |post 0.006s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.009s |net 0.047s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.010s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.009s |net 0.046s |dec 0.001s |post 0.006s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.009s |net 0.046s |dec 0.001s |post 0.007s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.009s |net 0.046s |dec 0.001s |post 0.007s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.010s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.010s |net 0.046s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.065s |load 0.000s |pre 0.011s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.010s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.010s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.009s |net 0.047s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.011s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.010s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.062s |load 0.000s |pre 0.009s |net 0.046s |dec 0.001s |post 0.006s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.009s |net 0.046s |dec 0.001s |post 0.006s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.065s |load 0.000s |pre 0.010s |net 0.046s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.010s |net 0.046s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.009s |net 0.046s |dec 0.001s |post 0.007s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.009s |net 0.047s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.010s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.062s |load 0.000s |pre 0.009s |net 0.046s |dec 0.001s |post 0.006s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.009s |net 0.047s |dec 0.001s |post 0.007s |merge 0.000s |\n",
      "tot 0.071s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.013s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.010s |net 0.046s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.068s |load 0.000s |pre 0.009s |net 0.046s |dec 0.003s |post 0.010s |merge 0.000s |\n",
      "tot 0.069s |load 0.000s |pre 0.014s |net 0.046s |dec 0.003s |post 0.007s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.009s |net 0.047s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.010s |net 0.046s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.010s |net 0.046s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.009s |net 0.047s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.065s |load 0.000s |pre 0.011s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.009s |net 0.047s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.010s |net 0.046s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.009s |net 0.047s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.009s |net 0.047s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.064s |load 0.000s |pre 0.009s |net 0.047s |dec 0.002s |post 0.007s |merge 0.000s |\n",
      "tot 0.063s |load 0.000s |pre 0.009s |net 0.046s |dec 0.002s |post 0.006s |merge 0.000s |\n",
      "Moviepy - Building video ../Thermal/CenterNet-ThermalPose-master/osd/0516_10s_12r_dla.mp4.\n",
      "Moviepy - Writing video ../Thermal/CenterNet-ThermalPose-master/osd/0516_10s_12r_dla.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready ../Thermal/CenterNet-ThermalPose-master/osd/0516_10s_12r_dla.mp4\n"
     ]
    }
   ],
   "source": [
    "# Selects appropiate paths according to backbone selection\n",
    "if arch == 'dla':\n",
    "    # MODEL_PATH = \"../CenterNet/models/multi_pose_dla_3x_gray_384_0frz.pth\"\n",
    "    MODEL_PATH = \"../Thermal/CenterNet-ThermalPose-master/CenterNet/models/multi_pose_dla_3x_gray_384_0frz.pth\"\n",
    "    arch_name = 'dla_34'\n",
    "elif arch == 'hourglass':\n",
    "    # MODEL_PATH = \"../CenterNet/models/multi_pose_hg_3x_gray_0frz.pth\"\n",
    "    MODEL_PATH = \"../Thermal/CenterNet-ThermalPose-master/CenterNet/models/multi_pose_hg_3x_gray_0frz.pth\"\n",
    "    arch_name = arch\n",
    "elif arch =='hrnet':\n",
    "    # MODEL_PATH = \"../CenterNet/models/multi_pose_hrnet_3x_gray_finetune\"\n",
    "    MODEL_PATH = \"../Thermal/CenterNet-ThermalPose-master/CenterNet/models/multi_pose_hrnet_3x_gray_finetune\"\n",
    "    arch_name = 'hrnet32'\n",
    "# Initializes centernet options\n",
    "opt = opts().init('{} --load_model {} --arch {}'.format('multi_pose', MODEL_PATH,arch_name).split(' '))\n",
    "# Creates detector\n",
    "detector = detector_factory[opt.task](opt)\n",
    "detector.pause = False\n",
    "\n",
    "\n",
    "if demo == 'webcam' or \\\n",
    "    demo[demo.rfind('.') + 1:].lower() in video_ext:\n",
    "    # Initializes video capture for frame retrieval\n",
    "    cam = cv2.VideoCapture(0 if demo == 'webcam' else demo)\n",
    "    # In case output directory is specified, creates video writer for saving each frame into output video\n",
    "    if output_dir != '' and save_img:\n",
    "        output_video_path = '{}{}_{}.mp4'.format(output_dir,demo.split(\"/\")[-1].split(\".\")[0],arch)\n",
    "        # _, sample_img = cam.read()\n",
    "        # out = cv2.VideoWriter(output_video_path,cv2.VideoWriter_fourcc('M','J','P','G'), 16, \n",
    "        #       (sample_img.shape[0],sample_img.shape[1]))\n",
    "        # out = cv2.VideoWriter(output_video_path,cv2.VideoWriter_fourcc(*'X264'), 16, \n",
    "        #       (sample_img.shape[0],sample_img.shape[1]))\n",
    "    frame_idx = 0\n",
    "    imgs = []\n",
    "    annos = []\n",
    "    while True:\n",
    "        _, img = cam.read()        # reads frame from webcam or video\n",
    "        frame_idx += 1\n",
    "        if type(img) == type(None):\n",
    "          break\n",
    "        # print(type(img))\n",
    "        # print(img.shape)\n",
    "        ret = detector.run(img)    # runs detection over input image\n",
    "        annos.append([anno for anno in ret[\"results\"][1] if anno[4] > min_confidence])  # for ciis action classification\n",
    "        ret_img = draw_detection(img.copy(),ret[\"results\"],min_confidence) # draws detection over input image\n",
    "        # If user wants, fps can be shown over detection image\n",
    "        if show_fps:\n",
    "            cv2.putText(ret_img,'fps: {:.2f}'.format((1/ret['tot'])),(0,30), cv2.FONT_HERSHEY_SIMPLEX , 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        if visualize:\n",
    "            cv2.imshow('entrada', img) # shows input image\n",
    "            cv2.imshow('deteccion',ret_img) # shows image with detections\n",
    "        # Writes frame with detection over output video\n",
    "        if output_dir != '':\n",
    "            if save_img:\n",
    "                # out.write(ret_img)\n",
    "                imgs.append(ret_img)\n",
    "            if save_csv:\n",
    "                df_det = org_detections(ret[\"results\"],min_confidence)\n",
    "                df_det.to_csv(\"{}{}-{}_{}.csv\".format(output_dir,demo.split(\"/\")[-1].split(\".\")[0],frame_idx,arch))\n",
    "        # Prints time stats\n",
    "        time_str = ''\n",
    "        for stat in time_stats:\n",
    "            time_str = time_str + '{} {:.3f}s |'.format(stat, ret[stat])\n",
    "        print(time_str)\n",
    "        # Option for exiting program\n",
    "        if cv2.waitKey(0 if pause else 1) == 27:\n",
    "            cam.release()\n",
    "            # out.release()\n",
    "            import sys\n",
    "            sys.exit(0)\n",
    "    vid = mpy.ImageSequenceClip(imgs, fps=input_fps)\n",
    "    vid.write_videofile(output_video_path, remove_temp=True)\n",
    "    cam.release()\n",
    "    # out.release()\n",
    "\n",
    "else:\n",
    "    # If demo is image or directory with images, retrieves path for each one of them\n",
    "    if os.path.isdir(demo):\n",
    "        image_names = []\n",
    "        ls = os.listdir(demo)\n",
    "        print(demo)\n",
    "        for file_name in sorted(ls):\n",
    "            ext = file_name[file_name.rfind('.') + 1:].lower()\n",
    "            if ext in image_ext:\n",
    "                image_names.append(os.path.join(demo, file_name))\n",
    "    else:\n",
    "        image_names = [demo]\n",
    "\n",
    "    for (image_name) in image_names:\n",
    "        # Reads image\n",
    "        img = cv2.imread(image_name)\n",
    "        ret = detector.run(img)    # runs detection over image\n",
    "        annos.append([anno for anno in ret[\"results\"][1] if anno[4] > min_confidence])  # for ciis action classification\n",
    "        ret_img = draw_detection(img.copy(),ret[\"results\"],min_confidence) # draws detections over image\n",
    "        if visualize:\n",
    "            cv2.imshow('entrada', img) # shows input image\n",
    "            cv2.imshow('deteccion',ret_img) # shows image with detections\n",
    "\n",
    "        # saves output image with detections, if requested\n",
    "        if output_dir != '':\n",
    "            if save_img:\n",
    "                output_img_path = '{}{}_{}.png'.format(output_dir,image_name.split(\"/\")[-1].split(\".\")[0],arch)\n",
    "                cv2.imwrite(output_img_path,ret_img)\n",
    "            if save_csv:\n",
    "                df_det = org_detections(ret[\"results\"],min_confidence)\n",
    "                df_det.to_csv(\"{}{}_{}.csv\".format(output_dir,image_name.split(\"/\")[-1].split(\".\")[0],arch))\n",
    "        # Option for exiting program\n",
    "        if cv2.waitKey(0 if pause else 1) == 27:\n",
    "            import sys\n",
    "            sys.exit(0)\n",
    "        # Prints time stats\n",
    "        time_str = ''\n",
    "        for stat in time_stats:\n",
    "            time_str = time_str + '{} {:.3f}s |'.format(stat, ret[stat])\n",
    "        print(time_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ab352b0-5927-4ea2-9d8a-ff52faaa3e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_detections = [np.array([det[:4] for det in anno], dtype=np.float32) for anno in annos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b274f5ac-4323-4be2-8a6f-3e116173f4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_results = []\n",
    "for anno in annos:\n",
    "    pose = dict(\n",
    "        keypoints=[[list(xy_kpt) for xy_kpt in zip(det[5:39:2], det[6:39:2])] for det in anno],\n",
    "        keypoint_scores=[[0.8808185 , 0.8933128 , 0.8556601 , 0.9248522 , 0.9113513 ,\n",
    "                          0.83777976, 0.80287457, 0.89377964, 0.66614443, 0.8510387 ,\n",
    "                          0.70986986, 0.70710015, 0.64390606, 0.6184495 , 0.519892  ,\n",
    "                          0.55833685, 0.46859613] for det in anno],\n",
    "        bboxes=[det[:4] for det in anno]\n",
    "    )\n",
    "    pose_results.append(pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cc52445-850c-427d-9925-2320af1eace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "# import argparse\n",
    "import copy as cp\n",
    "import tempfile\n",
    "import warnings\n",
    "\n",
    "import cv2\n",
    "import mmcv\n",
    "import mmengine\n",
    "import numpy as np\n",
    "import torch\n",
    "from mmengine import DictAction\n",
    "from mmengine.structures import InstanceData\n",
    "\n",
    "from mmaction.apis import (detection_inference, inference_recognizer,\n",
    "                           inference_skeleton, init_recognizer, pose_inference)\n",
    "from mmaction.registry import VISUALIZERS\n",
    "from mmaction.structures import ActionDataSample\n",
    "from mmaction.utils import frame_extract\n",
    "\n",
    "from mmdet.apis import init_detector\n",
    "# try:\n",
    "#     from mmdet.apis import init_detector\n",
    "# except (ImportError, ModuleNotFoundError):\n",
    "#     warnings.warn('Failed to import `init_detector` form `mmdet.apis`. '\n",
    "#                   'These apis are required in skeleton-based applications! ')\n",
    "\n",
    "import moviepy.editor as mpy\n",
    "# try:\n",
    "#     import moviepy.editor as mpy\n",
    "# except ImportError:\n",
    "#     raise ImportError('Please install moviepy to enable output file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bba4f40d-f9a6-4da0-8764-1ec32ae12c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "FONTFACE = cv2.FONT_HERSHEY_DUPLEX\n",
    "FONTSCALE = 1.25\n",
    "FONTCOLOR = (255, 255, 255)  # BGR, white\n",
    "MSGCOLOR = (128, 128, 128)  # BGR, gray\n",
    "THICKNESS = 2  # int\n",
    "LINETYPE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4145efa8-5d3b-4b3d-a4ad-0b4c578c3cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hex2color(h):\n",
    "    \"\"\"Convert the 6-digit hex string to tuple of 3 int value (RGB)\"\"\"\n",
    "    return (int(h[:2], 16), int(h[2:4], 16), int(h[4:], 16))\n",
    "\n",
    "\n",
    "PLATEBLUE = '03045e-023e8a-0077b6-0096c7-00b4d8-48cae4'\n",
    "PLATEBLUE = PLATEBLUE.split('-')\n",
    "PLATEBLUE = [hex2color(h) for h in PLATEBLUE]\n",
    "PLATEGREEN = '004b23-006400-007200-008000-38b000-70e000'\n",
    "PLATEGREEN = PLATEGREEN.split('-')\n",
    "PLATEGREEN = [hex2color(h) for h in PLATEGREEN]\n",
    "\n",
    "def visualize(pose_config,\n",
    "              frames,\n",
    "              annotations,\n",
    "              pose_results,\n",
    "              action_result,\n",
    "              plate=PLATEBLUE,\n",
    "              max_num=5):\n",
    "    \"\"\"Visualize frames with predicted annotations.\n",
    "\n",
    "    Args:\n",
    "        frames (list[np.ndarray]): Frames for visualization, note that\n",
    "            len(frames) % len(annotations) should be 0.\n",
    "        annotations (list[list[tuple]]): The predicted spatio-temporal\n",
    "            detection results.\n",
    "        pose_results: The pose results.\n",
    "        action_result (str): The predicted action recognition results.\n",
    "        pose_model (nn.Module): The constructed pose model.\n",
    "        plate (str): The plate used for visualization. Default: PLATEBLUE.\n",
    "        max_num (int): Max number of labels to visualize for a person box.\n",
    "            Default: 5.\n",
    "\n",
    "    Returns:\n",
    "        list[np.ndarray]: Visualized frames.\n",
    "    \"\"\"\n",
    "\n",
    "    act_res = 'tidak berbahaya'\n",
    "    \n",
    "    assert max_num + 1 <= len(plate)\n",
    "    frames_ = cp.deepcopy(frames)\n",
    "    frames_ = [mmcv.imconvert(f, 'bgr', 'rgb') for f in frames_]\n",
    "    nf, na = len(frames), len(annotations)\n",
    "    assert nf % na == 0\n",
    "    nfpa = len(frames) // len(annotations)\n",
    "    anno = None\n",
    "    h, w, _ = frames[0].shape\n",
    "    scale_ratio = np.array([w, h, w, h])\n",
    "\n",
    "    # add pose results\n",
    "    for (i, d, f) in zip(range(na), pose_results, frames_):\n",
    "        anno = annotations[i]\n",
    "        if anno is None:\n",
    "            continue\n",
    "        for p, ann in enumerate(anno):\n",
    "            # for j in range(17):\n",
    "            #     if len(d['keypoints']) != 0:\n",
    "            #         print(d['keypoints'][p][j][0])\n",
    "                    # cv2.circle(f, (d['keypoints'][p][j][0] * scale_ratio, d['keypoints'][p][j][1] * scale_ratio), 3, colors_hp[j], -1)\n",
    "            frames_[i] = f\n",
    "\n",
    "    for i in range(na):\n",
    "        anno = annotations[i]\n",
    "        if anno is None:\n",
    "            continue\n",
    "        for j in range(nfpa):\n",
    "            ind = i * nfpa + j\n",
    "            frame = frames_[ind]\n",
    "\n",
    "            # add action result for whole video\n",
    "            # textsize = cv2.getTextSize(action_result, FONTFACE, FONTSCALE,\n",
    "            #                            THICKNESS)[0]\n",
    "            \n",
    "            # textwidth = textsize[0]\n",
    "            # location = (10, 14)\n",
    "            # diag0 = (location[0] + textwidth, location[1] - 14)\n",
    "            # diag1 = (location[0], location[1] + 22)\n",
    "            # cv2.rectangle(frame, diag0, diag1, (0, 119, 182), -1)\n",
    "            # bahaya = ['kekerasan bersenjata', 'kekerasan fisik', 'berbahaya']\n",
    "            # if action_result in bahaya:\n",
    "            #     cv2.putText(frame, action_result, (10, 30), FONTFACE, FONTSCALE,\n",
    "            #                 (255, 0, 0), THICKNESS, LINETYPE)\n",
    "            # else:\n",
    "            #     cv2.putText(frame, action_result, (10, 30), FONTFACE, FONTSCALE,\n",
    "            #                 FONTCOLOR, THICKNESS, LINETYPE)\n",
    "\n",
    "            # add spatio-temporal action detection results\n",
    "            for ann in anno:\n",
    "                box = ann[0]\n",
    "                label = ann[1]\n",
    "                if not len(label):\n",
    "                    continue\n",
    "                score = ann[2]\n",
    "                box = (box * scale_ratio).astype(np.int64)\n",
    "                st, ed = tuple(box[:2]), tuple(box[2:])\n",
    "                # cv2.rectangle(frame, st, ed, plate[0], 2)\n",
    "\n",
    "                for k, lb in enumerate(label):\n",
    "                    if k >= max_num:\n",
    "                        break\n",
    "                    text = abbrev(lb)\n",
    "                    text = ': '.join([text, f'{(score[k]*100):.1f}%'])\n",
    "                    location = (0 + st[0], 18 + k * 18 + st[1])\n",
    "                    textsize = cv2.getTextSize(text, FONTFACE, FONTSCALE,\n",
    "                                               THICKNESS)[0]\n",
    "                    textwidth = textsize[0]\n",
    "                    diag0 = (location[0] + textwidth, location[1] - 14)\n",
    "                    diag1 = (location[0], location[1] + 2)\n",
    "                    cv2.rectangle(frame, diag0, diag1, plate[k + 1], -1)\n",
    "                    bahaya = ['melempar', 'membidik senapan', 'membidik pistol', 'memukul', 'menendang', 'menusuk']\n",
    "                    if lb in bahaya:\n",
    "                        cv2.putText(frame, text, location, FONTFACE, FONTSCALE,\n",
    "                                    (255, 0, 0), THICKNESS, LINETYPE)\n",
    "                        # act_res = 'berbahaya'\n",
    "                    else:\n",
    "                        cv2.putText(frame, text, location, FONTFACE, FONTSCALE,\n",
    "                                    FONTCOLOR, THICKNESS, LINETYPE)\n",
    "\n",
    "                    # textsize = cv2.getTextSize(act_res, FONTFACE, FONTSCALE,\n",
    "                    #                            THICKNESS)[0]\n",
    "                    # textwidth = textsize[0]\n",
    "                    # location = (10, 14)\n",
    "                    # diag0 = (location[0] + textwidth, location[1] - 14)\n",
    "                    # diag1 = (location[0], location[1] + 22)\n",
    "                    # cv2.rectangle(frame, diag0, diag1, (0, 119, 182), -1)\n",
    "                    # if act_res == 'berbahaya':\n",
    "                    #     cv2.putText(frame, act_res, (10, 30), FONTFACE, FONTSCALE,\n",
    "                    #                 (255, 0, 0), THICKNESS, LINETYPE)\n",
    "                    # else:\n",
    "                    #     cv2.putText(frame, act_res, (10, 30), FONTFACE, FONTSCALE,\n",
    "                    #                 FONTCOLOR, THICKNESS, LINETYPE)\n",
    "\n",
    "    return frames_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df020acf-23a4-4537-b117-c802991bb870",
   "metadata": {},
   "outputs": [],
   "source": [
    "video = demo\n",
    "out_filename = output_dir + '0516_20s_out.mp4'\n",
    "\n",
    "# rgb-based spatio temporal detection config\n",
    "rgb_stdet_config =  \"mmaction2/configs/detection/slowonly/slowonly_kinetics400-pretrained-r101_8xb16-8x8x1-20e_ava21-rgb.py\"\n",
    "rgb_stdet_checkpoint = \"https://download.openmmlab.com/mmaction/detection/ava/slowonly_omnisource_pretrained_r101_8x8x1_20e_ava_rgb/slowonly_omnisource_pretrained_r101_8x8x1_20e_ava_rgb_20201217-16378594.pth\"\n",
    "\n",
    "# human detection config\n",
    "det_config = 'mmaction2/demo/demo_configs/faster-rcnn_r50_fpn_2x_coco_infer.py'\n",
    "det_checkpoint = 'http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth'\n",
    "det_score_thr = 0.9\n",
    "#det_cat_id = 0\n",
    "\n",
    "# pose estimation config\n",
    "pose_config = 'mmaction2/demo/demo_configs/td-hm_hrnet-w32_8xb64-210e_coco-256x192_infer.py'\n",
    "pose_checkpoint = 'https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth'\n",
    "\n",
    "# action classification config\n",
    "skeleton_config = \"mmaction2/configs/skeleton/posec3d/ciis_not-multi.py\"\n",
    "action_score_thr = 0.6\n",
    "# skeleton-based action recognition checkpoint\n",
    "skeleton_checkpoint = \"https://download.openmmlab.com/mmaction/skeleton/posec3d/posec3d_k400.pth\"\n",
    "# skeleton-based spatio temporal detection checkpoint\n",
    "skeleton_stdet_checkpoint = \"work_dirs/ciis_not-multi_10_best-550/best_acc_top1_epoch_550.pth\"\n",
    "\n",
    "# use skeleton-based method\n",
    "use_skeleton_stdet = True\n",
    "use_skeleton_recog = True\n",
    "\n",
    "# label_map_stdet = \"mmaction2/tools/data/ciis/label_map_no-berdiri.txt\"\n",
    "label_map_stdet = \"mmaction2/tools/data/ciis/label_map.txt\"\n",
    "label_map = \"mmaction2/tools/data/kinetics/label_map_k400.txt\"\n",
    "\n",
    "\n",
    "# rgb-based action recognition config\n",
    "rgb_config = \"configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x3-100e_kinetics400-rgb.py\"\n",
    "rgb_checkpoint = \"https://download.openmmlab.com/mmaction/recognition/tsn/tsn_r50_1x1x3_100e_kinetics400_rgb/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\"\n",
    "\n",
    "predict_stepsize = 4  # must even int, give out a spatio-temporal detection prediction per n frames\n",
    "output_stepsize = 1  # show one frame per n frames in the demo, we should have: predict_stepsize % output_stepsize == 0, speedUp/slowDown video output\n",
    "output_fps = 12  # the fps of demo video output, will speedUp/slowDown video output, must equal to (video_input_fps/output_stepsize) to get normal speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c43af11-3b1c-423e-95cf-d3dbb8c42493",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f38f52a-c759-4aa3-9081-c8535eca0007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_args():\n",
    "#     parser = argparse.ArgumentParser(description='MMAction2 demo')\n",
    "#     parser.add_argument(\n",
    "#         '--cfg-options',\n",
    "#         nargs='+',\n",
    "#         action=DictAction,\n",
    "#         default={},\n",
    "#         help='override some settings in the used config, the key-value pair '\n",
    "#         'in xxx=yyy format will be merged into config file. For example, '\n",
    "#         \"'--cfg-options model.backbone.depth=18 model.backbone.with_cp=True'\")\n",
    "#     args = parser.parse_args()\n",
    "#     return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54b10072-8152-490c-b05b-961873a676ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_label_map(file_path):\n",
    "    \"\"\"Load Label Map.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The file path of label map.\n",
    "\n",
    "    Returns:\n",
    "        dict: The label map (int -> label name).\n",
    "    \"\"\"\n",
    "    lines = open(file_path).readlines()\n",
    "    lines = [x.strip().split(': ') for x in lines]\n",
    "    return {int(x[0]): x[1] for x in lines}\n",
    "\n",
    "\n",
    "def abbrev(name):\n",
    "    \"\"\"Get the abbreviation of label name:\n",
    "\n",
    "    'take (an object) from (a person)' -> 'take ... from ...'\n",
    "    \"\"\"\n",
    "    while name.find('(') != -1:\n",
    "        st, ed = name.find('('), name.find(')')\n",
    "        name = name[:st] + '...' + name[ed + 1:]\n",
    "    return name\n",
    "\n",
    "\n",
    "def pack_result(human_detection, result, img_h, img_w):\n",
    "    \"\"\"Short summary.\n",
    "\n",
    "    Args:\n",
    "        human_detection (np.ndarray): Human detection result.\n",
    "        result (type): The predicted label of each human proposal.\n",
    "        img_h (int): The image height.\n",
    "        img_w (int): The image width.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple of human proposal, label name and label score.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        human_detection[:, 0::2] /= img_w\n",
    "        human_detection[:, 1::2] /= img_h\n",
    "    except:\n",
    "        pass\n",
    "    results = []\n",
    "    if result is None:\n",
    "        return None\n",
    "    for prop, res in zip(human_detection, result):\n",
    "        res.sort(key=lambda x: -x[1])\n",
    "        results.append(\n",
    "            (prop.data.cpu().numpy(), [x[0] for x in res], [x[1]\n",
    "                                                            for x in res]))\n",
    "    return results\n",
    "\n",
    "\n",
    "def expand_bbox(bbox, h, w, ratio=1.25):\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    center_x = (x1 + x2) // 2\n",
    "    center_y = (y1 + y2) // 2\n",
    "    width = x2 - x1\n",
    "    height = y2 - y1\n",
    "\n",
    "    square_l = max(width, height)\n",
    "    new_width = new_height = square_l * ratio\n",
    "\n",
    "    new_x1 = max(0, int(center_x - new_width / 2))\n",
    "    new_x2 = min(int(center_x + new_width / 2), w)\n",
    "    new_y1 = max(0, int(center_y - new_height / 2))\n",
    "    new_y2 = min(int(center_y + new_height / 2), h)\n",
    "    return (new_x1, new_y1, new_x2, new_y2)\n",
    "\n",
    "\n",
    "def cal_iou(box1, box2):\n",
    "    xmin1, ymin1, xmax1, ymax1 = box1\n",
    "    xmin2, ymin2, xmax2, ymax2 = box2\n",
    "\n",
    "    s1 = (xmax1 - xmin1) * (ymax1 - ymin1)\n",
    "    s2 = (xmax2 - xmin2) * (ymax2 - ymin2)\n",
    "\n",
    "    xmin = max(xmin1, xmin2)\n",
    "    ymin = max(ymin1, ymin2)\n",
    "    xmax = min(xmax1, xmax2)\n",
    "    ymax = min(ymax1, ymax2)\n",
    "\n",
    "    w = max(0, xmax - xmin)\n",
    "    h = max(0, ymax - ymin)\n",
    "    intersect = w * h\n",
    "    union = s1 + s2 - intersect\n",
    "    iou = intersect / union\n",
    "\n",
    "    return iou\n",
    "\n",
    "\n",
    "def skeleton_based_action_recognition(skeleton_config, skeleton_checkpoint, device, label_map, pose_results, h, w):\n",
    "    label_map = [x.strip() for x in open(label_map).readlines()]\n",
    "    num_class = len(label_map)\n",
    "\n",
    "    skeleton_config = mmengine.Config.fromfile(skeleton_config)\n",
    "    skeleton_config.model.cls_head.num_classes = num_class  # for K400 dataset\n",
    "\n",
    "    skeleton_model = init_recognizer(\n",
    "        skeleton_config, skeleton_checkpoint, device=device)\n",
    "    result = inference_skeleton(skeleton_model, pose_results, (h, w))\n",
    "    action_idx = result.pred_score.argmax().item()\n",
    "    return label_map[action_idx]\n",
    "\n",
    "\n",
    "def rgb_based_action_recognition(rgb_config, rgb_checkpoint, device, video, label_map):\n",
    "    rgb_config = mmengine.Config.fromfile(rgb_config)\n",
    "    rgb_config.model.backbone.pretrained = None\n",
    "    rgb_model = init_recognizer(rgb_config, rgb_checkpoint, device)\n",
    "    action_results = inference_recognizer(rgb_model, video)\n",
    "    rgb_action_result = action_results.pred_score.argmax().item()\n",
    "    label_map = [x.strip() for x in open(label_map).readlines()]\n",
    "    return label_map[rgb_action_result]\n",
    "\n",
    "def skeleton_based_stdet(predict_stepsize, skeleton_config, skeleton_stdet_checkpoint, device, action_score_thr, label_map, human_detections, pose_results,\n",
    "                         num_frame, clip_len, frame_interval, h, w):\n",
    "    window_size = clip_len * frame_interval\n",
    "    assert clip_len % 2 == 0, 'We would like to have an even clip_len'\n",
    "    timestamps = np.arange(window_size // 2, num_frame + 1 - window_size // 2,\n",
    "                           predict_stepsize)\n",
    "\n",
    "    skeleton_config = mmengine.Config.fromfile(skeleton_config)\n",
    "    num_class = max(label_map.keys()) + 1  # for AVA dataset (80 + 1), for CIIS dataset (9 + 1) == len(label_map)\n",
    "    skeleton_config.model.cls_head.num_classes = num_class\n",
    "    skeleton_stdet_model = init_recognizer(skeleton_config,\n",
    "                                           skeleton_stdet_checkpoint,\n",
    "                                           device)\n",
    "\n",
    "    # skeleton_stdet_model.eval()\n",
    "    skeleton_predictions = []\n",
    "\n",
    "    print('Performing SpatioTemporal Action Detection for each clip')\n",
    "    prog_bar = mmengine.ProgressBar(len(timestamps))\n",
    "    for timestamp in timestamps:\n",
    "        proposal = human_detections[timestamp - 1]\n",
    "        if proposal.shape[0] == 0:  # no people detected\n",
    "            skeleton_predictions.append(None)\n",
    "            continue\n",
    "\n",
    "        start_frame = timestamp - (clip_len // 2 - 1) * frame_interval\n",
    "        frame_inds = start_frame + np.arange(0, window_size, frame_interval)\n",
    "        frame_inds = list(frame_inds - 1)\n",
    "        num_frame = len(frame_inds)  # 30\n",
    "\n",
    "        pose_result = [pose_results[ind] for ind in frame_inds]\n",
    "\n",
    "        skeleton_prediction = []\n",
    "        for i in range(proposal.shape[0]):  # num_person\n",
    "            skeleton_prediction.append([])\n",
    "\n",
    "            fake_anno = dict(\n",
    "                frame_dict='',\n",
    "                label=-1,\n",
    "                img_shape=(h, w),\n",
    "                origin_shape=(h, w),\n",
    "                start_index=0,\n",
    "                modality='Pose',\n",
    "                num_clips=1,\n",
    "                clip_len=clip_len,\n",
    "                total_frames=num_frame)\n",
    "            num_person = 1\n",
    "\n",
    "            num_keypoint = 17\n",
    "            keypoint = np.zeros(\n",
    "                (num_person, num_frame, num_keypoint, 2))  # M T V 2\n",
    "            keypoint_score = np.zeros(\n",
    "                (num_person, num_frame, num_keypoint))  # M T V\n",
    "\n",
    "            # pose matching\n",
    "            person_bbox = proposal[i][:4]  #x1, y1, x2, y2\n",
    "            area = expand_bbox(person_bbox, h, w)\n",
    "\n",
    "            for j, poses in enumerate(pose_result):  # num_frame\n",
    "                max_iou = float('-inf')\n",
    "                index = -1\n",
    "                if len(poses['keypoints']) == 0:\n",
    "                    continue\n",
    "                for k, bbox in enumerate(poses['bboxes']):  # num_person\n",
    "                    iou = cal_iou(bbox, area)\n",
    "                    if max_iou < iou:  # if isBelong\n",
    "                        index = k\n",
    "                        max_iou = iou\n",
    "                keypoint[0, j] = poses['keypoints'][index]\n",
    "                keypoint_score[0, j] = poses['keypoint_scores'][index]\n",
    "\n",
    "            fake_anno['keypoint'] = keypoint\n",
    "            fake_anno['keypoint_score'] = keypoint_score\n",
    "\n",
    "            output = inference_recognizer(skeleton_stdet_model, fake_anno)\n",
    "            # for multi-label recognition\n",
    "            score = output.pred_score.tolist()\n",
    "            for k in range(len(score)):  # 81\n",
    "                if k not in label_map:\n",
    "                    continue\n",
    "                if score[k] > action_score_thr:\n",
    "                    skeleton_prediction[i].append((label_map[k], score[k]))\n",
    "                    \n",
    "            # crop the image -> resize -> extract pose -> as input for poseC3D\n",
    "\n",
    "        skeleton_predictions.append(skeleton_prediction)\n",
    "        prog_bar.update()\n",
    "\n",
    "    return timestamps, skeleton_predictions\n",
    "\n",
    "\n",
    "def rgb_based_stdet(rgb_stdet_config, rgb_stdet_checkpoint, device, action_score_thr, predict_stepsize, frames, label_map, human_detections, w, h, new_w,\n",
    "                    new_h, w_ratio, h_ratio):\n",
    "\n",
    "    rgb_stdet_config = mmengine.Config.fromfile(rgb_stdet_config)\n",
    "    # rgb_stdet_config.merge_from_dict(args.cfg_options)\n",
    "\n",
    "    val_pipeline = rgb_stdet_config.val_pipeline\n",
    "    sampler = [x for x in val_pipeline if x['type'] == 'SampleAVAFrames'][0]\n",
    "    clip_len, frame_interval = sampler['clip_len'], sampler['frame_interval']\n",
    "    assert clip_len % 2 == 0, 'We would like to have an even clip_len'\n",
    "\n",
    "    window_size = clip_len * frame_interval\n",
    "    num_frame = len(frames)\n",
    "    # Note that it's 1 based here\n",
    "    timestamps = np.arange(window_size // 2, num_frame + 1 - window_size // 2,\n",
    "                           predict_stepsize)\n",
    "\n",
    "    # Get img_norm_cfg\n",
    "    img_norm_cfg = dict(\n",
    "        mean=np.array(rgb_stdet_config.model.data_preprocessor.mean),\n",
    "        std=np.array(rgb_stdet_config.model.data_preprocessor.std),\n",
    "        to_rgb=False)\n",
    "\n",
    "    # Build STDET model\n",
    "    try:\n",
    "        # In our spatiotemporal detection demo, different actions should have\n",
    "        # the same number of bboxes.\n",
    "        rgb_stdet_config['model']['test_cfg']['rcnn'] = dict(action_thr=0)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    rgb_stdet_config.model.backbone.pretrained = None\n",
    "    rgb_stdet_model = init_detector(\n",
    "        rgb_stdet_config, rgb_stdet_checkpoint, device=device)\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    print('Performing SpatioTemporal Action Detection for each clip')\n",
    "    prog_bar = mmengine.ProgressBar(len(timestamps))\n",
    "    # for timestamp, proposal in zip(timestamps, human_detections):\n",
    "    for timestamp in timestamps:\n",
    "        proposal = human_detections[timestamp - 1]\n",
    "        if proposal.shape[0] == 0:\n",
    "            predictions.append(None)\n",
    "            continue\n",
    "\n",
    "        start_frame = timestamp - (clip_len // 2 - 1) * frame_interval\n",
    "        frame_inds = start_frame + np.arange(0, window_size, frame_interval)\n",
    "        frame_inds = list(frame_inds - 1)\n",
    "\n",
    "        imgs = [frames[ind].astype(np.float32) for ind in frame_inds]\n",
    "        _ = [mmcv.imnormalize_(img, **img_norm_cfg) for img in imgs]\n",
    "        # THWC -> CTHW -> 1CTHW\n",
    "        input_array = np.stack(imgs).transpose((3, 0, 1, 2))[np.newaxis]\n",
    "        input_tensor = torch.from_numpy(input_array).to(device)\n",
    "\n",
    "        datasample = ActionDataSample()\n",
    "        datasample.proposals = InstanceData(bboxes=proposal)\n",
    "        datasample.set_metainfo(dict(img_shape=(new_h, new_w)))\n",
    "        with torch.no_grad():\n",
    "            result = rgb_stdet_model(\n",
    "                input_tensor, [datasample], mode='predict')\n",
    "            scores = result[0].pred_instances.scores\n",
    "            prediction = []\n",
    "            # N proposals\n",
    "            for i in range(proposal.shape[0]):\n",
    "                prediction.append([])\n",
    "            # Perform action score thr\n",
    "            for i in range(scores.shape[1]):\n",
    "                if i not in label_map:\n",
    "                    continue\n",
    "                for j in range(proposal.shape[0]):\n",
    "                    if scores[j, i] > action_score_thr:\n",
    "                        prediction[j].append((label_map[i], scores[j,\n",
    "                                                                   i].item()))\n",
    "            predictions.append(prediction)\n",
    "        prog_bar.update()\n",
    "\n",
    "    return timestamps, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31a3917c-9766-4939-988c-db068e5fcf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#args = parse_args()\n",
    "tmp_dir = tempfile.TemporaryDirectory()\n",
    "frame_paths, original_frames = frame_extract(\n",
    "    video, 810, out_dir=tmp_dir.name)\n",
    "num_frame = len(frame_paths)\n",
    "h, w, _ = original_frames[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05e963a3-9141-47ba-9a91-d5436fcab54f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122, 1080, 810, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(original_frames).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1efb83f-385b-4a75-8072-3f3295680b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize frames to shortside 256\n",
    "# new_w, new_h = mmcv.rescale_size((w, h), (256, np.Inf))\n",
    "new_w, new_h = w, h\n",
    "# frames = [mmcv.imresize(img, (new_w, new_h)) for img in original_frames]\n",
    "frames = original_frames\n",
    "w_ratio, h_ratio = new_w / w, new_h / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b26ea491-b1b3-4e33-bfe8-ac973c8a2933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spatio-temporal detection label_map\n",
    "stdet_label_map = load_label_map(label_map_stdet)\n",
    "rgb_stdet_config = mmengine.Config.fromfile(rgb_stdet_config)\n",
    "#rgb_stdet_config.merge_from_dict(args.cfg_options)\n",
    "try:\n",
    "    if rgb_stdet_config['data']['train']['custom_classes'] is not None:\n",
    "        stdet_label_map = {\n",
    "            id + 1: stdet_label_map[cls]\n",
    "            for id, cls in enumerate(rgb_stdet_config['data']['train']\n",
    "                                     ['custom_classes'])\n",
    "        }\n",
    "except KeyError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b12be23-7e0b-47cf-84be-da1eb28f3c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# action_result = None\n",
    "# if use_skeleton_recog:\n",
    "#     print('Use skeleton-based recognition')\n",
    "#     action_result = skeleton_based_action_recognition(\n",
    "#         skeleton_config, skeleton_checkpoint, device, label_map, pose_results, h, w)\n",
    "# else:\n",
    "#     print('Use rgb-based recognition')\n",
    "#     action_result = rgb_based_action_recognition(rgb_config, rgb_checkpoint, device, video, label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77ca5495-aca1-4d5a-8390-28beb907be26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use skeleton-based SpatioTemporal Action Detection\n",
      "Loads checkpoint by local backend from path: work_dirs/ciis_not-multi_10_best-550/best_acc_top1_epoch_550.pth\n",
      "Performing SpatioTemporal Action Detection for each clip\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 28/30, 48.5 task/s, elapsed: 1s, ETA:     0s"
     ]
    }
   ],
   "source": [
    "stdet_preds = None\n",
    "if use_skeleton_stdet:\n",
    "    print('Use skeleton-based SpatioTemporal Action Detection')\n",
    "    # clip_len, frame_interval = 30, 1\n",
    "    clip_len, frame_interval = predict_stepsize, 1\n",
    "    timestamps, stdet_preds = skeleton_based_stdet(predict_stepsize,\n",
    "                                                   skeleton_config,\n",
    "                                                   skeleton_stdet_checkpoint,\n",
    "                                                   device,\n",
    "                                                   action_score_thr,\n",
    "                                                   stdet_label_map,\n",
    "                                                   human_detections,\n",
    "                                                   pose_results, num_frame,\n",
    "                                                   clip_len,\n",
    "                                                   frame_interval, h, w)\n",
    "    for i in range(len(human_detections)):\n",
    "        det = human_detections[i]\n",
    "        try:\n",
    "            det[:, 0:4:2] *= w_ratio\n",
    "            det[:, 1:4:2] *= h_ratio\n",
    "            human_detections[i] = torch.from_numpy(det[:, :4]).to(device)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "else:\n",
    "    print('Use rgb-based SpatioTemporal Action Detection')\n",
    "    for i in range(len(human_detections)):\n",
    "        det = human_detections[i]\n",
    "        # if type(det[:, 0:4:2]) == type((0, 0)):\n",
    "        #     det[:, 0:4:2] *= w_ratio\n",
    "        #     det[:, 1:4:2] *= h_ratio\n",
    "        # else:\n",
    "\n",
    "\n",
    "        det[:, 0:4:2] *= w_ratio\n",
    "        det[:, 1:4:2] *= h_ratio\n",
    "        human_detections[i] = torch.from_numpy(det[:, :4]).to(device)\n",
    "    timestamps, stdet_preds = rgb_based_stdet(rgb_stdet_config,\n",
    "                                              rgb_stdet_checkpoint,\n",
    "                                              device,\n",
    "                                              action_score_thr,\n",
    "                                              predict_stepsize,\n",
    "                                              frames,\n",
    "                                              stdet_label_map,\n",
    "                                              human_detections, w, h,\n",
    "                                              new_w, new_h, w_ratio,\n",
    "                                              h_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6dbf9264-8a9e-4ce1-bb45-1aec960e3498",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdet_results = []\n",
    "for timestamp, prediction in zip(timestamps, stdet_preds):\n",
    "    human_detection = human_detections[timestamp - 1]\n",
    "    stdet_results.append(\n",
    "        pack_result(human_detection, prediction, new_h, new_w))\n",
    "\n",
    "def dense_timestamps(timestamps, n):\n",
    "    \"\"\"Make it nx frames.\"\"\"\n",
    "    old_frame_interval = (timestamps[1] - timestamps[0])\n",
    "    start = timestamps[0] - old_frame_interval / n * (n - 1) / 2\n",
    "    new_frame_inds = np.arange(\n",
    "        len(timestamps) * n) * old_frame_interval / n + start\n",
    "    return new_frame_inds.astype(np.int64)\n",
    "\n",
    "dense_n = int(predict_stepsize / output_stepsize)\n",
    "# output_timestamps = dense_timestamps(timestamps, dense_n)\n",
    "output_timestamps = dense_timestamps(timestamps, dense_n) + 1\n",
    "imgs = [imgs[timestamp] for timestamp in output_timestamps]\n",
    "frames = [\n",
    "    cv2.imread(frame_paths[timestamp - 1])\n",
    "    # cv2.imread(\"../../../Downloads/1280x720-white-solid-color-background.jpg\")\n",
    "    for timestamp in output_timestamps\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98eccae1-68c3-47dc-a6c4-a6636d0b0359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #loop until found berbahaya\n",
    "# action_result = 'tidak berbahaya'\n",
    "# bahaya = ['melempar', 'membidik senapan', 'membidik pistol', 'memukul', 'menendang', 'menusuk']\n",
    "# for prediction_step in stdet_results:\n",
    "#     for person_prop in prediction_step:\n",
    "#         for label in person_prop[1]:\n",
    "#             if label in bahaya:\n",
    "#                 action_result = 'berbahaya'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ad95eca-d528-4011-9527-6ea0ba4871ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e86754fd-de74-49f2-986b-19289918c520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video ../Thermal/CenterNet-ThermalPose-master/osd/0516_20s_out.mp4.\n",
      "Moviepy - Writing video ../Thermal/CenterNet-ThermalPose-master/osd/0516_20s_out.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready ../Thermal/CenterNet-ThermalPose-master/osd/0516_20s_out.mp4\n"
     ]
    }
   ],
   "source": [
    "vis_frames = visualize(pose_config, np.array(imgs), stdet_results, pose_results,\n",
    "                       \"tidak berbahaya\")\n",
    "vid = mpy.ImageSequenceClip(vis_frames, fps=output_fps)\n",
    "vid.write_videofile(out_filename)\n",
    "tmp_dir.cleanup()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
