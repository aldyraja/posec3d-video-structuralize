{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9f660d7-20e5-45cd-9e8e-47a0f9d5b159",
   "metadata": {},
   "source": [
    "# Pose Extraction + Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a971dbaa-8cb8-4b55-a91e-5c8ecfc7a03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) CIIS-Lab. All rights reserved.\n",
    "import os.path as osp\n",
    "\n",
    "# import argparse\n",
    "import copy as cp\n",
    "import tempfile\n",
    "# import warnings\n",
    "\n",
    "import cv2\n",
    "import mmcv\n",
    "import mmengine\n",
    "import numpy as np\n",
    "import torch\n",
    "# from mmengine import DictAction\n",
    "# from mmengine.structures import InstanceData\n",
    "\n",
    "from mmaction.apis import (detection_inference,\n",
    "                           # inference_recognizer, inference_skeleton, init_recognizer,\n",
    "                           pose_inference)\n",
    "from mmaction.registry import VISUALIZERS\n",
    "# from mmaction.structures import ActionDataSample\n",
    "from mmaction.utils import frame_extract\n",
    "\n",
    "# from mmdet.apis import init_detector\n",
    "\n",
    "import moviepy.editor as mpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901f90fa-6681-4fda-b96d-78311755cb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "FONTFACE = cv2.FONT_HERSHEY_DUPLEX\n",
    "FONTSCALE = 1\n",
    "FONTCOLOR = (255, 255, 255)  # BGR, white\n",
    "# MSGCOLOR = (128, 128, 128)  # BGR, gray\n",
    "THICKNESS = 1\n",
    "LINETYPE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7fa058-e46c-4ff9-abdb-06082817c5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hex2color(h):\n",
    "    \"\"\"Convert the 6-digit hex string to tuple of 3 int value (RGB)\"\"\"\n",
    "    return (int(h[:2], 16), int(h[2:4], 16), int(h[4:], 16))\n",
    "\n",
    "PLATEBLUE = '03045e-023e8a-0077b6-0096c7-00b4d8-48cae4'\n",
    "PLATEBLUE = PLATEBLUE.split('-')\n",
    "PLATEBLUE = [hex2color(h) for h in PLATEBLUE]\n",
    "# PLATEGREEN = '004b23-006400-007200-008000-38b000-70e000'\n",
    "# PLATEGREEN = PLATEGREEN.split('-')\n",
    "# PLATEGREEN = [hex2color(h) for h in PLATEGREEN]\n",
    "\n",
    "\n",
    "def visualize(pose_config,\n",
    "              frames,\n",
    "              annotations,\n",
    "              pose_data_samples,\n",
    "              action_result,\n",
    "              plate=PLATEBLUE,\n",
    "              max_num=5):\n",
    "    \"\"\"Visualize frames with predicted annotations.\n",
    "\n",
    "    Args:\n",
    "        frames (list[np.ndarray]): Frames for visualization, note that\n",
    "            len(frames) % len(annotations) should be 0.\n",
    "        annotations (list[list[tuple]]): The predicted spatio-temporal\n",
    "            detection results.\n",
    "        pose_data_samples (list[list[PoseDataSample]): The pose results.\n",
    "        action_result (str): The predicted action recognition results.\n",
    "        pose_model (nn.Module): The constructed pose model.\n",
    "        plate (str): The plate used for visualization. Default: PLATEBLUE.\n",
    "        max_num (int): Max number of labels to visualize for a person box.\n",
    "            Default: 5.\n",
    "\n",
    "    Returns:\n",
    "        list[np.ndarray]: Visualized frames.\n",
    "    \"\"\"\n",
    "\n",
    "    assert max_num + 1 <= len(plate)\n",
    "    frames_ = cp.deepcopy(frames)\n",
    "    frames_ = [mmcv.imconvert(f, 'bgr', 'rgb') for f in frames_]\n",
    "    nf, na = len(frames), len(annotations)\n",
    "    assert nf % na == 0\n",
    "    nfpa = len(frames) // len(annotations)\n",
    "    anno = None\n",
    "    h, w, _ = frames[0].shape\n",
    "    scale_ratio = np.array([w, h, w, h])\n",
    "\n",
    "    # add pose results\n",
    "    if pose_data_samples:\n",
    "        pose_config = mmengine.Config.fromfile(pose_config)\n",
    "        visualizer = VISUALIZERS.build(pose_config.visualizer | {'line_width':5, 'bbox_color':(101,193,255), 'radius': 8})  # https://mmpose.readthedocs.io/en/latest/api.html#mmpose.visualization.PoseLocalVisualizer\n",
    "        visualizer.set_dataset_meta(pose_data_samples[0].dataset_meta)\n",
    "        for i, (d, f) in enumerate(zip(pose_data_samples, frames_)):\n",
    "            visualizer.add_datasample(\n",
    "                'result',\n",
    "                f,\n",
    "                data_sample=d,\n",
    "                draw_gt=False,\n",
    "                draw_heatmap=False,\n",
    "                draw_bbox=True,\n",
    "                draw_pred=True,\n",
    "                show=False,\n",
    "                wait_time=0,\n",
    "                out_file=None,\n",
    "                kpt_thr=0.3)\n",
    "            frames_[i] = visualizer.get_image()\n",
    "            cv2.putText(frames_[i], action_result, (10, 30), FONTFACE,\n",
    "                        FONTSCALE, FONTCOLOR, THICKNESS, LINETYPE)\n",
    "\n",
    "    for i in range(na):\n",
    "        anno = annotations[i]\n",
    "        if anno is None:\n",
    "            continue\n",
    "        for j in range(nfpa):\n",
    "            ind = i * nfpa + j\n",
    "            frame = frames_[ind]\n",
    "\n",
    "            # add action result for whole video\n",
    "            cv2.putText(frame, action_result, (10, 30), FONTFACE, FONTSCALE,\n",
    "                        FONTCOLOR, THICKNESS, LINETYPE)\n",
    "\n",
    "            # add spatio-temporal action detection results\n",
    "            for ann in anno:\n",
    "                box = ann[0]\n",
    "                label = ann[1]\n",
    "                if not len(label):\n",
    "                    continue\n",
    "                score = ann[2]\n",
    "                box = (box * scale_ratio).astype(np.int64)\n",
    "                st, ed = tuple(box[:2]), tuple(box[2:])\n",
    "                if not pose_data_samples:\n",
    "                    cv2.rectangle(frame, st, ed, plate[0], 2)\n",
    "\n",
    "                for k, lb in enumerate(label):\n",
    "                    if k >= max_num:\n",
    "                        break\n",
    "                    text = abbrev(lb)\n",
    "                    text = ': '.join([text, f'{score[k]:.3f}'])\n",
    "                    location = (0 + st[0], 18 + k * 18 + st[1])\n",
    "                    textsize = cv2.getTextSize(text, FONTFACE, FONTSCALE,\n",
    "                                               THICKNESS)[0]\n",
    "                    textwidth = textsize[0]\n",
    "                    diag0 = (location[0] + textwidth, location[1] - 14)\n",
    "                    diag1 = (location[0], location[1] + 2)\n",
    "                    cv2.rectangle(frame, diag0, diag1, plate[k + 1], -1)\n",
    "                    cv2.putText(frame, text, location, FONTFACE, FONTSCALE,\n",
    "                                FONTCOLOR, THICKNESS, LINETYPE)\n",
    "\n",
    "    return frames_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b297f190-39ef-42dd-9f12-e669e029db0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "video = '../cut/70d_1s3.mp4'\n",
    "ann_filename = 'data/new/anno/70d_1s3_edited.csv'\n",
    "pkl_filename = 'data/new/anno/70d_1s3.pkl'\n",
    "out_filename = 'data/new/anno/70d_1s3_ann.mp4'\n",
    "pkl_final = 'data/new/train_1/70d_1s3.pkl'\n",
    "\n",
    "# human detection config\n",
    "det_config = 'mmaction2/demo/demo_configs/faster-rcnn_r50_fpn_2x_coco_infer.py'\n",
    "det_checkpoint = 'http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth'\n",
    "det_score_thr = 0.9\n",
    "#det_cat_id = 0\n",
    "\n",
    "# pose estimation config\n",
    "pose_config = 'mmaction2/demo/demo_configs/td-hm_hrnet-w32_8xb64-210e_coco-256x192_infer.py'\n",
    "pose_checkpoint = 'https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth'\n",
    "\n",
    "# skeleton-based action classification config\n",
    "skeleton_config = \"mmaction2/configs/skeleton/posec3d/slowonly_r50_8xb16-u48-240e_ntu60-xsub-keypoint.py\"\n",
    "action_score_thr = 0.4\n",
    "# skeleton-based action recognition checkpoint\n",
    "skeleton_checkpoint = \"https://download.openmmlab.com/mmaction/skeleton/posec3d/slowonly_r50_u48_240e_ntu60_xsub_keypoint/slowonly_r50_u48_240e_ntu60_xsub_keypoint-f3adabf1.pth\"\n",
    "# skeleton-based spatio temporal detection checkpoint\n",
    "skeleton_stdet_checkpoint = \"https://download.openmmlab.com/mmaction/skeleton/posec3d/slowonly_r50_u48_240e_ntu60_xsub_keypoint/slowonly_r50_u48_240e_ntu60_xsub_keypoint-f3adabf1.pth\"\n",
    "\n",
    "# use skeleton-based method\n",
    "use_skeleton_stdet = True\n",
    "use_skeleton_recog = True\n",
    "\n",
    "label_map = \"mmaction2/tools/data/skeleton/label_map_ntu60.txt\"\n",
    "label_map_stdet = \"mmaction2/tools/data/ciis/label_map.txt\"\n",
    "\n",
    "predict_stepsize = 4  # must even int, give out a spatio-temporal detection prediction per n frames\n",
    "output_stepsize = 1  # show one frame per n frames in the demo, we should have: predict_stepsize % output_stepsize == 0, speedUp/slowDown video output\n",
    "output_fps = 12  # the fps of demo video output, will speedUp/slowDown video output, must equal to (video_input_fps/output_stepsize) to get normal speed\n",
    "\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23467484-3784-4272-b862-bb1fcee3bdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_label_map(file_path):\n",
    "#     \"\"\"Load Label Map.\n",
    "\n",
    "#     Args:\n",
    "#         file_path (str): The file path of label map.\n",
    "\n",
    "#     Returns:\n",
    "#         dict: The label map (int -> label name).\n",
    "#     \"\"\"\n",
    "#     lines = open(file_path).readlines()\n",
    "#     lines = [x.strip().split(': ') for x in lines]\n",
    "#     return {int(x[0]): x[1] for x in lines}\n",
    "\n",
    "\n",
    "def abbrev(name):\n",
    "    \"\"\"Get the abbreviation of label name:\n",
    "\n",
    "    'take (an object) from (a person)' -> 'take ... from ...'\n",
    "    \"\"\"\n",
    "    while name.find('(') != -1:\n",
    "        st, ed = name.find('('), name.find(')')\n",
    "        name = name[:st] + '...' + name[ed + 1:]\n",
    "    return name\n",
    "\n",
    "def pack_result(human_detection, result, img_h, img_w):\n",
    "    \"\"\"Short summary.\n",
    "\n",
    "    Args:\n",
    "        human_detection (np.ndarray): Human detection result.\n",
    "        result (type): The predicted label of each human proposal.\n",
    "        img_h (int): The image height.\n",
    "        img_w (int): The image width.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple of human proposal, label name and label score.\n",
    "    \"\"\"\n",
    "    human_detection[:, 0::2] /= img_w\n",
    "    human_detection[:, 1::2] /= img_h\n",
    "    results = []\n",
    "    if result is None:\n",
    "        return None\n",
    "    for prop, res in zip(human_detection, result):\n",
    "        res.sort(key=lambda x: -x[1])\n",
    "        results.append(\n",
    "            (prop.data.cpu().numpy(), [x[0] for x in res], [x[1]\n",
    "                                                            for x in res]))\n",
    "    return results\n",
    "\n",
    "\n",
    "def expand_bbox(bbox, h, w, ratio=1.25):\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    center_x = (x1 + x2) // 2\n",
    "    center_y = (y1 + y2) // 2\n",
    "    width = x2 - x1\n",
    "    height = y2 - y1\n",
    "\n",
    "    square_l = max(width, height)\n",
    "    new_width = new_height = square_l * ratio\n",
    "\n",
    "    new_x1 = max(0, int(center_x - new_width / 2))\n",
    "    new_x2 = min(int(center_x + new_width / 2), w)\n",
    "    new_y1 = max(0, int(center_y - new_height / 2))\n",
    "    new_y2 = min(int(center_y + new_height / 2), h)\n",
    "    return (new_x1, new_y1, new_x2, new_y2)\n",
    "\n",
    "\n",
    "def cal_iou(box1, box2):\n",
    "    xmin1, ymin1, xmax1, ymax1 = box1\n",
    "    xmin2, ymin2, xmax2, ymax2 = box2\n",
    "\n",
    "    s1 = (xmax1 - xmin1) * (ymax1 - ymin1)\n",
    "    s2 = (xmax2 - xmin2) * (ymax2 - ymin2)\n",
    "\n",
    "    xmin = max(xmin1, xmin2)\n",
    "    ymin = max(ymin1, ymin2)\n",
    "    xmax = min(xmax1, xmax2)\n",
    "    ymax = min(ymax1, ymax2)\n",
    "\n",
    "    w = max(0, xmax - xmin)\n",
    "    h = max(0, ymax - ymin)\n",
    "    intersect = w * h\n",
    "    union = s1 + s2 - intersect\n",
    "    iou = intersect / union\n",
    "\n",
    "    return iou\n",
    "\n",
    "\n",
    "# clip_pose_extraction\n",
    "def skeleton_based_stdet(predict_stepsize, video,\n",
    "                         # skeleton_config, skeleton_stdet_checkpoint, device, action_score_thr, label_map,\n",
    "                         human_detections, pose_results, num_frame, clip_len, frame_interval, h, w):\n",
    "    window_size = clip_len * frame_interval\n",
    "    assert clip_len % 2 == 0, 'We would like to have an even clip_len'\n",
    "    timestamps = np.arange(window_size // 2, num_frame + 1 - window_size // 2,\n",
    "                           predict_stepsize)\n",
    "\n",
    "    # skeleton_config = mmengine.Config.fromfile(skeleton_config)\n",
    "    # num_class = max(label_map.keys()) + 1  # for AVA dataset (81)\n",
    "    # skeleton_config.model.cls_head.num_classes = num_class\n",
    "    # skeleton_stdet_model = init_recognizer(skeleton_config,\n",
    "    #                                        skeleton_stdet_checkpoint,\n",
    "    #                                        device)\n",
    "\n",
    "    skeleton_predictions = []\n",
    "    skeleton_datasets = []\n",
    "\n",
    "    print('Building skeleton datasets from existing keypoint data for each clip')\n",
    "    prog_bar = mmengine.ProgressBar(len(timestamps))\n",
    "    for timestamp in timestamps:  # iterate each clip\n",
    "        proposal = human_detections[timestamp - 1] # get bboxes for persons in timestamp (first frame of clip)\n",
    "        if proposal.shape[0] == 0:  # no people detected\n",
    "            skeleton_predictions.append(None)\n",
    "            continue\n",
    "\n",
    "        start_frame = timestamp - (clip_len // 2 - 1) * frame_interval\n",
    "        frame_inds = start_frame + np.arange(0, window_size, frame_interval)\n",
    "        frame_inds = list(frame_inds - 1)\n",
    "        num_frame = len(frame_inds)  # 30\n",
    "\n",
    "        pose_result = [pose_results[ind] for ind in frame_inds]  # grouping frames poses for each clip\n",
    "\n",
    "        skeleton_prediction = []\n",
    "        for i in range(proposal.shape[0]):  # num_person  # iterate each bbox in timestamp (first frame of clip)\n",
    "            skeleton_prediction.append([])\n",
    "\n",
    "            fake_anno = dict(\n",
    "                frame_dir=osp.splitext(osp.basename(video))[0]+\"_\"+str(timestamp+(i+1)*0.001),\n",
    "                label=-1,\n",
    "                img_shape=(h, w),\n",
    "                original_shape=(h, w),\n",
    "                total_frames=num_frame)\n",
    "            num_person = 1\n",
    "\n",
    "            num_keypoint = 17\n",
    "            keypoint = np.zeros(\n",
    "                (num_person, num_frame, num_keypoint, 2))  # M T V 2\n",
    "            keypoint_score = np.zeros(\n",
    "                (num_person, num_frame, num_keypoint))  # M T V\n",
    "\n",
    "            # pose matching\n",
    "            person_bbox = proposal[i][:4]  # get bbox for a person in timestamp (first frame of clip)\n",
    "            area = expand_bbox(person_bbox, h, w)  # bbox expanded by 1.25 ratio with square shape\n",
    "\n",
    "            for j, poses in enumerate(pose_result):  # num_frame  # iterate each frame of clip\n",
    "                max_iou = float('-inf')\n",
    "                index = -1\n",
    "                if len(poses['keypoints']) == 0:\n",
    "                    continue\n",
    "                for k, bbox in enumerate(poses['bboxes']):  # iterate each bbox/pose in each frame\n",
    "                    iou = cal_iou(bbox, area)  # compare each bbox in each frame with current area (calculate_intersect/union)\n",
    "                    if max_iou < iou:\n",
    "                        index = k  # pose from the biggest intersect/union (iou) will be considered\n",
    "                        max_iou = iou\n",
    "                keypoint[0, j] = poses['keypoints'][index]\n",
    "                keypoint_score[0, j] = poses['keypoint_scores'][index]\n",
    "\n",
    "            fake_anno['keypoint'] = keypoint\n",
    "            fake_anno['keypoint_score'] = keypoint_score\n",
    "\n",
    "            skeleton_datasets.append(fake_anno)\n",
    "            # output = inference_recognizer(skeleton_stdet_model, fake_anno)\n",
    "            # # for multi-label recognition\n",
    "            # score = output.pred_score.tolist()\n",
    "            # for k in range(len(score)):  # 81\n",
    "            #     if k not in label_map:\n",
    "            #         continue\n",
    "            #     if score[k] > action_score_thr:\n",
    "            #         skeleton_prediction[i].append((label_map[k], score[k]))\n",
    "            skeleton_prediction[i].append((\"annotate!\", timestamp + (i+1)*0.001))\n",
    "\n",
    "        skeleton_predictions.append(skeleton_prediction)\n",
    "        prog_bar.update()\n",
    "\n",
    "    return timestamps, skeleton_predictions, skeleton_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596aac39-11c2-4554-ab67-832b5f39816b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#args = parse_args()\n",
    "tmp_dir = tempfile.TemporaryDirectory()\n",
    "frame_paths, original_frames = frame_extract(\n",
    "    video, 720, out_dir=tmp_dir.name)\n",
    "num_frame = len(frame_paths)\n",
    "h, w, _ = original_frames[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59d5bd0-48e2-47e5-a7c9-d4261ad35873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Human detection results and pose results\n",
    "human_detections, _ = detection_inference(\n",
    "    det_config,\n",
    "    det_checkpoint,\n",
    "    frame_paths,\n",
    "    det_score_thr,\n",
    "    device=device)\n",
    "torch.cuda.empty_cache()\n",
    "pose_datasample = None\n",
    "if use_skeleton_recog or use_skeleton_stdet:\n",
    "    pose_results, pose_datasample = pose_inference(\n",
    "        pose_config,\n",
    "        pose_checkpoint,\n",
    "        frame_paths,\n",
    "        human_detections,\n",
    "        device=device)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8274ffe-a280-4584-a842-676b860dc34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdet_preds = None\n",
    "if use_skeleton_stdet:\n",
    "    print('Use skeleton-based SpatioTemporal Action Detection')\n",
    "    # clip_len, frame_interval = 30, 1\n",
    "    clip_len, frame_interval = predict_stepsize, 1\n",
    "\n",
    "    # clip_pose_extraction\n",
    "    timestamps, stdet_preds, skeleton_datasets = skeleton_based_stdet(predict_stepsize, video,\n",
    "                                                                      # skeleton_config,\n",
    "                                                                      # skeleton_stdet_checkpoint,\n",
    "                                                                      # device,\n",
    "                                                                      # action_score_thr,\n",
    "                                                                      # stdet_label_map,\n",
    "                                                                      human_detections,\n",
    "                                                                      pose_results, num_frame,\n",
    "                                                                      clip_len,\n",
    "                                                                      frame_interval, h, w)\n",
    "    for i in range(len(human_detections)):\n",
    "        det = human_detections[i]\n",
    "        # det[:, 0:4:2] *= w_ratio\n",
    "        # det[:, 1:4:2] *= h_ratio\n",
    "        det[:, 0:4:2] *= 1\n",
    "        det[:, 1:4:2] *= 1\n",
    "        human_detections[i] = torch.from_numpy(det[:, :4]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d481a4de-843b-4dff-80ec-4fdcd773f294",
   "metadata": {},
   "outputs": [],
   "source": [
    "anno = \"\"\n",
    "for clip in stdet_preds:\n",
    "    if clip == None:\n",
    "        continue\n",
    "    for person_attr in clip:\n",
    "        anno += str(person_attr[0][0]) + \",\" + str(person_attr[0][1]) + \"\\n\"\n",
    "\n",
    "with open(ann_filename,'w') as data:\n",
    "    data.write(anno)\n",
    "\n",
    "mmengine.dump(skeleton_datasets, pkl_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa487242-f475-4e5d-b382-f8c482c364d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdet_results = []\n",
    "for timestamp, prediction in zip(timestamps, stdet_preds):\n",
    "    human_detection = human_detections[timestamp - 1]\n",
    "    stdet_results.append(\n",
    "        pack_result(human_detection, prediction, h, w))\n",
    "\n",
    "def dense_timestamps(timestamps, n):\n",
    "    \"\"\"Make it nx frames.\"\"\"\n",
    "    old_frame_interval = (timestamps[1] - timestamps[0])\n",
    "    start = timestamps[0] - old_frame_interval / n * (n - 1) / 2\n",
    "    new_frame_inds = np.arange(\n",
    "        len(timestamps) * n) * old_frame_interval / n + start\n",
    "    return new_frame_inds.astype(np.int64)\n",
    "\n",
    "dense_n = int(predict_stepsize / output_stepsize)\n",
    "output_timestamps = dense_timestamps(timestamps, dense_n) + 1\n",
    "frames = [\n",
    "    cv2.imread(frame_paths[timestamp - 1])\n",
    "    for timestamp in output_timestamps\n",
    "]\n",
    "\n",
    "if use_skeleton_recog or use_skeleton_stdet:\n",
    "    pose_datasample = [\n",
    "        pose_datasample[timestamp - 1] for timestamp in output_timestamps\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49614666-c51b-45a1-9658-2d2192a15a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_frames = visualize(pose_config, frames, stdet_results, pose_datasample,\n",
    "                       None)\n",
    "vid = mpy.ImageSequenceClip(vis_frames, fps=output_fps)\n",
    "vid.write_videofile(out_filename)\n",
    "tmp_dir.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4b009e-2616-401c-814e-326c99aaca54",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_extract(\n",
    "    out_filename, out_dir=\"../../skripsi/extracted/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f05741-9b0b-4cc5-ad76-d7ac602caf76",
   "metadata": {},
   "source": [
    "# Add Label to Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f6e203-1374-4c1b-823d-d1fb013761cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def load_label_map(file_path):\n",
    "    \"\"\"Load Label Map.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The file path of label map.\n",
    "\n",
    "    Returns:\n",
    "        dict: The label map (label name -> int).\n",
    "    \"\"\"\n",
    "    lines = open(file_path).readlines()\n",
    "    lines = [x.strip().split(': ') for x in lines]\n",
    "    return {x[1]: int(x[0]) for x in lines}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672a45a1-e630-4ad1-ac43-5fff8a27a8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdet_label_map = load_label_map(label_map_stdet)\n",
    "\n",
    "custom_annos = []\n",
    "\n",
    "with open(ann_filename, newline='') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in spamreader:\n",
    "        if row[0] == 'none':\n",
    "            continue\n",
    "        label = stdet_label_map[row[0]]\n",
    "        id = float(row[1])\n",
    "        custom_annos.append([id, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8733dbe-bc1a-4550-9d38-eebfa19d8fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "skeleton_datasets = mmengine.load(pkl_filename)\n",
    "\n",
    "custom_dataset = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af24374-c83a-432a-bc31-032e5bea093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, ann in enumerate(custom_annos):\n",
    "\n",
    "    fake_anno = dict(\n",
    "        frame_dir=osp.splitext(osp.basename(pkl_filename))[0]+\"_\"+str(ann[0]))\n",
    "\n",
    "    for j, data in enumerate(skeleton_datasets):\n",
    "        if fake_anno['frame_dir'] == data['frame_dir']:\n",
    "            fake_anno['frame_dir'] += '_' + str(index)\n",
    "            fake_anno['label'] = ann[1]\n",
    "            fake_anno['img_shape'] = data['img_shape']\n",
    "            fake_anno['original_shape'] = data['original_shape']\n",
    "            fake_anno['total_frames'] = data['total_frames']\n",
    "            fake_anno['keypoint'] = data['keypoint']\n",
    "            fake_anno['keypoint_score'] = data['keypoint_score']\n",
    "\n",
    "    custom_dataset.append(fake_anno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11b5d5c-6606-44e8-ba9a-07bea8dd4966",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmengine.dump(custom_dataset, pkl_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbe022e-78e3-4c08-bc71-7ba293e895c5",
   "metadata": {},
   "source": [
    "# Combine PKL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3415cdce-806f-4f55-bcbe-fca63a12cee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecf48c26-7996-41fb-b6b9-ef1c7954427a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickles_path = 'data/new/train_1'\n",
    "combined_pkl = 'data/skeleton/ciis_1.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3f6b6f7-20c9-4b6f-ab2e-b12520f58599",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_datasets = dict(split=dict(xsub_train=[],\n",
    "                                  xsub_val=[],\n",
    "                                  xview_train=[],\n",
    "                                  xview_val=[]),\n",
    "                       annotations=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2cb2274-76a3-4ce1-84fe-59684f6b91c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(pickles_path):\n",
    "    if not file.endswith('.pkl'):\n",
    "        continue\n",
    "    custom_dataset = mmengine.load(os.path.join(pickles_path, file))\n",
    "    for data in custom_dataset:\n",
    "        custom_datasets['annotations'].append(data)\n",
    "        custom_datasets['split']['xsub_train'].append(data['frame_dir'])\n",
    "        custom_datasets['split']['xsub_val'].append(data['frame_dir'])\n",
    "        custom_datasets['split']['xview_train'].append(data['frame_dir'])\n",
    "        custom_datasets['split']['xview_val'].append(data['frame_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec9f3f55-ec49-4110-a576-49bdced5d870",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmengine.dump(custom_datasets, combined_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a58a621-375b-40c9-b57a-8bc52cc93da6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
