{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7df9318-3492-43a9-9f19-5a589a6e57f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a83e8aa-9403-42e3-ab17-07b34e1e49fa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Testing recognition from bash - DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bdfe89-b1e2-4238-940d-7586d302535a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python mmaction2/demo/demo_skeleton.py data/uji_jalan.mp4 data/uji_jalan_out_demo.mp4 \\\n",
    "    --config mmaction2/configs/skeleton/posec3d/slowonly_r50_8xb16-u48-240e_ntu60-xsub-keypoint.py \\\n",
    "    --checkpoint https://download.openmmlab.com/mmaction/skeleton/posec3d/slowonly_r50_u48_240e_ntu60_xsub_keypoint/slowonly_r50_u48_240e_ntu60_xsub_keypoint-f3adabf1.pth \\\n",
    "    --det-config mmaction2/demo/demo_configs/faster-rcnn_r50_fpn_2x_coco_infer.py \\\n",
    "    --det-checkpoint http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth \\\n",
    "    --det-score-thr 0.9 \\\n",
    "    --det-cat-id 0 \\\n",
    "    --pose-config mmaction2/demo/demo_configs/td-hm_hrnet-w32_8xb64-210e_coco-256x192_infer.py \\\n",
    "    --pose-checkpoint https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth \\\n",
    "    --label-map mmaction2/tools/data/skeleton/label_map_ntu60.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241d889f-adf2-443f-8922-ed99ed002070",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('<video width=50% controls autoplay loop><source src=\"data/uji_jalan.mp4\"></video>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fad1db3-acd5-4dd2-84e3-f8a7a67d5aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('<video width=50% controls autoplay loop><source src=\"data/uji_jalan_out_demo.mp4\"></video>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6244f32-9406-4f93-b9c6-6cdc7557dccf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Testing recognition from python - DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aae07f-cdab-480b-b8ab-34882bf3dda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "import cv2\n",
    "import mmcv\n",
    "import mmengine\n",
    "import torch\n",
    "from mmengine.utils import track_iter_progress\n",
    "\n",
    "from mmaction.apis import (detection_inference, inference_skeleton,\n",
    "                           init_recognizer, pose_inference)\n",
    "from mmaction.registry import VISUALIZERS\n",
    "from mmaction.utils import frame_extract\n",
    "\n",
    "import moviepy.editor as mpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860bdd57-2045-48c9-899e-a424e6eaf8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import moviepy.config as cf\n",
    "print ( cf.get_setting(\"FFMPEG_BINARY\") ) # prints the current setting, make sure to use imageio_ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0f48c9-a98a-4742-86d8-e4bfc65dbe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "FONTFACE = cv2.FONT_HERSHEY_DUPLEX\n",
    "FONTSCALE = 1.25\n",
    "FONTCOLOR = (255, 255, 255)  # BGR, white\n",
    "THICKNESS = 2\n",
    "LINETYPE = 1\n",
    "\n",
    "def visualize(pose_config, out_filename, frames, data_samples, action_label):\n",
    "    pose_config = mmengine.Config.fromfile(pose_config)\n",
    "    visualizer = VISUALIZERS.build(pose_config.visualizer)  # https://mmpose.readthedocs.io/en/latest/api.html#mmpose.visualization.PoseLocalVisualizer\n",
    "    visualizer.set_dataset_meta(data_samples[0].dataset_meta)\n",
    "\n",
    "    vis_frames = []\n",
    "    print('Drawing skeleton for each frame')\n",
    "    for d, f in track_iter_progress(list(zip(data_samples, frames))):\n",
    "        f = mmcv.imconvert(f, 'bgr', 'rgb')\n",
    "        visualizer.add_datasample(\n",
    "            'result',\n",
    "            f,\n",
    "            data_sample=d,\n",
    "            draw_gt=False,\n",
    "            draw_heatmap=True,\n",
    "            draw_bbox=True,\n",
    "            show=False,\n",
    "            wait_time=0,\n",
    "            out_file=None,\n",
    "            kpt_thr=0.3)\n",
    "        vis_frame = visualizer.get_image()\n",
    "        # heatmap = visualizer.draw_featmap(featmap, img, channel_reduction='select_max')\n",
    "        bahaya = ['pistol', 'laras_panjang']\n",
    "        cv2.putText(vis_frame, action_label, (10, 30), FONTFACE, FONTSCALE,\n",
    "                    FONTCOLOR, THICKNESS, LINETYPE)\n",
    "        vis_frames.append(vis_frame)\n",
    "\n",
    "    vid = mpy.ImageSequenceClip(vis_frames, fps=12)\n",
    "    vid.write_videofile(out_filename, remove_temp=True)\n",
    "    # vid.ipython_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2361e83b-ea17-425f-be37-ff9737c1c480",
   "metadata": {},
   "outputs": [],
   "source": [
    "video = '../cut/DJI_0013_12r_10s_2.mp4'\n",
    "out_filename = 'data/DJI_0013_12r_10s_2_2019.mp4'\n",
    "\n",
    "# Choose to use an action classification config\n",
    "config = 'mmaction2/configs/skeleton/posec3d/slowonly_r50_8xb16-u48-240e_ntu60-xsub-keypoint.py'\n",
    "# Setup a checkpoint file to load\n",
    "checkpoint = 'https://download.openmmlab.com/mmaction/skeleton/posec3d/slowonly_r50_u48_240e_ntu60_xsub_keypoint/slowonly_r50_u48_240e_ntu60_xsub_keypoint-f3adabf1.pth'\n",
    "\n",
    "# human detection config\n",
    "det_config = 'mmaction2/demo/demo_configs/faster-rcnn_r50_fpn_2x_coco_infer.py'\n",
    "det_checkpoint = 'http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth'\n",
    "det_score_thr = 0.9\n",
    "det_cat_id = 0\n",
    "\n",
    "# pose estimation config\n",
    "pose_config = 'mmaction2/demo/demo_configs/td-hm_hrnet-w32_8xb64-210e_coco-256x192_infer.py'\n",
    "pose_checkpoint = 'https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth'\n",
    "label_map = 'mmaction2/tools/data/skeleton/label_map_ntu60.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4a8fb4-15d2-4877-9486-c4c365b47a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "short_side = 480"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3007172d-6374-483a-8902-6781e9a7119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dir = tempfile.TemporaryDirectory()\n",
    "frame_paths, frames = frame_extract(video, short_side,\n",
    "                                    tmp_dir.name)\n",
    "\n",
    "h, w, _ = frames[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce84a3f-cec2-49bc-9295-e256de6ba1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Human detection results.\n",
    "det_results, _ = detection_inference(det_config, det_checkpoint,\n",
    "                                     frame_paths, det_score_thr,\n",
    "                                     det_cat_id, device)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad15062f-e2f6-420b-8254-14f21c1607b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Pose estimation results.\n",
    "pose_results, pose_data_samples = pose_inference(pose_config,\n",
    "                                                 pose_checkpoint,\n",
    "                                                 frame_paths, det_results,\n",
    "                                                 device)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733874cf-67e2-49fd-be59-b1ff44b4303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the recognizer\n",
    "config = mmengine.Config.fromfile(config)\n",
    "# build the model from a config file and a checkpoint file\n",
    "model = init_recognizer(config, checkpoint, device)\n",
    "\n",
    "# Get Action classification results.\n",
    "result = inference_skeleton(model, pose_results, (h, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5749f941-b6f4-4161-8314-9ce8e0705872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the index of highest predicted score on result\n",
    "max_pred_index = result.pred_score.argmax().item()\n",
    "\n",
    "label_map = [x.strip() for x in open(label_map).readlines()]\n",
    "\n",
    "# set the highest predicted label as action_label\n",
    "action_label = label_map[max_pred_index]\n",
    "print(action_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f1ad4f-bbad-4bdf-8132-7e77e18044cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(pose_config, out_filename, frames, pose_data_samples, 'jalan')\n",
    "\n",
    "tmp_dir.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa4a081-a5a2-4a1f-9397-1348491673e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('<video width=50% controls autoplay loop><source src=\"data/uji_jalan.mp4\"></video>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628f1dd4-5ade-4d36-8885-735f9c41acb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('<video width=50% controls autoplay loop><source src=\"data/uji_jalan_out_demo.mp4\"></video>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f232e4e6-c864-4e1b-b731-5572c316b614",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Testing recognition from bash - 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1f70ed-737b-4612-9591-3d99766ff2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python mmaction2/demo/demo_skeleton.py data/uji_jalan.mp4 data/uji_jalan_out_2019.mp4 \\\n",
    "    --config mmaction2/configs/skeleton/posec3d/slowonly_r50_u48_240e_ntu120_xsub_keypoint.py \\\n",
    "    --checkpoint work_dirs/slowonly_r50_u48_240e_ntu120_xsub_keypoint/best_top1_acc_epoch_90_8.pth \\\n",
    "    --det-config mmaction2/demo/demo_configs/faster-rcnn_r50_fpn_2x_coco_infer.py \\\n",
    "    --det-checkpoint http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth \\\n",
    "    --det-score-thr 0.9 \\\n",
    "    --det-cat-id 0 \\\n",
    "    --pose-config mmaction2/demo/demo_configs/td-hm_hrnet-w32_8xb64-210e_coco-256x192_infer.py \\\n",
    "    --pose-checkpoint https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth \\\n",
    "    --label-map mmaction2/tools/data/skeleton/label_5.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7745f8-0278-4289-88bd-5a014ddfb6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('<video width=50% controls autoplay loop><source src=\"data/uji_jalan.mp4\"></video>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96005e2e-901e-45ca-99fb-67cd10960249",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('<video width=50% controls autoplay loop><source src=\"data/uji_jalan_out_2019.mp4\"></video>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d38b7ee-309c-4c34-9156-1ab51ffdac00",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Testing recognition from python - 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a9c591-0fa9-4269-b82b-c34cff2b6470",
   "metadata": {},
   "outputs": [],
   "source": [
    "FONTFACE = cv2.FONT_HERSHEY_DUPLEX\n",
    "FONTSCALE = 0.75\n",
    "FONTCOLOR = (255, 255, 255)  # BGR, white\n",
    "THICKNESS = 1\n",
    "LINETYPE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0209fee4-5945-4519-9d69-bbda6c076184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(pose_config, out_filename, frames, data_samples, action_label):\n",
    "    pose_config = mmengine.Config.fromfile(pose_config)\n",
    "    visualizer = VISUALIZERS.build(pose_config.visualizer)\n",
    "    visualizer.set_dataset_meta(data_samples[0].dataset_meta)\n",
    "\n",
    "    vis_frames = []\n",
    "    print('Drawing skeleton for each frame')\n",
    "    for d, f in track_iter_progress(list(zip(data_samples, frames))):\n",
    "        f = mmcv.imconvert(f, 'bgr', 'rgb')\n",
    "        visualizer.add_datasample(\n",
    "            'result',\n",
    "            f,\n",
    "            data_sample=d,\n",
    "            draw_gt=False,\n",
    "            draw_heatmap=False,\n",
    "            draw_bbox=True,\n",
    "            show=False,\n",
    "            wait_time=0,\n",
    "            out_file=None,\n",
    "            kpt_thr=0.3)\n",
    "        vis_frame = visualizer.get_image()\n",
    "        cv2.putText(vis_frame, action_label, (10, 30), FONTFACE, FONTSCALE,\n",
    "                    FONTCOLOR, THICKNESS, LINETYPE)\n",
    "        vis_frames.append(vis_frame)\n",
    "\n",
    "    vid = mpy.ImageSequenceClip(vis_frames, fps=24)\n",
    "    vid.write_videofile(out_filename, remove_temp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161763e5-54b7-46d4-a7f6-c2c5bdceb362",
   "metadata": {},
   "outputs": [],
   "source": [
    "video = 'data/uji_jalan.mp4'\n",
    "out_filename = 'data/uji_jalan_out_2019.mp4'\n",
    "\n",
    "# Choose to use an action classification config\n",
    "config = 'mmaction2/configs/skeleton/posec3d/slowonly_r50_u48_240e_ntu120_xsub_keypoint.py'\n",
    "# Setup a checkpoint file to load\n",
    "checkpoint = 'work_dirs/slowonly_r50_u48_240e_ntu120_xsub_keypoint/best_top1_acc_epoch_90_8.pth' #class 5 label\n",
    "\n",
    "# human detection config\n",
    "det_config = 'mmaction2/demo/demo_configs/faster-rcnn_r50_fpn_2x_coco_infer.py'\n",
    "det_checkpoint = 'http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth'\n",
    "det_score_thr = 0.9\n",
    "det_cat_id = 0\n",
    "\n",
    "# pose estimation config\n",
    "pose_config = 'mmaction2/demo/demo_configs/td-hm_hrnet-w32_8xb64-210e_coco-256x192_infer.py'\n",
    "pose_checkpoint = 'https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth'\n",
    "label_map = 'mmaction2/tools/data/skeleton/label_5.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692dbfa0-773c-4630-9266-4fdbb1614887",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "short_side = 480"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a88ba2-b35a-4b5c-b4da-edafcddc6789",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dir = tempfile.TemporaryDirectory()\n",
    "frame_paths, frames = frame_extract(video, short_side,\n",
    "                                    tmp_dir.name)\n",
    "\n",
    "h, w, _ = frames[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2300462-6796-4921-a04f-27c4da85f37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Human detection results.\n",
    "det_results, _ = detection_inference(det_config, det_checkpoint,\n",
    "                                     frame_paths, det_score_thr,\n",
    "                                     det_cat_id, device)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b40730-9de2-46b9-b547-6f1821009529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Pose estimation results.\n",
    "pose_results, pose_data_samples = pose_inference(pose_config,\n",
    "                                                 pose_checkpoint,\n",
    "                                                 frame_paths, det_results,\n",
    "                                                 device)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1916d8-ad3b-4edf-95e1-e1a6ffbdf87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the recognizer\n",
    "config = mmengine.Config.fromfile(config)\n",
    "# build the model from a config file and a checkpoint file\n",
    "model = init_recognizer(config, checkpoint, device)\n",
    "\n",
    "# Get Action classification results.\n",
    "result = inference_skeleton(model, pose_results, (h, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc6b686-9988-419b-bac8-f477fcc10423",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = 'mmaction2/configs/skeleton/posec3d/slowonly_r50_u48_240e_ntu120_xsub_keypoint.py'\n",
    "checkpoint = \"work_dirs/slowonly_r50_u48_240e_ntu120_xsub_keypoint/best_top1_acc_epoch_90_8.pth\"\n",
    "\n",
    "config = mmengine.Config.fromfile(config)\n",
    "\n",
    "# build the model from a config file and a checkpoint file\n",
    "model = init_recognizer(config, checkpoint, device)\n",
    "\n",
    "# Get Action classification results.\n",
    "result = inference_skeleton(model, pose_results, (h, w))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2413eac7-a2e2-4745-9cf9-1726a7f8d1de",
   "metadata": {},
   "source": [
    "ann_file_train = '/home/ciis-compnew/Desktop/ciis_dataset/8 class 50/custom_8508020_train_new.pkl'  # not available\n",
    "ann_file_val = '/home/ciis-compnew/Desktop/ciis_dataset/8 class 50/custom_8508020_test_new.pkl'  # not available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f2d05f-d66c-4ee8-90bb-aa22fb5afea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the index of highest predicted score on result\n",
    "max_pred_index = result.pred_score.argmax().item()\n",
    "\n",
    "label_map = [x.strip() for x in open(label_map).readlines()]\n",
    "\n",
    "# set the highest predicted label as action_label\n",
    "action_label = label_map[max_pred_index]\n",
    "print(action_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5658e56b-b691-4576-ad68-da48b4f9deb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(pose_config, out_filename, frames, pose_data_samples, action_label)\n",
    "\n",
    "tmp_dir.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e67272-bce0-4b5f-bc13-eb7f27c31ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('<video width=50% controls autoplay loop><source src=\"data/video_out.mp4\"></video>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ebca12-2f2e-4bcb-9e10-f23c26706fcb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Testing recognition + detection from bash - DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d541ce2f-026e-4c1e-aedf-4b41180467c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python mmaction2/demo/demo_video_structuralize.py \\\n",
    "    --video mmaction2/demo/test_video_structuralize.mp4 \\\n",
    "    --out-filename mmaction2/demo/test_stdet_recognition_output.mp4 \\\n",
    "\\\n",
    "    --rgb-stdet-config mmaction2/configs/detection/slowonly/slowonly_kinetics400-pretrained-r101_8xb16-8x8x1-20e_ava21-rgb.py \\\n",
    "    --skeleton-stdet-checkpoint https://download.openmmlab.com/mmaction/skeleton/posec3d/posec3d_ava.pth \\\n",
    "\\\n",
    "    --det-config mmaction2/demo/demo_configs/faster-rcnn_r50_fpn_2x_coco_infer.py \\\n",
    "    --det-checkpoint http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth \\\n",
    "\\\n",
    "    --pose-config mmaction2/demo/demo_configs/td-hm_hrnet-w32_8xb64-210e_coco-256x192_infer.py \\\n",
    "    --pose-checkpoint https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth \\\n",
    "\\\n",
    "    --skeleton-config mmaction2/configs/skeleton/posec3d/slowonly_r50_8xb16-u48-240e_ntu60-xsub-keypoint.py \\\n",
    "    --skeleton-checkpoint https://download.openmmlab.com/mmaction/skeleton/posec3d/posec3d_k400.pth \\\n",
    "\\\n",
    "    --use-skeleton-stdet \\\n",
    "    --use-skeleton-recog \\\n",
    "\\\n",
    "    --label-map-stdet mmaction2/tools/data/ava/label_map.txt \\\n",
    "    --label-map mmaction2/tools/data/kinetics/label_map_k400.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad587218-846b-4b5b-b016-083302936b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('<video width=50% controls autoplay loop><source src=\"mmaction2/demo/test_video_structuralize.mp4\"></video>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f98e148-3cf2-46a8-a265-f8aa7c65649f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "HTML('<video width=50% controls autoplay loop><source src=\"mmaction2/demo/test_stdet_recognition_output.mp4\"></video>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a283883-e7c5-49c7-830a-9b3c0c73b1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python mmaction2/demo/demo_video_structuralize.py \\\n",
    "    --video mmaction2/demo/demo_skeleton.mp4 \\\n",
    "    --out-filename mmaction2/demo/demo_skeleton_vs_out.mp4 \\\n",
    "    --rgb-stdet-config mmaction2/configs/detection/slowonly/slowonly_kinetics400-pretrained-r101_8xb16-8x8x1-20e_ava21-rgb.py \\\n",
    "    --skeleton-stdet-checkpoint https://download.openmmlab.com/mmaction/skeleton/posec3d/posec3d_ava.pth \\\n",
    "    --det-config mmaction2/demo/demo_configs/faster-rcnn_r50_fpn_2x_coco_infer.py \\\n",
    "    --det-checkpoint http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth \\\n",
    "    --pose-config mmaction2/demo/demo_configs/td-hm_hrnet-w32_8xb64-210e_coco-256x192_infer.py \\\n",
    "    --pose-checkpoint https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth \\\n",
    "    --skeleton-config mmaction2/configs/skeleton/posec3d/slowonly_r50_8xb16-u48-240e_ntu60-xsub-keypoint.py \\\n",
    "    --skeleton-checkpoint https://download.openmmlab.com/mmaction/skeleton/posec3d/posec3d_k400.pth \\\n",
    "    --use-skeleton-stdet \\\n",
    "    --use-skeleton-recog \\\n",
    "    --label-map-stdet mmaction2/tools/data/ava/label_map.txt \\\n",
    "    --label-map mmaction2/tools/data/kinetics/label_map_k400.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9339752f-e382-4ca7-baa8-e3f69978157b",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('<video width=50% controls autoplay loop><source src=\"mmaction2/demo/demo_skeleton.mp4\"></video>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3318144-3cc6-40ff-8c40-0d3afca7b00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('<video width=50% controls autoplay loop><source src=\"mmaction2/demo/demo_skeleton_vs_out.mp4\"></video>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5586624f-67a1-491c-93ae-4093f18e0ed6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## POSEC3D_AVA (Progress: evaluate ava_30f_50x_22.py model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30ab25d-2321-4755-aac4-664b6620469d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = 'mmaction2/configs/skeleton/posec3d/slowonly_r50_8xb16-u48-240e_ntu60-xsub-keypoint.py'\n",
    "checkpoint = 'https://download.openmmlab.com/mmaction/skeleton/posec3d/posec3d_ava.pth'\n",
    "device = 'cuda:0'\n",
    "\n",
    "# build the model from a config file and a checkpoint file\n",
    "model = init_recognizer(config, checkpoint, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b40fad-2961-4333-a1ec-4afc131fbd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61afaf4d-1eba-40de-b98c-26e75c8e747d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_recognizer(skeleton_config, skeleton_stdet_checkpoint, device)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f750f2db-b8ea-45cf-815d-9426cf48f0f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb2f4cc-2707-4468-a0de-d386c4d1157d",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_number = 1\n",
    "with open(\"mmaction2/tools/data/skeleton/label_map_ntu60.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        print(str(line_number) + \": \" + line)\n",
    "        line_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cefebab-e02f-4da9-8eb6-e4e2c5152f32",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Testing recognition + detection from python - SHOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fdbd98-a770-40fd-b627-d2077e002a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "# import argparse\n",
    "import copy as cp\n",
    "import tempfile\n",
    "import warnings\n",
    "\n",
    "import cv2\n",
    "import mmcv\n",
    "import mmengine\n",
    "import numpy as np\n",
    "import torch\n",
    "from mmengine import DictAction\n",
    "from mmengine.structures import InstanceData\n",
    "\n",
    "from mmaction.apis import (detection_inference, inference_recognizer,\n",
    "                           inference_skeleton, init_recognizer, pose_inference)\n",
    "from mmaction.registry import VISUALIZERS\n",
    "from mmaction.structures import ActionDataSample\n",
    "from mmaction.utils import frame_extract\n",
    "\n",
    "from mmdet.apis import init_detector\n",
    "# try:\n",
    "#     from mmdet.apis import init_detector\n",
    "# except (ImportError, ModuleNotFoundError):\n",
    "#     warnings.warn('Failed to import `init_detector` form `mmdet.apis`. '\n",
    "#                   'These apis are required in skeleton-based applications! ')\n",
    "\n",
    "import moviepy.editor as mpy\n",
    "# try:\n",
    "#     import moviepy.editor as mpy\n",
    "# except ImportError:\n",
    "#     raise ImportError('Please install moviepy to enable output file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101b3955-cc1d-46cf-854c-3a8e2e036cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "FONTFACE = cv2.FONT_HERSHEY_DUPLEX\n",
    "FONTSCALE = 1.25\n",
    "FONTCOLOR = (255, 255, 255)  # BGR, white\n",
    "MSGCOLOR = (128, 128, 128)  # BGR, gray\n",
    "THICKNESS = 2  # int\n",
    "LINETYPE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19edb240-0933-4b1f-90de-8dc24091bca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hex2color(h):\n",
    "    \"\"\"Convert the 6-digit hex string to tuple of 3 int value (RGB)\"\"\"\n",
    "    return (int(h[:2], 16), int(h[2:4], 16), int(h[4:], 16))\n",
    "\n",
    "\n",
    "PLATEBLUE = '03045e-023e8a-0077b6-0096c7-00b4d8-48cae4'\n",
    "PLATEBLUE = PLATEBLUE.split('-')\n",
    "PLATEBLUE = [hex2color(h) for h in PLATEBLUE]\n",
    "PLATEGREEN = '004b23-006400-007200-008000-38b000-70e000'\n",
    "PLATEGREEN = PLATEGREEN.split('-')\n",
    "PLATEGREEN = [hex2color(h) for h in PLATEGREEN]\n",
    "\n",
    "\n",
    "def visualize(pose_config,\n",
    "              frames,\n",
    "              annotations,\n",
    "              pose_data_samples,\n",
    "              action_result,\n",
    "              plate=PLATEBLUE,\n",
    "              max_num=5):\n",
    "    \"\"\"Visualize frames with predicted annotations.\n",
    "\n",
    "    Args:\n",
    "        frames (list[np.ndarray]): Frames for visualization, note that\n",
    "            len(frames) % len(annotations) should be 0.\n",
    "        annotations (list[list[tuple]]): The predicted spatio-temporal\n",
    "            detection results.\n",
    "        pose_data_samples (list[list[PoseDataSample]): The pose results.\n",
    "        action_result (str): The predicted action recognition results.\n",
    "        pose_model (nn.Module): The constructed pose model.\n",
    "        plate (str): The plate used for visualization. Default: PLATEBLUE.\n",
    "        max_num (int): Max number of labels to visualize for a person box.\n",
    "            Default: 5.\n",
    "\n",
    "    Returns:\n",
    "        list[np.ndarray]: Visualized frames.\n",
    "    \"\"\"\n",
    "\n",
    "    assert max_num + 1 <= len(plate)\n",
    "    frames_ = cp.deepcopy(frames)\n",
    "    frames_ = [mmcv.imconvert(f, 'bgr', 'rgb') for f in frames_]\n",
    "    nf, na = len(frames), len(annotations)\n",
    "    assert nf % na == 0\n",
    "    nfpa = len(frames) // len(annotations)\n",
    "    anno = None\n",
    "    h, w, _ = frames[0].shape\n",
    "    scale_ratio = np.array([w, h, w, h])\n",
    "\n",
    "    # add pose results\n",
    "    if pose_data_samples:\n",
    "        pose_config = mmengine.Config.fromfile(pose_config)\n",
    "        visualizer = VISUALIZERS.build(pose_config.visualizer | {'line_width':5, 'bbox_color':(101,193,255), 'radius': 8})  # https://mmpose.readthedocs.io/en/latest/api.html#mmpose.visualization.PoseLocalVisualizer\n",
    "        visualizer.set_dataset_meta(pose_data_samples[0].dataset_meta)\n",
    "        for i, (d, f) in enumerate(zip(pose_data_samples, frames_)):\n",
    "            visualizer.add_datasample(\n",
    "                'result',\n",
    "                f,\n",
    "                data_sample=d,\n",
    "                draw_gt=True,\n",
    "                draw_heatmap=False,\n",
    "                draw_bbox=False,\n",
    "                draw_pred=False,\n",
    "                show=False,\n",
    "                wait_time=0,\n",
    "                out_file=None,\n",
    "                kpt_thr=0.3)\n",
    "            frames_[i] = visualizer.get_image()\n",
    "            bahaya = ['baku tembak', 'berkelahi', 'kekerasan bersenjata', 'berbahaya']\n",
    "            cv2.putText(frames_[i], action_result, (10, 30), FONTFACE,\n",
    "                        FONTSCALE, FONTCOLOR, THICKNESS, LINETYPE)\n",
    "\n",
    "    for i in range(na):\n",
    "        anno = annotations[i]\n",
    "        if anno is None:\n",
    "            continue\n",
    "        for j in range(nfpa):\n",
    "            ind = i * nfpa + j\n",
    "            frame = frames_[ind]\n",
    "\n",
    "            # add action result for whole video\n",
    "            textsize = cv2.getTextSize(action_result, FONTFACE, FONTSCALE,\n",
    "                                       THICKNESS)[0]\n",
    "            textwidth = textsize[0]\n",
    "            location = (10, 14)\n",
    "            diag0 = (location[0] + textwidth, location[1] - 14)\n",
    "            diag1 = (location[0], location[1] + 22)\n",
    "            cv2.rectangle(frame, diag0, diag1, (0, 119, 182), -1)\n",
    "            bahaya = ['baku tembak', 'berkelahi', 'kekerasan bersenjata', 'berbahaya']\n",
    "            if action_result in bahaya:\n",
    "                cv2.putText(frame, action_result, (10, 30), FONTFACE, FONTSCALE,\n",
    "                            (255, 0, 0), THICKNESS, LINETYPE)\n",
    "            else:\n",
    "                cv2.putText(frame, action_result, (10, 30), FONTFACE, FONTSCALE,\n",
    "                            FONTCOLOR, THICKNESS, LINETYPE)\n",
    "\n",
    "            # add spatio-temporal action detection results\n",
    "            for ann in anno:\n",
    "                box = ann[0]\n",
    "                label = ann[1]\n",
    "                if not len(label):\n",
    "                    continue\n",
    "                score = ann[2]\n",
    "                box = (box * scale_ratio).astype(np.int64)\n",
    "                st, ed = tuple(box[:2]), tuple(box[2:])\n",
    "                if not pose_data_samples:\n",
    "                    cv2.rectangle(frame, st, ed, plate[0], 2)\n",
    "\n",
    "                for k, lb in enumerate(label):\n",
    "                    if k >= max_num:\n",
    "                        break\n",
    "                    text = abbrev(lb)\n",
    "                    text = ': '.join([text, f'{(score[k]*100):.1f}%'])\n",
    "                    location = (0 + st[0], 18 + k * 18 + st[1])\n",
    "                    textsize = cv2.getTextSize(text, FONTFACE, FONTSCALE,\n",
    "                                               THICKNESS)[0]\n",
    "                    textwidth = textsize[0]\n",
    "                    diag0 = (location[0] + textwidth, location[1] - 14)\n",
    "                    diag1 = (location[0], location[1] + 2)\n",
    "                    cv2.rectangle(frame, diag0, diag1, plate[k + 1], -1)\n",
    "                    bahaya = ['melempar', 'membidik (l. panjang)', 'membidik (l. pendek)', 'memukul', 'menendang', 'menusuk']\n",
    "                    if lb in bahaya:\n",
    "                        cv2.putText(frame, text, location, FONTFACE, FONTSCALE,\n",
    "                                    (255, 0, 0), THICKNESS, LINETYPE)\n",
    "                    else:\n",
    "                        cv2.putText(frame, text, location, FONTFACE, FONTSCALE,\n",
    "                                    FONTCOLOR, THICKNESS, LINETYPE)\n",
    "\n",
    "    return frames_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559e4271-6ca6-4808-ba08-13e75f8bbacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "video = '../cut/Action-Recognition.mp4'\n",
    "out_filename = 'data/Action-Recognition_out.mp4'\n",
    "\n",
    "# rgb-based spatio temporal detection config\n",
    "rgb_stdet_config =  \"mmaction2/configs/detection/slowonly/slowonly_kinetics400-pretrained-r101_8xb16-8x8x1-20e_ava21-rgb.py\"\n",
    "rgb_stdet_checkpoint = \"https://download.openmmlab.com/mmaction/detection/ava/slowonly_omnisource_pretrained_r101_8x8x1_20e_ava_rgb/slowonly_omnisource_pretrained_r101_8x8x1_20e_ava_rgb_20201217-16378594.pth\"\n",
    "\n",
    "# human detection config\n",
    "det_config = 'mmaction2/demo/demo_configs/faster-rcnn_r50_fpn_2x_coco_infer.py'\n",
    "det_checkpoint = 'http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth'\n",
    "det_score_thr = 0.9\n",
    "#det_cat_id = 0\n",
    "\n",
    "# pose estimation config\n",
    "pose_config = 'mmaction2/demo/demo_configs/td-hm_hrnet-w32_8xb64-210e_coco-256x192_infer.py'\n",
    "pose_checkpoint = 'https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth'\n",
    "\n",
    "# action classification config\n",
    "skeleton_config = \"mmaction2/configs/skeleton/posec3d/slowonly_r50_8xb16-u48-240e_ntu60-xsub-keypoint.py\"\n",
    "action_score_thr = 0.4\n",
    "# skeleton-based action recognition checkpoint\n",
    "skeleton_checkpoint = \"https://download.openmmlab.com/mmaction/skeleton/posec3d/posec3d_k400.pth\"\n",
    "# skeleton-based spatio temporal detection checkpoint\n",
    "skeleton_stdet_checkpoint = \"https://download.openmmlab.com/mmaction/skeleton/posec3d/posec3d_ava.pth\"\n",
    "\n",
    "# use skeleton-based method\n",
    "use_skeleton_stdet = True\n",
    "use_skeleton_recog = True\n",
    "\n",
    "label_map_stdet = \"mmaction2/tools/data/ava/label_map.txt\"\n",
    "label_map = \"mmaction2/tools/data/kinetics/label_map_k400.txt\"\n",
    "\n",
    "\n",
    "# rgb-based action recognition config\n",
    "rgb_config = \"configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x3-100e_kinetics400-rgb.py\"\n",
    "rgb_checkpoint = \"https://download.openmmlab.com/mmaction/recognition/tsn/tsn_r50_1x1x3_100e_kinetics400_rgb/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\"\n",
    "\n",
    "predict_stepsize = 24  # must even int, give out a spatio-temporal detection prediction per n frames\n",
    "output_stepsize = 1  # show one frame per n frames in the demo, we should have: predict_stepsize % output_stepsize == 0, speedUp/slowDown video output\n",
    "output_fps = 12  # the fps of demo video output, will speedUp/slowDown video output, must equal to (video_input_fps/output_stepsize) to get normal speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2366010a-56e0-4a49-893c-a5f935550634",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba36d91-4c56-4ef3-b964-6befd56eafaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_args():\n",
    "#     parser = argparse.ArgumentParser(description='MMAction2 demo')\n",
    "#     parser.add_argument(\n",
    "#         '--cfg-options',\n",
    "#         nargs='+',\n",
    "#         action=DictAction,\n",
    "#         default={},\n",
    "#         help='override some settings in the used config, the key-value pair '\n",
    "#         'in xxx=yyy format will be merged into config file. For example, '\n",
    "#         \"'--cfg-options model.backbone.depth=18 model.backbone.with_cp=True'\")\n",
    "#     args = parser.parse_args()\n",
    "#     return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491d431c-3b6f-41e8-b2b8-fa11964aa3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_label_map(file_path):\n",
    "    \"\"\"Load Label Map.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The file path of label map.\n",
    "\n",
    "    Returns:\n",
    "        dict: The label map (int -> label name).\n",
    "    \"\"\"\n",
    "    lines = open(file_path).readlines()\n",
    "    lines = [x.strip().split(': ') for x in lines]\n",
    "    return {int(x[0]): x[1] for x in lines}\n",
    "\n",
    "\n",
    "def abbrev(name):\n",
    "    \"\"\"Get the abbreviation of label name:\n",
    "\n",
    "    'take (an object) from (a person)' -> 'take ... from ...'\n",
    "    \"\"\"\n",
    "    while name.find('(') != -1:\n",
    "        st, ed = name.find('('), name.find(')')\n",
    "        name = name[:st] + '...' + name[ed + 1:]\n",
    "    return name\n",
    "\n",
    "\n",
    "def pack_result(human_detection, result, img_h, img_w):\n",
    "    \"\"\"Short summary.\n",
    "\n",
    "    Args:\n",
    "        human_detection (np.ndarray): Human detection result.\n",
    "        result (type): The predicted label of each human proposal.\n",
    "        img_h (int): The image height.\n",
    "        img_w (int): The image width.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple of human proposal, label name and label score.\n",
    "    \"\"\"\n",
    "    human_detection[:, 0::2] /= img_w\n",
    "    human_detection[:, 1::2] /= img_h\n",
    "    results = []\n",
    "    if result is None:\n",
    "        return None\n",
    "    for prop, res in zip(human_detection, result):\n",
    "        res.sort(key=lambda x: -x[1])\n",
    "        results.append(\n",
    "            (prop.data.cpu().numpy(), [x[0] for x in res], [x[1]\n",
    "                                                            for x in res]))\n",
    "    return results\n",
    "\n",
    "\n",
    "def expand_bbox(bbox, h, w, ratio=1.25):\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    center_x = (x1 + x2) // 2\n",
    "    center_y = (y1 + y2) // 2\n",
    "    width = x2 - x1\n",
    "    height = y2 - y1\n",
    "\n",
    "    square_l = max(width, height)\n",
    "    new_width = new_height = square_l * ratio\n",
    "\n",
    "    new_x1 = max(0, int(center_x - new_width / 2))\n",
    "    new_x2 = min(int(center_x + new_width / 2), w)\n",
    "    new_y1 = max(0, int(center_y - new_height / 2))\n",
    "    new_y2 = min(int(center_y + new_height / 2), h)\n",
    "    return (new_x1, new_y1, new_x2, new_y2)\n",
    "\n",
    "\n",
    "def cal_iou(box1, box2):\n",
    "    xmin1, ymin1, xmax1, ymax1 = box1\n",
    "    xmin2, ymin2, xmax2, ymax2 = box2\n",
    "\n",
    "    s1 = (xmax1 - xmin1) * (ymax1 - ymin1)\n",
    "    s2 = (xmax2 - xmin2) * (ymax2 - ymin2)\n",
    "\n",
    "    xmin = max(xmin1, xmin2)\n",
    "    ymin = max(ymin1, ymin2)\n",
    "    xmax = min(xmax1, xmax2)\n",
    "    ymax = min(ymax1, ymax2)\n",
    "\n",
    "    w = max(0, xmax - xmin)\n",
    "    h = max(0, ymax - ymin)\n",
    "    intersect = w * h\n",
    "    union = s1 + s2 - intersect\n",
    "    iou = intersect / union\n",
    "\n",
    "    return iou\n",
    "\n",
    "\n",
    "def skeleton_based_action_recognition(skeleton_config, skeleton_checkpoint, device, label_map, pose_results, h, w):\n",
    "    label_map = [x.strip() for x in open(label_map).readlines()]\n",
    "    num_class = len(label_map)\n",
    "\n",
    "    skeleton_config = mmengine.Config.fromfile(skeleton_config)\n",
    "    skeleton_config.model.cls_head.num_classes = num_class  # for K400 dataset\n",
    "\n",
    "    skeleton_model = init_recognizer(\n",
    "        skeleton_config, skeleton_checkpoint, device=device)\n",
    "    result = inference_skeleton(skeleton_model, pose_results, (h, w))\n",
    "    action_idx = result.pred_score.argmax().item()\n",
    "    return label_map[action_idx]\n",
    "\n",
    "\n",
    "def rgb_based_action_recognition(rgb_config, rgb_checkpoint, device, video, label_map):\n",
    "    rgb_config = mmengine.Config.fromfile(rgb_config)\n",
    "    rgb_config.model.backbone.pretrained = None\n",
    "    rgb_model = init_recognizer(rgb_config, rgb_checkpoint, device)\n",
    "    action_results = inference_recognizer(rgb_model, video)\n",
    "    rgb_action_result = action_results.pred_score.argmax().item()\n",
    "    label_map = [x.strip() for x in open(label_map).readlines()]\n",
    "    return label_map[rgb_action_result]\n",
    "\n",
    "def skeleton_based_stdet(predict_stepsize, skeleton_config, skeleton_stdet_checkpoint, device, action_score_thr, label_map, human_detections, pose_results,\n",
    "                         num_frame, clip_len, frame_interval, h, w):\n",
    "    window_size = clip_len * frame_interval\n",
    "    assert clip_len % 2 == 0, 'We would like to have an even clip_len'\n",
    "    timestamps = np.arange(window_size // 2, num_frame + 1 - window_size // 2,\n",
    "                           predict_stepsize)\n",
    "\n",
    "    skeleton_config = mmengine.Config.fromfile(skeleton_config)\n",
    "    num_class = max(label_map.keys()) + 1  # for AVA dataset (80 + 1), for CIIS dataset (9 + 1) == len(label_map)\n",
    "    skeleton_config.model.cls_head.num_classes = num_class\n",
    "    skeleton_stdet_model = init_recognizer(skeleton_config,\n",
    "                                           skeleton_stdet_checkpoint,\n",
    "                                           device)\n",
    "\n",
    "    skeleton_stdet_model.eval()\n",
    "    skeleton_predictions = []\n",
    "\n",
    "    print('Performing SpatioTemporal Action Detection for each clip')\n",
    "    prog_bar = mmengine.ProgressBar(len(timestamps))\n",
    "    for timestamp in timestamps:\n",
    "        proposal = human_detections[timestamp - 1]\n",
    "        if proposal.shape[0] == 0:  # no people detected\n",
    "            skeleton_predictions.append(None)\n",
    "            continue\n",
    "\n",
    "        start_frame = timestamp - (clip_len // 2 - 1) * frame_interval\n",
    "        frame_inds = start_frame + np.arange(0, window_size, frame_interval)\n",
    "        frame_inds = list(frame_inds - 1)\n",
    "        num_frame = len(frame_inds)  # 30\n",
    "\n",
    "        pose_result = [pose_results[ind] for ind in frame_inds]\n",
    "\n",
    "        skeleton_prediction = []\n",
    "        for i in range(proposal.shape[0]):  # num_person\n",
    "            skeleton_prediction.append([])\n",
    "\n",
    "            fake_anno = dict(\n",
    "                frame_dict='',\n",
    "                label=-1,\n",
    "                img_shape=(h, w),\n",
    "                origin_shape=(h, w),\n",
    "                start_index=0,\n",
    "                modality='Pose',\n",
    "                total_frames=num_frame)\n",
    "            num_person = 1\n",
    "\n",
    "            num_keypoint = 17\n",
    "            keypoint = np.zeros(\n",
    "                (num_person, num_frame, num_keypoint, 2))  # M T V 2\n",
    "            keypoint_score = np.zeros(\n",
    "                (num_person, num_frame, num_keypoint))  # M T V\n",
    "\n",
    "            # pose matching\n",
    "            person_bbox = proposal[i][:4]\n",
    "            area = expand_bbox(person_bbox, h, w)\n",
    "\n",
    "            for j, poses in enumerate(pose_result):  # num_frame\n",
    "                max_iou = float('-inf')\n",
    "                index = -1\n",
    "                if len(poses['keypoints']) == 0:\n",
    "                    continue\n",
    "                for k, bbox in enumerate(poses['bboxes']):  # num_person\n",
    "                    iou = cal_iou(bbox, area)\n",
    "                    if max_iou < iou:  # if isBelong\n",
    "                        index = k\n",
    "                        max_iou = iou\n",
    "                keypoint[0, j] = poses['keypoints'][index]\n",
    "                keypoint_score[0, j] = poses['keypoint_scores'][index]\n",
    "\n",
    "            fake_anno['keypoint'] = keypoint\n",
    "            fake_anno['keypoint_score'] = keypoint_score\n",
    "\n",
    "            output = inference_recognizer(skeleton_stdet_model, fake_anno)\n",
    "            # for multi-label recognition\n",
    "            score = output.pred_score.tolist()\n",
    "            for k in range(len(score)):  # 81\n",
    "                if k not in label_map:\n",
    "                    continue\n",
    "                if score[k] > action_score_thr:\n",
    "                    skeleton_prediction[i].append((label_map[k], score[k]))\n",
    "                    \n",
    "            # crop the image -> resize -> extract pose -> as input for poseC3D\n",
    "\n",
    "        skeleton_predictions.append(skeleton_prediction)\n",
    "        prog_bar.update()\n",
    "\n",
    "    return timestamps, skeleton_predictions\n",
    "\n",
    "\n",
    "def rgb_based_stdet(rgb_stdet_config, rgb_stdet_checkpoint, device, action_score_thr, predict_stepsize, frames, label_map, human_detections, w, h, new_w,\n",
    "                    new_h, w_ratio, h_ratio):\n",
    "\n",
    "    rgb_stdet_config = mmengine.Config.fromfile(rgb_stdet_config)\n",
    "    # rgb_stdet_config.merge_from_dict(args.cfg_options)\n",
    "\n",
    "    val_pipeline = rgb_stdet_config.val_pipeline\n",
    "    sampler = [x for x in val_pipeline if x['type'] == 'SampleAVAFrames'][0]\n",
    "    clip_len, frame_interval = sampler['clip_len'], sampler['frame_interval']\n",
    "    assert clip_len % 2 == 0, 'We would like to have an even clip_len'\n",
    "\n",
    "    window_size = clip_len * frame_interval\n",
    "    num_frame = len(frames)\n",
    "    # Note that it's 1 based here\n",
    "    timestamps = np.arange(window_size // 2, num_frame + 1 - window_size // 2,\n",
    "                           predict_stepsize)\n",
    "\n",
    "    # Get img_norm_cfg\n",
    "    img_norm_cfg = dict(\n",
    "        mean=np.array(rgb_stdet_config.model.data_preprocessor.mean),\n",
    "        std=np.array(rgb_stdet_config.model.data_preprocessor.std),\n",
    "        to_rgb=False)\n",
    "\n",
    "    # Build STDET model\n",
    "    try:\n",
    "        # In our spatiotemporal detection demo, different actions should have\n",
    "        # the same number of bboxes.\n",
    "        rgb_stdet_config['model']['test_cfg']['rcnn'] = dict(action_thr=0)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    rgb_stdet_config.model.backbone.pretrained = None\n",
    "    rgb_stdet_model = init_detector(\n",
    "        rgb_stdet_config, rgb_stdet_checkpoint, device=device)\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    print('Performing SpatioTemporal Action Detection for each clip')\n",
    "    prog_bar = mmengine.ProgressBar(len(timestamps))\n",
    "    # for timestamp, proposal in zip(timestamps, human_detections):\n",
    "    for timestamp in timestamps:\n",
    "        proposal = human_detections[timestamp - 1]\n",
    "        if proposal.shape[0] == 0:\n",
    "            predictions.append(None)\n",
    "            continue\n",
    "\n",
    "        start_frame = timestamp - (clip_len // 2 - 1) * frame_interval\n",
    "        frame_inds = start_frame + np.arange(0, window_size, frame_interval)\n",
    "        frame_inds = list(frame_inds - 1)\n",
    "\n",
    "        imgs = [frames[ind].astype(np.float32) for ind in frame_inds]\n",
    "        _ = [mmcv.imnormalize_(img, **img_norm_cfg) for img in imgs]\n",
    "        # THWC -> CTHW -> 1CTHW\n",
    "        input_array = np.stack(imgs).transpose((3, 0, 1, 2))[np.newaxis]\n",
    "        input_tensor = torch.from_numpy(input_array).to(device)\n",
    "\n",
    "        datasample = ActionDataSample()\n",
    "        datasample.proposals = InstanceData(bboxes=proposal)\n",
    "        datasample.set_metainfo(dict(img_shape=(new_h, new_w)))\n",
    "        with torch.no_grad():\n",
    "            result = rgb_stdet_model(\n",
    "                input_tensor, [datasample], mode='predict')\n",
    "            scores = result[0].pred_instances.scores\n",
    "            prediction = []\n",
    "            # N proposals\n",
    "            for i in range(proposal.shape[0]):\n",
    "                prediction.append([])\n",
    "            # Perform action score thr\n",
    "            for i in range(scores.shape[1]):\n",
    "                if i not in label_map:\n",
    "                    continue\n",
    "                for j in range(proposal.shape[0]):\n",
    "                    if scores[j, i] > action_score_thr:\n",
    "                        prediction[j].append((label_map[i], scores[j,\n",
    "                                                                   i].item()))\n",
    "            predictions.append(prediction)\n",
    "        prog_bar.update()\n",
    "\n",
    "    return timestamps, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63748bc-e8d7-4a87-87d8-29337e5a38c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#args = parse_args()\n",
    "tmp_dir = tempfile.TemporaryDirectory()\n",
    "frame_paths, original_frames = frame_extract(\n",
    "    video, 720, out_dir=tmp_dir.name)\n",
    "num_frame = len(frame_paths)\n",
    "h, w, _ = original_frames[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5604e3b1-b2e9-41c8-bc88-26375735eca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Human detection results and pose results\n",
    "human_detections, _ = detection_inference(\n",
    "    det_config,\n",
    "    det_checkpoint,\n",
    "    frame_paths,\n",
    "    det_score_thr,\n",
    "    device=device)\n",
    "torch.cuda.empty_cache()\n",
    "pose_datasample = None\n",
    "if use_skeleton_recog or use_skeleton_stdet:\n",
    "    pose_results, pose_datasample = pose_inference(\n",
    "        pose_config,\n",
    "        pose_checkpoint,\n",
    "        frame_paths,\n",
    "        human_detections,\n",
    "        device=device)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac991f52-b652-42eb-a2c3-bac13e5994f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('human_detections.txt','w') as data:  \n",
    "#       data.write(str(human_detections))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b863e4a-35ad-43f5-ba37-e4daa87bfcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('pose_results.txt','w') as data:  \n",
    "#       data.write(str(pose_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47f8b12-53fb-47df-9e47-20307e1008c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('pose_datasample.txt','w') as data:  \n",
    "#       data.write(str(pose_datasample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2412293-c3fb-4858-95c2-9da9ff3df249",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(original_frames).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e11d874-d369-469a-a682-4e791a710e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize frames to shortside 256\n",
    "# new_w, new_h = mmcv.rescale_size((w, h), (256, np.Inf))\n",
    "new_w, new_h = w, h\n",
    "# frames = [mmcv.imresize(img, (new_w, new_h)) for img in original_frames]\n",
    "frames = original_frames\n",
    "w_ratio, h_ratio = new_w / w, new_h / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c011a8-ff51-445c-a9e9-aed669959dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spatio-temporal detection label_map\n",
    "stdet_label_map = load_label_map(label_map_stdet)\n",
    "rgb_stdet_config = mmengine.Config.fromfile(rgb_stdet_config)\n",
    "#rgb_stdet_config.merge_from_dict(args.cfg_options)\n",
    "try:\n",
    "    if rgb_stdet_config['data']['train']['custom_classes'] is not None:\n",
    "        stdet_label_map = {\n",
    "            id + 1: stdet_label_map[cls]\n",
    "            for id, cls in enumerate(rgb_stdet_config['data']['train']\n",
    "                                     ['custom_classes'])\n",
    "        }\n",
    "except KeyError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ad613f-2022-4fd8-adca-1713d69008a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_result = None\n",
    "if use_skeleton_recog:\n",
    "    print('Use skeleton-based recognition')\n",
    "    action_result = skeleton_based_action_recognition(\n",
    "        skeleton_config, skeleton_checkpoint, device, label_map, pose_results, h, w)\n",
    "else:\n",
    "    print('Use rgb-based recognition')\n",
    "    action_result = rgb_based_action_recognition(rgb_config, rgb_checkpoint, device, video, label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedba28d-a65b-47f5-833b-9083d8fbcbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdet_preds = None\n",
    "if use_skeleton_stdet:\n",
    "    print('Use skeleton-based SpatioTemporal Action Detection')\n",
    "    # clip_len, frame_interval = 30, 1\n",
    "    clip_len, frame_interval = predict_stepsize, 1\n",
    "    timestamps, stdet_preds = skeleton_based_stdet(predict_stepsize,\n",
    "                                                   skeleton_config,\n",
    "                                                   skeleton_stdet_checkpoint,\n",
    "                                                   device,\n",
    "                                                   action_score_thr,\n",
    "                                                   stdet_label_map,\n",
    "                                                   human_detections,\n",
    "                                                   pose_results, num_frame,\n",
    "                                                   clip_len,\n",
    "                                                   frame_interval, h, w)\n",
    "    for i in range(len(human_detections)):\n",
    "        det = human_detections[i]\n",
    "        det[:, 0:4:2] *= w_ratio\n",
    "        det[:, 1:4:2] *= h_ratio\n",
    "        human_detections[i] = torch.from_numpy(det[:, :4]).to(device)\n",
    "\n",
    "else:\n",
    "    print('Use rgb-based SpatioTemporal Action Detection')\n",
    "    for i in range(len(human_detections)):\n",
    "        det = human_detections[i]\n",
    "        det[:, 0:4:2] *= w_ratio\n",
    "        det[:, 1:4:2] *= h_ratio\n",
    "        human_detections[i] = torch.from_numpy(det[:, :4]).to(device)\n",
    "    timestamps, stdet_preds = rgb_based_stdet(rgb_stdet_config,\n",
    "                                              rgb_stdet_checkpoint,\n",
    "                                              device,\n",
    "                                              action_score_thr,\n",
    "                                              predict_stepsize,\n",
    "                                              frames,\n",
    "                                              stdet_label_map,\n",
    "                                              human_detections, w, h,\n",
    "                                              new_w, new_h, w_ratio,\n",
    "                                              h_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35678108-efc2-4aba-9894-6427846bd066",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdet_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871efa5d-d41c-4c47-a953-bca22f7ad60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdet_preds = [[[('berdiri', 0.7004162073135376), ('membidik (l. panjang)', 0.7004162073135376)], [('berdiri', 0.7004162073135376)]],\n",
    " [[('membidik (l. panjang)', 0.9829767346382141)], [('merayap', 0.468430757522583)]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e0de0b-922b-4016-947c-29a70274487e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdet_results = []\n",
    "for timestamp, prediction in zip(timestamps, stdet_preds):\n",
    "    human_detection = human_detections[timestamp - 1]\n",
    "    stdet_results.append(\n",
    "        pack_result(human_detection, prediction, new_h, new_w))\n",
    "\n",
    "def dense_timestamps(timestamps, n):\n",
    "    \"\"\"Make it nx frames.\"\"\"\n",
    "    old_frame_interval = (timestamps[1] - timestamps[0])\n",
    "    start = timestamps[0] - old_frame_interval / n * (n - 1) / 2\n",
    "    new_frame_inds = np.arange(\n",
    "        len(timestamps) * n) * old_frame_interval / n + start\n",
    "    return new_frame_inds.astype(np.int64)\n",
    "\n",
    "dense_n = int(predict_stepsize / output_stepsize)\n",
    "# output_timestamps = dense_timestamps(timestamps, dense_n)\n",
    "output_timestamps = dense_timestamps(timestamps, dense_n) + 1\n",
    "frames = [\n",
    "    cv2.imread(frame_paths[timestamp - 1])\n",
    "    # cv2.imread(\"../854x480-white-solid-color-background.jpg\")\n",
    "    for timestamp in output_timestamps\n",
    "]\n",
    "\n",
    "if use_skeleton_recog or use_skeleton_stdet:\n",
    "    pose_datasample = [\n",
    "        pose_datasample[timestamp - 1] for timestamp in output_timestamps\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9ad532-bcea-4fe7-b27e-eb029bcfb430",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output_timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcf0f4a-2e5e-486e-ae69-ba4179e012d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_frames = visualize(pose_config, frames, stdet_results, pose_datasample,\n",
    "                       'kekerasan bersenjata')\n",
    "vid = mpy.ImageSequenceClip(vis_frames, fps=output_fps)\n",
    "vid.write_videofile(out_filename)\n",
    "tmp_dir.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4d0404-b751-44c5-9a56-144e51b8f366",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdet_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07438bd9-4716-40c3-b58d-67118ee350a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_frames = visualize(pose_config, frames, [None], pose_datasample,\n",
    "                       None)\n",
    "vid = mpy.ImageSequenceClip(vis_frames, fps=output_fps)\n",
    "vid.write_videofile(\"dji_fly_20240216_153920_12_1708073394745_video_720p_30r_10s_2_skeleton_no-anno.mp4\")\n",
    "tmp_dir.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfdb18f-96ef-42c9-9844-ecedfd363b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('<video width=50% controls autoplay loop><source src=\"data/DJI_0011_720p_30r_10s_1.MP4\"></video>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e735be-0531-4bcc-b10e-dfadde74c7d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "HTML('<video width=50% controls autoplay loop><source src=\"dji_fly_20240216_153920_12_1708073394745_video_720p_30r_10s_2_out_bold.mp4\"></video>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071649fd-78ca-4884-8410-46776a80e17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stdet_results.txt','w') as data:  \n",
    "      data.write(str(stdet_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dc7f72-76bc-4edf-9240-c4ebfa143b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_result = \"kekerasan bersenjata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8387f557-8e32-4a54-bd02-100031ce6f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdet_results = [[(\n",
    " [0.48046237, 0.2826953 , 0.58922434, 0.9219833 ],\n",
    " ['berjalan'], [0.6516427993774414])],\n",
    " \n",
    " [(\n",
    " [0.49903056, 0.28123865, 0.6010084 , 0.9144287 ],\n",
    " [], [])],\n",
    " \n",
    " [(\n",
    " [0.53933424, 0.27578855, 0.6247528 , 0.9082195 ],\n",
    " ['berjalan'], [0.7929941415786743])],\n",
    " \n",
    " [(\n",
    " [0.5425681 , 0.2608861 , 0.65210605, 0.92551756],\n",
    " [], [])],\n",
    " \n",
    " [(\n",
    " [0.5370982 , 0.29594222, 0.7014558 , 0.92983055],\n",
    " ['berjalan'], [0.5045342445373535])],\n",
    " \n",
    " [(\n",
    " [0.53738445, 0.34534463, 0.7065569 , 0.9449916 ],\n",
    " ['berjalan'], [0.7290676832199097]), (\n",
    " [1.9916268e-04, 5.0976318e-01, 6.5174706e-02, 6.9015694e-01],\n",
    " [], [])],\n",
    " \n",
    " [(\n",
    " [0.5268151, 0.3649384, 0.6689306, 0.9752418],\n",
    " ['berjalan'], [0.7396131157875061]), (\n",
    " [0.        , 0.17990077, 0.09557094, 0.67891884],\n",
    " ['berjalan'], [0.40284472703933716])],\n",
    " \n",
    " [(\n",
    " [0.5039204, 0.3866339, 0.6685614, 0.9840485],\n",
    " ['berjalan'], [0.4768950641155243]), (\n",
    " [0.        , 0.16926248, 0.159517  , 0.6903268 ],\n",
    " ['berjalan'], [0.6423807144165039])],\n",
    " \n",
    " [(\n",
    " [0.00077708, 0.18208253, 0.17196336, 0.75154096],\n",
    " ['berjalan'], [0.8692366480827332]), (\n",
    " [0.51862544, 0.4229225 , 0.70295805, 0.9966346 ],\n",
    " ['berjalan'], [0.6290099024772644])],\n",
    " \n",
    " [(\n",
    " [0.02083336, 0.20106731, 0.17874175, 0.7735189 ],\n",
    " ['berjalan'], [0.8075441718101501]), (\n",
    " [0.5263421 , 0.42615634, 0.73938084, 0.99915487],\n",
    " ['berjalan'], [0.7030906677246094])],\n",
    " \n",
    " [(\n",
    " [0.07839173, 0.21341152, 0.24439614, 0.74690765],\n",
    " [], []), (\n",
    " [0.51992446, 0.4610273 , 0.7296746 , 0.99751836],\n",
    " [], [])],\n",
    " \n",
    " [(\n",
    " [0.13035715, 0.19829439, 0.2630955 , 0.73814696],\n",
    " ['berjalan'], [0.47298890352249146]), (\n",
    " [0.5037779 , 0.50203604, 0.69085234, 0.9969859 ],\n",
    " [], [])],\n",
    " \n",
    " [(\n",
    " [0.18892232, 0.21443586, 0.3303552 , 0.7353606 ],\n",
    " ['berjalan'], [0.4672069251537323]), (\n",
    " [0.56789285, 0.49028873, 0.6785473 , 0.9974631 ],\n",
    " [], [])],\n",
    " \n",
    " [(\n",
    " [0.25530723, 0.19707297, 0.3888575 , 0.7250715 ],\n",
    " [], []), (\n",
    " [0.57205224, 0.4528523 , 0.6822692 , 0.999494  ],\n",
    " [], [])],\n",
    " \n",
    " [(\n",
    " [0.31346345, 0.18880942, 0.4064358 , 0.6934109 ],\n",
    " ['berjalan'], [0.45306482911109924]), (\n",
    " [0.56473345, 0.43473983, 0.67779756, 0.9953781 ],\n",
    " [], [])],\n",
    " \n",
    " [(\n",
    " [0.53344506, 0.4817697 , 0.67519224, 0.99788755],\n",
    " [], []), (\n",
    " [0.36238623, 0.19106022, 0.47143894, 0.6778218 ],\n",
    " ['berjalan'], [0.6831617951393127])],\n",
    " \n",
    " [(\n",
    " [0.50663114, 0.4956691 , 0.67674255, 0.99168444],\n",
    " [], []), (\n",
    " [0.3939347, 0.1849217, 0.5185849, 0.6727996],\n",
    " ['berjalan'], [0.40376487374305725])],\n",
    " \n",
    " [(\n",
    " [0.42016324, 0.1896712 , 0.5370663 , 0.6715273 ],\n",
    " [], []), (\n",
    " [0.4967625 , 0.47799036, 0.6643044 , 0.9942598 ],\n",
    " [], [])],\n",
    " \n",
    " [(\n",
    " [0.45843035, 0.19253796, 0.56786835, 0.65041745],\n",
    " [], []), (\n",
    " [0.46810722, 0.46758887, 0.63295364, 0.98916584],\n",
    " [], [])],\n",
    " \n",
    " [(\n",
    " [0.40261754, 0.45769238, 0.5879931 , 0.9974334 ],\n",
    " [], []), (\n",
    " [0.50584674, 0.18837103, 0.6149382 , 0.71526796],\n",
    " [], [])],\n",
    " \n",
    " [(\n",
    " [0.36209267, 0.4430162 , 0.5564627 , 0.99690145],\n",
    " [], []), (\n",
    " [0.5315389 , 0.1913318 , 0.63186777, 0.7139902 ],\n",
    " ['membidik'], [0.6690452694892883])],\n",
    " \n",
    " [(\n",
    " [0.53467923, 0.19892102, 0.6299276 , 0.719789  ],\n",
    " ['membidik'], [0.6169623732566833]), (\n",
    " [0.32375905, 0.4255201 , 0.5063102 , 0.9995426 ],\n",
    " [], [])],\n",
    " \n",
    " [(\n",
    " [0.26135835, 0.36958945, 0.4584922 , 0.9979094 ],\n",
    " [], []), (\n",
    " [0.5232837 , 0.19389734, 0.6293985 , 0.7309556 ],\n",
    " ['membidik'], [0.45500636100769043])],\n",
    " \n",
    " [(\n",
    " [0.20032604, 0.41384354, 0.39622757, 0.9956406 ],\n",
    " [], []), (\n",
    " [0.5194849 , 0.20094734, 0.63264114, 0.7338626 ],\n",
    " ['membidik'], [0.5269035696983337])],\n",
    " \n",
    " [(\n",
    " [0.13076015, 0.38753268, 0.31265774, 0.9927651 ],\n",
    " [], [0.40346986055374146]), (\n",
    " [0.51092196, 0.20948297, 0.62921625, 0.7336499 ],\n",
    " ['membidik'], [0.708939254283905])],\n",
    " \n",
    " [(\n",
    " [0.4996279 , 0.22167045, 0.62460417, 0.7638106 ],\n",
    " ['membidik'], [0.4154243469238281]), (\n",
    " [0.06867792, 0.43370816, 0.25127035, 0.99140316],\n",
    " [], [])],\n",
    " \n",
    " [(\n",
    " [0.49055672, 0.21901229, 0.6181816 , 0.76330274],\n",
    " ['membidik', 'berjalan'], [0.7491381168365479, 0.5671381168365479]), (\n",
    " [0.00292282, 0.3823529 , 0.18002348, 0.98933464],\n",
    " [], [])],\n",
    " \n",
    " [(\n",
    " [0.46649653, 0.23537844, 0.60467714, 0.76583743],\n",
    " ['membidik', 'berjalan'], [0.4681381168365479, 0.68962162733078]), (\n",
    " [4.2701719e-04, 3.7130839e-01, 1.1861143e-01, 1.0000000e+00],\n",
    " ['sit'], [0.8068248629570007])],\n",
    " \n",
    " [(\n",
    " [0.45237088, 0.25236842, 0.59199196, 0.84444666],\n",
    " ['membidik'], [0.5885922312736511]), (\n",
    " [3.6440234e-04, 5.3448135e-01, 8.6183235e-02, 9.9572343e-01],\n",
    " [], [])],\n",
    " \n",
    " [(\n",
    " [0.42831546, 0.2643174 , 0.58030695, 0.83967954],\n",
    " ['membidik'], [0.6781381168365479])]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a1cef7-4b16-4490-8d65-900581e836f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_extract(\n",
    "    \"data/DJI_0013_12r_10s_2_heatmap.MP4\", out_dir=\"../../skripsi/extracted/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38be2c34-5494-4563-a349-8a9aaa0026e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dir.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bd9b96-170a-4ae5-86bb-15c95c62f61d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pose_datasample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91281c3-c2cc-4871-a4ce-640cd0688fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(stdet_preds[0][0][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac3f15a-ac7b-4a3c-a4c6-217079360df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdet_preds[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0951f3ec-0243-4b64-8da3-2c1494e51e29",
   "metadata": {},
   "source": [
    "## Testing recognition + detection from python - DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cc52445-850c-427d-9925-2320af1eace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "# import argparse\n",
    "import copy as cp\n",
    "import tempfile\n",
    "import warnings\n",
    "\n",
    "import cv2\n",
    "import mmcv\n",
    "import mmengine\n",
    "import numpy as np\n",
    "import torch\n",
    "from mmengine import DictAction\n",
    "from mmengine.structures import InstanceData\n",
    "\n",
    "from mmaction.apis import (detection_inference, inference_recognizer,\n",
    "                           inference_skeleton, init_recognizer, pose_inference)\n",
    "from mmaction.registry import VISUALIZERS\n",
    "from mmaction.structures import ActionDataSample\n",
    "from mmaction.utils import frame_extract\n",
    "\n",
    "from mmdet.apis import init_detector\n",
    "# try:\n",
    "#     from mmdet.apis import init_detector\n",
    "# except (ImportError, ModuleNotFoundError):\n",
    "#     warnings.warn('Failed to import `init_detector` form `mmdet.apis`. '\n",
    "#                   'These apis are required in skeleton-based applications! ')\n",
    "\n",
    "import moviepy.editor as mpy\n",
    "# try:\n",
    "#     import moviepy.editor as mpy\n",
    "# except ImportError:\n",
    "#     raise ImportError('Please install moviepy to enable output file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bba4f40d-f9a6-4da0-8764-1ec32ae12c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "FONTFACE = cv2.FONT_HERSHEY_DUPLEX\n",
    "FONTSCALE = 1.25\n",
    "FONTCOLOR = (255, 255, 255)  # BGR, white\n",
    "MSGCOLOR = (128, 128, 128)  # BGR, gray\n",
    "THICKNESS = 2  # int\n",
    "LINETYPE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4145efa8-5d3b-4b3d-a4ad-0b4c578c3cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hex2color(h):\n",
    "    \"\"\"Convert the 6-digit hex string to tuple of 3 int value (RGB)\"\"\"\n",
    "    return (int(h[:2], 16), int(h[2:4], 16), int(h[4:], 16))\n",
    "\n",
    "\n",
    "PLATEBLUE = '03045e-023e8a-0077b6-0096c7-00b4d8-48cae4'\n",
    "PLATEBLUE = PLATEBLUE.split('-')\n",
    "PLATEBLUE = [hex2color(h) for h in PLATEBLUE]\n",
    "PLATEGREEN = '004b23-006400-007200-008000-38b000-70e000'\n",
    "PLATEGREEN = PLATEGREEN.split('-')\n",
    "PLATEGREEN = [hex2color(h) for h in PLATEGREEN]\n",
    "\n",
    "\n",
    "def visualize(pose_config,\n",
    "              frames,\n",
    "              annotations,\n",
    "              pose_data_samples,\n",
    "              action_result,\n",
    "              plate=PLATEBLUE,\n",
    "              max_num=5):\n",
    "    \"\"\"Visualize frames with predicted annotations.\n",
    "\n",
    "    Args:\n",
    "        frames (list[np.ndarray]): Frames for visualization, note that\n",
    "            len(frames) % len(annotations) should be 0.\n",
    "        annotations (list[list[tuple]]): The predicted spatio-temporal\n",
    "            detection results.\n",
    "        pose_data_samples (list[list[PoseDataSample]): The pose results.\n",
    "        action_result (str): The predicted action recognition results.\n",
    "        pose_model (nn.Module): The constructed pose model.\n",
    "        plate (str): The plate used for visualization. Default: PLATEBLUE.\n",
    "        max_num (int): Max number of labels to visualize for a person box.\n",
    "            Default: 5.\n",
    "\n",
    "    Returns:\n",
    "        list[np.ndarray]: Visualized frames.\n",
    "    \"\"\"\n",
    "\n",
    "    act_res = 'tidak berbahaya'\n",
    "    \n",
    "    assert max_num + 1 <= len(plate)\n",
    "    frames_ = cp.deepcopy(frames)\n",
    "    frames_ = [mmcv.imconvert(f, 'bgr', 'rgb') for f in frames_]\n",
    "    nf, na = len(frames), len(annotations)\n",
    "    assert nf % na == 0\n",
    "    nfpa = len(frames) // len(annotations)\n",
    "    anno = None\n",
    "    h, w, _ = frames[0].shape\n",
    "    scale_ratio = np.array([w, h, w, h])\n",
    "\n",
    "    # add pose results\n",
    "    if pose_data_samples:\n",
    "        pose_config = mmengine.Config.fromfile(pose_config)\n",
    "        visualizer = VISUALIZERS.build(pose_config.visualizer | {'line_width':5, 'bbox_color':(101,193,255), 'radius': 8})  # https://mmpose.readthedocs.io/en/latest/api.html#mmpose.visualization.PoseLocalVisualizer\n",
    "        visualizer.set_dataset_meta(pose_data_samples[0].dataset_meta)\n",
    "        for i, (d, f) in enumerate(zip(pose_data_samples, frames_)):\n",
    "            visualizer.add_datasample(\n",
    "                'result',\n",
    "                f,\n",
    "                data_sample=d,\n",
    "                draw_gt=False,\n",
    "                draw_heatmap=False,\n",
    "                draw_bbox=True,\n",
    "                draw_pred=True,\n",
    "                show=False,\n",
    "                wait_time=0,\n",
    "                out_file=None,\n",
    "                kpt_thr=0.3)\n",
    "            frames_[i] = visualizer.get_image()\n",
    "            # bahaya = ['kekerasan bersenjata', 'kekerasan fisik', 'berbahaya']\n",
    "            # if action_result in bahaya:\n",
    "            #     cv2.putText(frames_[i], action_result, (10, 30), FONTFACE,\n",
    "            #                 FONTSCALE, (255, 0, 0), THICKNESS, LINETYPE)\n",
    "            # else:\n",
    "            #     cv2.putText(frames_[i], action_result, (10, 30), FONTFACE,\n",
    "            #                 FONTSCALE, FONTCOLOR, THICKNESS, LINETYPE)\n",
    "\n",
    "    for i in range(na):\n",
    "        anno = annotations[i]\n",
    "        if anno is None:\n",
    "            continue\n",
    "        for j in range(nfpa):\n",
    "            ind = i * nfpa + j\n",
    "            frame = frames_[ind]\n",
    "\n",
    "            # add action result for whole video\n",
    "            # textsize = cv2.getTextSize(action_result, FONTFACE, FONTSCALE,\n",
    "            #                            THICKNESS)[0]\n",
    "            \n",
    "            # textwidth = textsize[0]\n",
    "            # location = (10, 14)\n",
    "            # diag0 = (location[0] + textwidth, location[1] - 14)\n",
    "            # diag1 = (location[0], location[1] + 22)\n",
    "            # cv2.rectangle(frame, diag0, diag1, (0, 119, 182), -1)\n",
    "            # bahaya = ['kekerasan bersenjata', 'kekerasan fisik', 'berbahaya']\n",
    "            # if action_result in bahaya:\n",
    "            #     cv2.putText(frame, action_result, (10, 30), FONTFACE, FONTSCALE,\n",
    "            #                 (255, 0, 0), THICKNESS, LINETYPE)\n",
    "            # else:\n",
    "            #     cv2.putText(frame, action_result, (10, 30), FONTFACE, FONTSCALE,\n",
    "            #                 FONTCOLOR, THICKNESS, LINETYPE)\n",
    "\n",
    "            # add spatio-temporal action detection results\n",
    "            for ann in anno:\n",
    "                box = ann[0]\n",
    "                label = ann[1]\n",
    "                if not len(label):\n",
    "                    continue\n",
    "                score = ann[2]\n",
    "                box = (box * scale_ratio).astype(np.int64)\n",
    "                st, ed = tuple(box[:2]), tuple(box[2:])\n",
    "                if not pose_data_samples:\n",
    "                    cv2.rectangle(frame, st, ed, plate[0], 2)\n",
    "\n",
    "                for k, lb in enumerate(label):\n",
    "                    if k >= max_num:\n",
    "                        break\n",
    "                    text = abbrev(lb)\n",
    "                    text = ': '.join([text, f'{(score[k]*100):.1f}%'])\n",
    "                    location = (0 + st[0], 18 + k * 18 + st[1])\n",
    "                    textsize = cv2.getTextSize(text, FONTFACE, FONTSCALE,\n",
    "                                               THICKNESS)[0]\n",
    "                    textwidth = textsize[0]\n",
    "                    diag0 = (location[0] + textwidth, location[1] - 14)\n",
    "                    diag1 = (location[0], location[1] + 2)\n",
    "                    cv2.rectangle(frame, diag0, diag1, plate[k + 1], -1)\n",
    "                    bahaya = ['melempar', 'membidik (l. panjang)', 'membidik (l. pendek)', 'memukul', 'menendang', 'menusuk']\n",
    "                    if lb in bahaya:\n",
    "                        cv2.putText(frame, text, location, FONTFACE, FONTSCALE,\n",
    "                                    (255, 0, 0), THICKNESS, LINETYPE)\n",
    "                        act_res = 'berbahaya'\n",
    "                    else:\n",
    "                        cv2.putText(frame, text, location, FONTFACE, FONTSCALE,\n",
    "                                    FONTCOLOR, THICKNESS, LINETYPE)\n",
    "\n",
    "                    textsize = cv2.getTextSize(act_res, FONTFACE, FONTSCALE,\n",
    "                                               THICKNESS)[0]\n",
    "                    textwidth = textsize[0]\n",
    "                    location = (10, 14)\n",
    "                    diag0 = (location[0] + textwidth, location[1] - 14)\n",
    "                    diag1 = (location[0], location[1] + 22)\n",
    "                    cv2.rectangle(frame, diag0, diag1, (0, 119, 182), -1)\n",
    "                    if act_res == 'berbahaya':\n",
    "                        cv2.putText(frame, act_res, (10, 30), FONTFACE, FONTSCALE,\n",
    "                                    (255, 0, 0), THICKNESS, LINETYPE)\n",
    "                    else:\n",
    "                        cv2.putText(frame, act_res, (10, 30), FONTFACE, FONTSCALE,\n",
    "                                    FONTCOLOR, THICKNESS, LINETYPE)\n",
    "                        \n",
    "\n",
    "    return frames_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df020acf-23a4-4537-b117-c802991bb870",
   "metadata": {},
   "outputs": [],
   "source": [
    "video = '../cut/DJI_0012_12r_10s_3.mp4'\n",
    "out_filename = 'data/DJI_0012_12r_10s_3_out.mp4'\n",
    "\n",
    "# rgb-based spatio temporal detection config\n",
    "rgb_stdet_config =  \"mmaction2/configs/detection/slowonly/slowonly_kinetics400-pretrained-r101_8xb16-8x8x1-20e_ava21-rgb.py\"\n",
    "rgb_stdet_checkpoint = \"https://download.openmmlab.com/mmaction/detection/ava/slowonly_omnisource_pretrained_r101_8x8x1_20e_ava_rgb/slowonly_omnisource_pretrained_r101_8x8x1_20e_ava_rgb_20201217-16378594.pth\"\n",
    "\n",
    "# human detection config\n",
    "det_config = 'mmaction2/demo/demo_configs/faster-rcnn_r50_fpn_2x_coco_infer.py'\n",
    "det_checkpoint = 'http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth'\n",
    "det_score_thr = 0.9\n",
    "#det_cat_id = 0\n",
    "\n",
    "# pose estimation config\n",
    "pose_config = 'mmaction2/demo/demo_configs/td-hm_hrnet-w32_8xb64-210e_coco-256x192_infer.py'\n",
    "pose_checkpoint = 'https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth'\n",
    "\n",
    "# action classification config\n",
    "skeleton_config = \"mmaction2/configs/skeleton/posec3d/ciis.py\"\n",
    "action_score_thr = 0.1\n",
    "# skeleton-based action recognition checkpoint\n",
    "skeleton_checkpoint = \"https://download.openmmlab.com/mmaction/skeleton/posec3d/posec3d_k400.pth\"\n",
    "# skeleton-based spatio temporal detection checkpoint\n",
    "skeleton_stdet_checkpoint = \"../../../Downloads/best_acc_top1_epoch_294.pth\"\n",
    "\n",
    "# use skeleton-based method\n",
    "use_skeleton_stdet = True\n",
    "use_skeleton_recog = True\n",
    "\n",
    "label_map_stdet = \"mmaction2/tools/data/ciis/label_map.txt\"\n",
    "label_map = \"mmaction2/tools/data/kinetics/label_map_k400.txt\"\n",
    "\n",
    "\n",
    "# rgb-based action recognition config\n",
    "rgb_config = \"configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x3-100e_kinetics400-rgb.py\"\n",
    "rgb_checkpoint = \"https://download.openmmlab.com/mmaction/recognition/tsn/tsn_r50_1x1x3_100e_kinetics400_rgb/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\"\n",
    "\n",
    "predict_stepsize = 4  # must even int, give out a spatio-temporal detection prediction per n frames\n",
    "output_stepsize = 1  # show one frame per n frames in the demo, we should have: predict_stepsize % output_stepsize == 0, speedUp/slowDown video output\n",
    "output_fps = 12  # the fps of demo video output, will speedUp/slowDown video output, must equal to (video_input_fps/output_stepsize) to get normal speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c43af11-3b1c-423e-95cf-d3dbb8c42493",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f38f52a-c759-4aa3-9081-c8535eca0007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_args():\n",
    "#     parser = argparse.ArgumentParser(description='MMAction2 demo')\n",
    "#     parser.add_argument(\n",
    "#         '--cfg-options',\n",
    "#         nargs='+',\n",
    "#         action=DictAction,\n",
    "#         default={},\n",
    "#         help='override some settings in the used config, the key-value pair '\n",
    "#         'in xxx=yyy format will be merged into config file. For example, '\n",
    "#         \"'--cfg-options model.backbone.depth=18 model.backbone.with_cp=True'\")\n",
    "#     args = parser.parse_args()\n",
    "#     return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54b10072-8152-490c-b05b-961873a676ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_label_map(file_path):\n",
    "    \"\"\"Load Label Map.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The file path of label map.\n",
    "\n",
    "    Returns:\n",
    "        dict: The label map (int -> label name).\n",
    "    \"\"\"\n",
    "    lines = open(file_path).readlines()\n",
    "    lines = [x.strip().split(': ') for x in lines]\n",
    "    return {int(x[0]): x[1] for x in lines}\n",
    "\n",
    "\n",
    "def abbrev(name):\n",
    "    \"\"\"Get the abbreviation of label name:\n",
    "\n",
    "    'take (an object) from (a person)' -> 'take ... from ...'\n",
    "    \"\"\"\n",
    "    while name.find('(') != -1:\n",
    "        st, ed = name.find('('), name.find(')')\n",
    "        name = name[:st] + '...' + name[ed + 1:]\n",
    "    return name\n",
    "\n",
    "\n",
    "def pack_result(human_detection, result, img_h, img_w):\n",
    "    \"\"\"Short summary.\n",
    "\n",
    "    Args:\n",
    "        human_detection (np.ndarray): Human detection result.\n",
    "        result (type): The predicted label of each human proposal.\n",
    "        img_h (int): The image height.\n",
    "        img_w (int): The image width.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple of human proposal, label name and label score.\n",
    "    \"\"\"\n",
    "    human_detection[:, 0::2] /= img_w\n",
    "    human_detection[:, 1::2] /= img_h\n",
    "    results = []\n",
    "    if result is None:\n",
    "        return None\n",
    "    for prop, res in zip(human_detection, result):\n",
    "        res.sort(key=lambda x: -x[1])\n",
    "        results.append(\n",
    "            (prop.data.cpu().numpy(), [x[0] for x in res], [x[1]\n",
    "                                                            for x in res]))\n",
    "    return results\n",
    "\n",
    "\n",
    "def expand_bbox(bbox, h, w, ratio=1.25):\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    center_x = (x1 + x2) // 2\n",
    "    center_y = (y1 + y2) // 2\n",
    "    width = x2 - x1\n",
    "    height = y2 - y1\n",
    "\n",
    "    square_l = max(width, height)\n",
    "    new_width = new_height = square_l * ratio\n",
    "\n",
    "    new_x1 = max(0, int(center_x - new_width / 2))\n",
    "    new_x2 = min(int(center_x + new_width / 2), w)\n",
    "    new_y1 = max(0, int(center_y - new_height / 2))\n",
    "    new_y2 = min(int(center_y + new_height / 2), h)\n",
    "    return (new_x1, new_y1, new_x2, new_y2)\n",
    "\n",
    "\n",
    "def cal_iou(box1, box2):\n",
    "    xmin1, ymin1, xmax1, ymax1 = box1\n",
    "    xmin2, ymin2, xmax2, ymax2 = box2\n",
    "\n",
    "    s1 = (xmax1 - xmin1) * (ymax1 - ymin1)\n",
    "    s2 = (xmax2 - xmin2) * (ymax2 - ymin2)\n",
    "\n",
    "    xmin = max(xmin1, xmin2)\n",
    "    ymin = max(ymin1, ymin2)\n",
    "    xmax = min(xmax1, xmax2)\n",
    "    ymax = min(ymax1, ymax2)\n",
    "\n",
    "    w = max(0, xmax - xmin)\n",
    "    h = max(0, ymax - ymin)\n",
    "    intersect = w * h\n",
    "    union = s1 + s2 - intersect\n",
    "    iou = intersect / union\n",
    "\n",
    "    return iou\n",
    "\n",
    "\n",
    "def skeleton_based_action_recognition(skeleton_config, skeleton_checkpoint, device, label_map, pose_results, h, w):\n",
    "    label_map = [x.strip() for x in open(label_map).readlines()]\n",
    "    num_class = len(label_map)\n",
    "\n",
    "    skeleton_config = mmengine.Config.fromfile(skeleton_config)\n",
    "    skeleton_config.model.cls_head.num_classes = num_class  # for K400 dataset\n",
    "\n",
    "    skeleton_model = init_recognizer(\n",
    "        skeleton_config, skeleton_checkpoint, device=device)\n",
    "    result = inference_skeleton(skeleton_model, pose_results, (h, w))\n",
    "    action_idx = result.pred_score.argmax().item()\n",
    "    return label_map[action_idx]\n",
    "\n",
    "\n",
    "def rgb_based_action_recognition(rgb_config, rgb_checkpoint, device, video, label_map):\n",
    "    rgb_config = mmengine.Config.fromfile(rgb_config)\n",
    "    rgb_config.model.backbone.pretrained = None\n",
    "    rgb_model = init_recognizer(rgb_config, rgb_checkpoint, device)\n",
    "    action_results = inference_recognizer(rgb_model, video)\n",
    "    rgb_action_result = action_results.pred_score.argmax().item()\n",
    "    label_map = [x.strip() for x in open(label_map).readlines()]\n",
    "    return label_map[rgb_action_result]\n",
    "\n",
    "def skeleton_based_stdet(predict_stepsize, skeleton_config, skeleton_stdet_checkpoint, device, action_score_thr, label_map, human_detections, pose_results,\n",
    "                         num_frame, clip_len, frame_interval, h, w):\n",
    "    window_size = clip_len * frame_interval\n",
    "    assert clip_len % 2 == 0, 'We would like to have an even clip_len'\n",
    "    timestamps = np.arange(window_size // 2, num_frame + 1 - window_size // 2,\n",
    "                           predict_stepsize)\n",
    "\n",
    "    skeleton_config = mmengine.Config.fromfile(skeleton_config)\n",
    "    num_class = max(label_map.keys()) + 1  # for AVA dataset (80 + 1), for CIIS dataset (9 + 1) == len(label_map)\n",
    "    skeleton_config.model.cls_head.num_classes = num_class\n",
    "    skeleton_stdet_model = init_recognizer(skeleton_config,\n",
    "                                           skeleton_stdet_checkpoint,\n",
    "                                           device)\n",
    "\n",
    "    skeleton_predictions = []\n",
    "\n",
    "    print('Performing SpatioTemporal Action Detection for each clip')\n",
    "    prog_bar = mmengine.ProgressBar(len(timestamps))\n",
    "    for timestamp in timestamps:\n",
    "        proposal = human_detections[timestamp - 1]\n",
    "        if proposal.shape[0] == 0:  # no people detected\n",
    "            skeleton_predictions.append(None)\n",
    "            continue\n",
    "\n",
    "        start_frame = timestamp - (clip_len // 2 - 1) * frame_interval\n",
    "        frame_inds = start_frame + np.arange(0, window_size, frame_interval)\n",
    "        frame_inds = list(frame_inds - 1)\n",
    "        num_frame = len(frame_inds)  # 30\n",
    "\n",
    "        pose_result = [pose_results[ind] for ind in frame_inds]\n",
    "\n",
    "        skeleton_prediction = []\n",
    "        for i in range(proposal.shape[0]):  # num_person\n",
    "            skeleton_prediction.append([])\n",
    "\n",
    "            fake_anno = dict(\n",
    "                frame_dict='',\n",
    "                label=-1,\n",
    "                img_shape=(h, w),\n",
    "                origin_shape=(h, w),\n",
    "                start_index=0,\n",
    "                modality='Pose',\n",
    "                num_clips=1,\n",
    "                clip_len=clip_len,\n",
    "                total_frames=num_frame)\n",
    "            num_person = 1\n",
    "\n",
    "            num_keypoint = 17\n",
    "            keypoint = np.zeros(\n",
    "                (num_person, num_frame, num_keypoint, 2))  # M T V 2\n",
    "            keypoint_score = np.zeros(\n",
    "                (num_person, num_frame, num_keypoint))  # M T V\n",
    "\n",
    "            # pose matching\n",
    "            person_bbox = proposal[i][:4]\n",
    "            area = expand_bbox(person_bbox, h, w)\n",
    "\n",
    "            for j, poses in enumerate(pose_result):  # num_frame\n",
    "                max_iou = float('-inf')\n",
    "                index = -1\n",
    "                if len(poses['keypoints']) == 0:\n",
    "                    continue\n",
    "                for k, bbox in enumerate(poses['bboxes']):\n",
    "                    iou = cal_iou(bbox, area)\n",
    "                    if max_iou < iou:\n",
    "                        index = k\n",
    "                        max_iou = iou\n",
    "                keypoint[0, j] = poses['keypoints'][index]\n",
    "                keypoint_score[0, j] = poses['keypoint_scores'][index]\n",
    "\n",
    "            fake_anno['keypoint'] = keypoint\n",
    "            fake_anno['keypoint_score'] = keypoint_score\n",
    "\n",
    "            output = inference_recognizer(skeleton_stdet_model, fake_anno)\n",
    "            # for multi-label recognition\n",
    "            score = output.pred_score.tolist()\n",
    "            for k in range(len(score)):  # 81\n",
    "                if k not in label_map:\n",
    "                    continue\n",
    "                if score[k] > action_score_thr:\n",
    "                    skeleton_prediction[i].append((label_map[k], score[k]))\n",
    "\n",
    "        skeleton_predictions.append(skeleton_prediction)\n",
    "        prog_bar.update()\n",
    "\n",
    "    return timestamps, skeleton_predictions\n",
    "\n",
    "\n",
    "def rgb_based_stdet(rgb_stdet_config, rgb_stdet_checkpoint, device, action_score_thr, predict_stepsize, frames, label_map, human_detections, w, h, new_w,\n",
    "                    new_h, w_ratio, h_ratio):\n",
    "\n",
    "    rgb_stdet_config = mmengine.Config.fromfile(rgb_stdet_config)\n",
    "    # rgb_stdet_config.merge_from_dict(args.cfg_options)\n",
    "\n",
    "    val_pipeline = rgb_stdet_config.val_pipeline\n",
    "    sampler = [x for x in val_pipeline if x['type'] == 'SampleAVAFrames'][0]\n",
    "    clip_len, frame_interval = sampler['clip_len'], sampler['frame_interval']\n",
    "    assert clip_len % 2 == 0, 'We would like to have an even clip_len'\n",
    "\n",
    "    window_size = clip_len * frame_interval\n",
    "    num_frame = len(frames)\n",
    "    # Note that it's 1 based here\n",
    "    timestamps = np.arange(window_size // 2, num_frame + 1 - window_size // 2,\n",
    "                           predict_stepsize)\n",
    "\n",
    "    # Get img_norm_cfg\n",
    "    img_norm_cfg = dict(\n",
    "        mean=np.array(rgb_stdet_config.model.data_preprocessor.mean),\n",
    "        std=np.array(rgb_stdet_config.model.data_preprocessor.std),\n",
    "        to_rgb=False)\n",
    "\n",
    "    # Build STDET model\n",
    "    try:\n",
    "        # In our spatiotemporal detection demo, different actions should have\n",
    "        # the same number of bboxes.\n",
    "        rgb_stdet_config['model']['test_cfg']['rcnn'] = dict(action_thr=0)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    rgb_stdet_config.model.backbone.pretrained = None\n",
    "    rgb_stdet_model = init_detector(\n",
    "        rgb_stdet_config, rgb_stdet_checkpoint, device=device)\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    print('Performing SpatioTemporal Action Detection for each clip')\n",
    "    prog_bar = mmengine.ProgressBar(len(timestamps))\n",
    "    # for timestamp, proposal in zip(timestamps, human_detections):\n",
    "    for timestamp in timestamps:\n",
    "        proposal = human_detections[timestamp - 1]\n",
    "        if proposal.shape[0] == 0:\n",
    "            predictions.append(None)\n",
    "            continue\n",
    "\n",
    "        start_frame = timestamp - (clip_len // 2 - 1) * frame_interval\n",
    "        frame_inds = start_frame + np.arange(0, window_size, frame_interval)\n",
    "        frame_inds = list(frame_inds - 1)\n",
    "\n",
    "        imgs = [frames[ind].astype(np.float32) for ind in frame_inds]\n",
    "        _ = [mmcv.imnormalize_(img, **img_norm_cfg) for img in imgs]\n",
    "        # THWC -> CTHW -> 1CTHW\n",
    "        input_array = np.stack(imgs).transpose((3, 0, 1, 2))[np.newaxis]\n",
    "        input_tensor = torch.from_numpy(input_array).to(device)\n",
    "\n",
    "        datasample = ActionDataSample()\n",
    "        datasample.proposals = InstanceData(bboxes=proposal)\n",
    "        datasample.set_metainfo(dict(img_shape=(new_h, new_w)))\n",
    "        with torch.no_grad():\n",
    "            result = rgb_stdet_model(\n",
    "                input_tensor, [datasample], mode='predict')\n",
    "            scores = result[0].pred_instances.scores\n",
    "            prediction = []\n",
    "            # N proposals\n",
    "            for i in range(proposal.shape[0]):\n",
    "                prediction.append([])\n",
    "            # Perform action score thr\n",
    "            for i in range(scores.shape[1]):\n",
    "                if i not in label_map:\n",
    "                    continue\n",
    "                for j in range(proposal.shape[0]):\n",
    "                    if scores[j, i] > action_score_thr:\n",
    "                        prediction[j].append((label_map[i], scores[j,\n",
    "                                                                   i].item()))\n",
    "            predictions.append(prediction)\n",
    "        prog_bar.update()\n",
    "\n",
    "    return timestamps, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31a3917c-9766-4939-988c-db068e5fcf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#args = parse_args()\n",
    "tmp_dir = tempfile.TemporaryDirectory()\n",
    "frame_paths, original_frames = frame_extract(\n",
    "    video, 720, out_dir=tmp_dir.name)\n",
    "num_frame = len(frame_paths)\n",
    "h, w, _ = original_frames[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "244ad1a3-d9a5-49c0-abe5-8c365a291b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by http backend from path: http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth\n",
      "Performing Human Detection for each frame\n",
      "[                                                  ] 0/122, elapsed: 0s, ETA:04/26 11:43:06 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
      "04/26 11:43:06 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 122/122, 4.0 task/s, elapsed: 31s, ETA:     0s\n",
      "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth\n",
      "Performing Human Pose Estimation for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 122/122, 13.0 task/s, elapsed: 9s, ETA:     0s\n"
     ]
    }
   ],
   "source": [
    "# Get Human detection results and pose results\n",
    "human_detections, _ = detection_inference(\n",
    "    det_config,\n",
    "    det_checkpoint,\n",
    "    frame_paths,\n",
    "    det_score_thr,\n",
    "    device=device)\n",
    "torch.cuda.empty_cache()\n",
    "pose_datasample = None\n",
    "if use_skeleton_recog or use_skeleton_stdet:\n",
    "    pose_results, pose_datasample = pose_inference(\n",
    "        pose_config,\n",
    "        pose_checkpoint,\n",
    "        frame_paths,\n",
    "        human_detections,\n",
    "        device=device)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "484e0e43-3f15-44bf-a0d7-fb4fdffec005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('human_detections.txt','w') as data:  \n",
    "#       data.write(str(human_detections))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9143043f-55f0-43e7-83ea-c5acb5a6451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('pose_results.txt','w') as data:  \n",
    "#       data.write(str(pose_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bca7943-9c6a-4883-b6ab-b222a8140d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('pose_datasample.txt','w') as data:  \n",
    "#       data.write(str(pose_datasample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e963a3-9141-47ba-9a91-d5436fcab54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(original_frames).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1efb83f-385b-4a75-8072-3f3295680b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize frames to shortside 256\n",
    "# new_w, new_h = mmcv.rescale_size((w, h), (256, np.Inf))\n",
    "new_w, new_h = w, h\n",
    "# frames = [mmcv.imresize(img, (new_w, new_h)) for img in original_frames]\n",
    "frames = original_frames\n",
    "w_ratio, h_ratio = new_w / w, new_h / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b26ea491-b1b3-4e33-bfe8-ac973c8a2933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spatio-temporal detection label_map\n",
    "stdet_label_map = load_label_map(label_map_stdet)\n",
    "rgb_stdet_config = mmengine.Config.fromfile(rgb_stdet_config)\n",
    "#rgb_stdet_config.merge_from_dict(args.cfg_options)\n",
    "try:\n",
    "    if rgb_stdet_config['data']['train']['custom_classes'] is not None:\n",
    "        stdet_label_map = {\n",
    "            id + 1: stdet_label_map[cls]\n",
    "            for id, cls in enumerate(rgb_stdet_config['data']['train']\n",
    "                                     ['custom_classes'])\n",
    "        }\n",
    "except KeyError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b12be23-7e0b-47cf-84be-da1eb28f3c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# action_result = None\n",
    "# if use_skeleton_recog:\n",
    "#     print('Use skeleton-based recognition')\n",
    "#     action_result = skeleton_based_action_recognition(\n",
    "#         skeleton_config, skeleton_checkpoint, device, label_map, pose_results, h, w)\n",
    "# else:\n",
    "#     print('Use rgb-based recognition')\n",
    "#     action_result = rgb_based_action_recognition(rgb_config, rgb_checkpoint, device, video, label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77ca5495-aca1-4d5a-8390-28beb907be26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use skeleton-based SpatioTemporal Action Detection\n",
      "Loads checkpoint by local backend from path: ../../../Downloads/best_acc_top1_epoch_294.pth\n",
      "Performing SpatioTemporal Action Detection for each clip\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 30/30, 51.7 task/s, elapsed: 1s, ETA:     0s"
     ]
    }
   ],
   "source": [
    "stdet_preds = None\n",
    "if use_skeleton_stdet:\n",
    "    print('Use skeleton-based SpatioTemporal Action Detection')\n",
    "    # clip_len, frame_interval = 30, 1\n",
    "    clip_len, frame_interval = predict_stepsize, 1\n",
    "    timestamps, stdet_preds = skeleton_based_stdet(predict_stepsize,\n",
    "                                                   skeleton_config,\n",
    "                                                   skeleton_stdet_checkpoint,\n",
    "                                                   device,\n",
    "                                                   action_score_thr,\n",
    "                                                   stdet_label_map,\n",
    "                                                   human_detections,\n",
    "                                                   pose_results, num_frame,\n",
    "                                                   clip_len,\n",
    "                                                   frame_interval, h, w)\n",
    "    for i in range(len(human_detections)):\n",
    "        det = human_detections[i]\n",
    "        det[:, 0:4:2] *= w_ratio\n",
    "        det[:, 1:4:2] *= h_ratio\n",
    "        human_detections[i] = torch.from_numpy(det[:, :4]).to(device)\n",
    "\n",
    "else:\n",
    "    print('Use rgb-based SpatioTemporal Action Detection')\n",
    "    for i in range(len(human_detections)):\n",
    "        det = human_detections[i]\n",
    "        det[:, 0:4:2] *= w_ratio\n",
    "        det[:, 1:4:2] *= h_ratio\n",
    "        human_detections[i] = torch.from_numpy(det[:, :4]).to(device)\n",
    "    timestamps, stdet_preds = rgb_based_stdet(rgb_stdet_config,\n",
    "                                              rgb_stdet_checkpoint,\n",
    "                                              device,\n",
    "                                              action_score_thr,\n",
    "                                              predict_stepsize,\n",
    "                                              frames,\n",
    "                                              stdet_label_map,\n",
    "                                              human_detections, w, h,\n",
    "                                              new_w, new_h, w_ratio,\n",
    "                                              h_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3f2de2-1bc0-4cca-b1e8-5a5ac74fa589",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdet_preds[0][0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dbf9264-8a9e-4ce1-bb45-1aec960e3498",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdet_results = []\n",
    "for timestamp, prediction in zip(timestamps, stdet_preds):\n",
    "    human_detection = human_detections[timestamp - 1]\n",
    "    stdet_results.append(\n",
    "        pack_result(human_detection, prediction, new_h, new_w))\n",
    "\n",
    "def dense_timestamps(timestamps, n):\n",
    "    \"\"\"Make it nx frames.\"\"\"\n",
    "    old_frame_interval = (timestamps[1] - timestamps[0])\n",
    "    start = timestamps[0] - old_frame_interval / n * (n - 1) / 2\n",
    "    new_frame_inds = np.arange(\n",
    "        len(timestamps) * n) * old_frame_interval / n + start\n",
    "    return new_frame_inds.astype(np.int64)\n",
    "\n",
    "dense_n = int(predict_stepsize / output_stepsize)\n",
    "# output_timestamps = dense_timestamps(timestamps, dense_n)\n",
    "output_timestamps = dense_timestamps(timestamps, dense_n) + 1\n",
    "frames = [\n",
    "    cv2.imread(frame_paths[timestamp - 1])\n",
    "    # cv2.imread(\"../854x480-white-solid-color-background.jpg\")\n",
    "    for timestamp in output_timestamps\n",
    "]\n",
    "\n",
    "if use_skeleton_recog or use_skeleton_stdet:\n",
    "    pose_datasample = [\n",
    "        pose_datasample[timestamp - 1] for timestamp in output_timestamps\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad889fa-c535-4c18-93fa-6f50eda6b618",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdet_results[20][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98eccae1-68c3-47dc-a6c4-a6636d0b0359",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop until found berbahaya\n",
    "action_result = 'tidak berbahaya'\n",
    "bahaya = ['melempar', 'membidik (l. panjang)', 'membidik (l. pendek)', 'memukul', 'menendang', 'menusuk']\n",
    "for prediction_step in stdet_results:\n",
    "    for person_prop in prediction_step:\n",
    "        for label in person_prop[1]:\n",
    "            if label in bahaya:\n",
    "                action_result = 'berbahaya'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad95eca-d528-4011-9527-6ea0ba4871ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output_timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e86754fd-de74-49f2-986b-19289918c520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video data/DJI_0012_12r_10s_3_out.mp4.\n",
      "Moviepy - Writing video data/DJI_0012_12r_10s_3_out.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready data/DJI_0012_12r_10s_3_out.mp4\n"
     ]
    }
   ],
   "source": [
    "vis_frames = visualize(pose_config, frames, stdet_results, pose_datasample,\n",
    "                       action_result)\n",
    "vid = mpy.ImageSequenceClip(vis_frames, fps=output_fps)\n",
    "vid.write_videofile(out_filename)\n",
    "tmp_dir.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8f350c-aca8-4289-8c4a-202220a0cf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdet_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb75af8-2dc9-4fd4-8932-884f33a0b1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_frames = visualize(pose_config, frames, [None], pose_datasample,\n",
    "                       None)\n",
    "vid = mpy.ImageSequenceClip(vis_frames, fps=output_fps)\n",
    "vid.write_videofile(\"dji_fly_20240216_153920_12_1708073394745_video_720p_30r_10s_2_skeleton_no-anno.mp4\")\n",
    "tmp_dir.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b95203-b9bf-4b0f-9c66-2d562b96fd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('<video width=50% controls autoplay loop><source src=\"data/DJI_0011_720p_30r_10s_1.MP4\"></video>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb9f4e8-314d-4de2-9a3e-4558146e1972",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "HTML('<video width=50% controls autoplay loop><source src=\"dji_fly_20240216_153920_12_1708073394745_video_720p_30r_10s_2_out_bold.mp4\"></video>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d448f0-ce74-4bb7-94ad-335c947c12e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stdet_results.txt','w') as data:  \n",
    "      data.write(str(stdet_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c95477b-8110-4ad9-a971-4ac8e6d7ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_extract(\n",
    "    \"data/DJI_0013_12r_10s_2_heatmap.MP4\", out_dir=\"../../skripsi/extracted/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319ae13b-b1c7-4b3b-adbe-fb2f8fc05fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dir.cleanup()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
