{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7df9318-3492-43a9-9f19-5a589a6e57f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a83e8aa-9403-42e3-ab17-07b34e1e49fa",
   "metadata": {},
   "source": [
    "## Testing recognition from bash - DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7bdfe89-b1e2-4238-940d-7586d302535a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by http backend from path: http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth\n",
      "Performing Human Detection for each frame\n",
      "[                                                  ] 0/319, elapsed: 0s, ETA:02/19 00:05:40 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
      "02/19 00:05:40 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 319/319, 3.9 task/s, elapsed: 82s, ETA:     0s\n",
      "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth\n",
      "Performing Human Pose Estimation for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 319/319, 9.9 task/s, elapsed: 32s, ETA:     0s\n",
      "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmaction/skeleton/posec3d/slowonly_r50_u48_240e_ntu60_xsub_keypoint/slowonly_r50_u48_240e_ntu60_xsub_keypoint-f3adabf1.pth\n",
      "Traceback (most recent call last):\n",
      "  File \"mmaction2/demo/demo_skeleton.py\", line 165, in <module>\n",
      "    main()\n",
      "  File \"mmaction2/demo/demo_skeleton.py\", line 153, in main\n",
      "    result = inference_skeleton(model, pose_results, (h, w))\n",
      "  File \"/home/aldy/miniconda3/envs/openmmlab/lib/python3.8/site-packages/mmaction/apis/inference.py\", line 168, in inference_skeleton\n",
      "    return inference_recognizer(model, fake_anno, test_pipeline)\n",
      "  File \"/home/aldy/miniconda3/envs/openmmlab/lib/python3.8/site-packages/mmaction/apis/inference.py\", line 110, in inference_recognizer\n",
      "    result = model.test_step(data)[0]\n",
      "  File \"/home/aldy/miniconda3/envs/openmmlab/lib/python3.8/site-packages/mmengine/model/base_model/base_model.py\", line 145, in test_step\n",
      "    return self._run_forward(data, mode='predict')  # type: ignore\n",
      "  File \"/home/aldy/miniconda3/envs/openmmlab/lib/python3.8/site-packages/mmengine/model/base_model/base_model.py\", line 346, in _run_forward\n",
      "    results = self(**data, mode=mode)\n",
      "  File \"/home/aldy/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/aldy/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/aldy/miniconda3/envs/openmmlab/lib/python3.8/site-packages/mmaction/models/recognizers/base.py\", line 260, in forward\n",
      "    return self.predict(inputs, data_samples, **kwargs)\n",
      "  File \"/home/aldy/miniconda3/envs/openmmlab/lib/python3.8/site-packages/mmaction/models/recognizers/base.py\", line 201, in predict\n",
      "    feats, predict_kwargs = self.extract_feat(inputs, test_mode=True)\n",
      "  File \"/home/aldy/miniconda3/envs/openmmlab/lib/python3.8/site-packages/mmaction/models/recognizers/recognizer3d.py\", line 92, in extract_feat\n",
      "    x = self.backbone(inputs)\n",
      "  File \"/home/aldy/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/aldy/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/aldy/miniconda3/envs/openmmlab/lib/python3.8/site-packages/mmaction/models/backbones/resnet3d.py\", line 873, in forward\n",
      "    x = self.conv1(x)\n",
      "  File \"/home/aldy/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/aldy/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/aldy/miniconda3/envs/openmmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py\", line 281, in forward\n",
      "    x = self.conv(x)\n",
      "  File \"/home/aldy/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/aldy/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/aldy/miniconda3/envs/openmmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/wrappers.py\", line 79, in forward\n",
      "    return super().forward(x)\n",
      "  File \"/home/aldy/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py\", line 610, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"/home/aldy/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py\", line 605, in _conv_forward\n",
      "    return F.conv3d(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 480.00 MiB. GPU 0 has a total capacty of 3.81 GiB of which 290.31 MiB is free. Process 5147 has 3.05 GiB memory in use. Including non-PyTorch memory, this process has 472.00 MiB memory in use. Of the allocated memory 383.94 MiB is allocated by PyTorch, and 6.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "!python mmaction2/demo/demo_skeleton.py data/uji_jalan.mp4 data/uji_jalan_out_demo.mp4 \\\n",
    "    --config mmaction2/configs/skeleton/posec3d/slowonly_r50_8xb16-u48-240e_ntu60-xsub-keypoint.py \\\n",
    "    --checkpoint https://download.openmmlab.com/mmaction/skeleton/posec3d/slowonly_r50_u48_240e_ntu60_xsub_keypoint/slowonly_r50_u48_240e_ntu60_xsub_keypoint-f3adabf1.pth \\\n",
    "    --det-config mmaction2/demo/demo_configs/faster-rcnn_r50_fpn_2x_coco_infer.py \\\n",
    "    --det-checkpoint http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth \\\n",
    "    --det-score-thr 0.9 \\\n",
    "    --det-cat-id 0 \\\n",
    "    --pose-config mmaction2/demo/demo_configs/td-hm_hrnet-w32_8xb64-210e_coco-256x192_infer.py \\\n",
    "    --pose-checkpoint https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth \\\n",
    "    --label-map mmaction2/tools/data/skeleton/label_map_ntu60.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "241d889f-adf2-443f-8922-ed99ed002070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=50% controls autoplay loop><source src=\"data/uji_jalan.mp4\"></video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<video width=50% controls autoplay loop><source src=\"data/uji_jalan.mp4\"></video>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fad1db3-acd5-4dd2-84e3-f8a7a67d5aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=50% controls autoplay loop><source src=\"data/uji_jalan_out.mp4\"></video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<video width=50% controls autoplay loop><source src=\"data/uji_jalan_out_demo.mp4\"></video>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6244f32-9406-4f93-b9c6-6cdc7557dccf",
   "metadata": {},
   "source": [
    "## Testing recognition from python - DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65aae07f-cdab-480b-b8ab-34882bf3dda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "import cv2\n",
    "import mmcv\n",
    "import mmengine\n",
    "import torch\n",
    "from mmengine.utils import track_iter_progress\n",
    "\n",
    "from mmaction.apis import (detection_inference, inference_skeleton,\n",
    "                           init_recognizer, pose_inference)\n",
    "from mmaction.registry import VISUALIZERS\n",
    "from mmaction.utils import frame_extract\n",
    "\n",
    "import moviepy.editor as mpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "860bdd57-2045-48c9-899e-a424e6eaf8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aldy/miniconda3/envs/openmmlab/lib/python3.8/site-packages/imageio_ffmpeg/binaries/ffmpeg-linux64-v4.2.2\n"
     ]
    }
   ],
   "source": [
    "import moviepy.config as cf\n",
    "print ( cf.get_setting(\"FFMPEG_BINARY\") ) # prints the current setting, make sure to use imageio_ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e0f48c9-a98a-4742-86d8-e4bfc65dbe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "FONTFACE = cv2.FONT_HERSHEY_DUPLEX\n",
    "FONTSCALE = 0.75\n",
    "FONTCOLOR = (255, 255, 255)  # BGR, white\n",
    "THICKNESS = 1\n",
    "LINETYPE = 1\n",
    "\n",
    "def visualize(pose_config, out_filename, frames, data_samples, action_label):\n",
    "    pose_config = mmengine.Config.fromfile(pose_config)\n",
    "    visualizer = VISUALIZERS.build(pose_config.visualizer)\n",
    "    visualizer.set_dataset_meta(data_samples[0].dataset_meta)\n",
    "\n",
    "    vis_frames = []\n",
    "    print('Drawing skeleton for each frame')\n",
    "    for d, f in track_iter_progress(list(zip(data_samples, frames))):\n",
    "        f = mmcv.imconvert(f, 'bgr', 'rgb')\n",
    "        visualizer.add_datasample(\n",
    "            'result',\n",
    "            f,\n",
    "            data_sample=d,\n",
    "            draw_gt=False,\n",
    "            draw_heatmap=False,\n",
    "            draw_bbox=True,\n",
    "            show=False,\n",
    "            wait_time=0,\n",
    "            out_file=None,\n",
    "            kpt_thr=0.3)\n",
    "        vis_frame = visualizer.get_image()\n",
    "        cv2.putText(vis_frame, action_label, (10, 30), FONTFACE, FONTSCALE,\n",
    "                    FONTCOLOR, THICKNESS, LINETYPE)\n",
    "        vis_frames.append(vis_frame)\n",
    "\n",
    "    vid = mpy.ImageSequenceClip(vis_frames, fps=24)\n",
    "    vid.write_videofile(out_filename, remove_temp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2361e83b-ea17-425f-be37-ff9737c1c480",
   "metadata": {},
   "outputs": [],
   "source": [
    "video = 'data/uji_jalan.mp4'\n",
    "out_filename = 'data/uji_jalan_out_demo.mp4'\n",
    "\n",
    "# Choose to use an action classification config\n",
    "config = 'mmaction2/configs/skeleton/posec3d/slowonly_r50_8xb16-u48-240e_ntu60-xsub-keypoint.py'\n",
    "# Setup a checkpoint file to load\n",
    "checkpoint = 'https://download.openmmlab.com/mmaction/skeleton/posec3d/slowonly_r50_u48_240e_ntu60_xsub_keypoint/slowonly_r50_u48_240e_ntu60_xsub_keypoint-f3adabf1.pth'\n",
    "\n",
    "# human detection config\n",
    "det_config = 'mmaction2/demo/demo_configs/faster-rcnn_r50_fpn_2x_coco_infer.py'\n",
    "det_checkpoint = 'http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth'\n",
    "det_score_thr = 0.9\n",
    "det_cat_id = 0\n",
    "\n",
    "# pose estimation config\n",
    "pose_config = 'mmaction2/demo/demo_configs/td-hm_hrnet-w32_8xb64-210e_coco-256x192_infer.py'\n",
    "pose_checkpoint = 'https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth'\n",
    "label_map = 'mmaction2/tools/data/skeleton/label_map_ntu60.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c4a8fb4-15d2-4877-9486-c4c365b47a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "short_side = 480"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3007172d-6374-483a-8902-6781e9a7119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dir = tempfile.TemporaryDirectory()\n",
    "frame_paths, frames = frame_extract(video, short_side,\n",
    "                                    tmp_dir.name)\n",
    "\n",
    "h, w, _ = frames[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ce84a3f-cec2-49bc-9295-e256de6ba1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by http backend from path: http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth\n",
      "Performing Human Detection for each frame\n",
      "[                                                  ] 0/319, elapsed: 0s, ETA:02/18 23:35:11 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
      "02/18 23:35:11 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 319/319, 4.0 task/s, elapsed: 80s, ETA:     0s\n"
     ]
    }
   ],
   "source": [
    "# Get Human detection results.\n",
    "det_results, _ = detection_inference(det_config, det_checkpoint,\n",
    "                                     frame_paths, det_score_thr,\n",
    "                                     det_cat_id, device)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad15062f-e2f6-420b-8254-14f21c1607b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth\n",
      "Performing Human Pose Estimation for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 319/319, 13.7 task/s, elapsed: 23s, ETA:     0s\n"
     ]
    }
   ],
   "source": [
    "# Get Pose estimation results.\n",
    "pose_results, pose_data_samples = pose_inference(pose_config,\n",
    "                                                 pose_checkpoint,\n",
    "                                                 frame_paths, det_results,\n",
    "                                                 device)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "733874cf-67e2-49fd-be59-b1ff44b4303b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmaction/skeleton/posec3d/slowonly_r50_u48_240e_ntu60_xsub_keypoint/slowonly_r50_u48_240e_ntu60_xsub_keypoint-f3adabf1.pth\n"
     ]
    }
   ],
   "source": [
    "# Initialize the recognizer\n",
    "config = mmengine.Config.fromfile(config)\n",
    "# build the model from a config file and a checkpoint file\n",
    "model = init_recognizer(config, checkpoint, device)\n",
    "\n",
    "# Get Action classification results.\n",
    "result = inference_skeleton(model, pose_results, (h, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5749f941-b6f4-4161-8314-9ce8e0705872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "staggering\n"
     ]
    }
   ],
   "source": [
    "# find the index of highest predicted score on result\n",
    "max_pred_index = result.pred_score.argmax().item()\n",
    "\n",
    "label_map = [x.strip() for x in open(label_map).readlines()]\n",
    "\n",
    "# set the highest predicted label as action_label\n",
    "action_label = label_map[max_pred_index]\n",
    "print(action_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02f1ad4f-bbad-4bdf-8132-7e77e18044cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drawing skeleton for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 319/319, 309.2 task/s, elapsed: 1s, ETA:     0s\n",
      "Moviepy - Building video data/uji_jalan_out.mp4.\n",
      "Moviepy - Writing video data/uji_jalan_out.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready data/uji_jalan_out.mp4\n"
     ]
    }
   ],
   "source": [
    "visualize(pose_config, out_filename, frames, pose_data_samples, action_label)\n",
    "\n",
    "tmp_dir.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffa4a081-a5a2-4a1f-9397-1348491673e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=50% controls autoplay loop><source src=\"data/uji_jalan.mp4\"></video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<video width=50% controls autoplay loop><source src=\"data/uji_jalan.mp4\"></video>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "628f1dd4-5ade-4d36-8885-735f9c41acb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=50% controls autoplay loop><source src=\"data/uji_jalan_out.mp4\"></video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<video width=50% controls autoplay loop><source src=\"data/uji_jalan_out_demo.mp4\"></video>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f232e4e6-c864-4e1b-b731-5572c316b614",
   "metadata": {},
   "source": [
    "## Testing recognition from bash - 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1f70ed-737b-4612-9591-3d99766ff2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python mmaction2/demo/demo_skeleton.py data/uji_jalan.mp4 data/uji_jalan_out_2019.mp4 \\\n",
    "    --config mmaction2/configs/skeleton/posec3d/slowonly_r50_u48_240e_ntu120_xsub_keypoint.py \\\n",
    "    --checkpoint work_dirs/slowonly_r50_u48_240e_ntu120_xsub_keypoint/best_top1_acc_epoch_90_8.pth \\\n",
    "    --det-config mmaction2/demo/demo_configs/faster-rcnn_r50_fpn_2x_coco_infer.py \\\n",
    "    --det-checkpoint http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth \\\n",
    "    --det-score-thr 0.9 \\\n",
    "    --det-cat-id 0 \\\n",
    "    --pose-config mmaction2/demo/demo_configs/td-hm_hrnet-w32_8xb64-210e_coco-256x192_infer.py \\\n",
    "    --pose-checkpoint https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth \\\n",
    "    --label-map mmaction2/tools/data/skeleton/label_5.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d38b7ee-309c-4c34-9156-1ab51ffdac00",
   "metadata": {},
   "source": [
    "## Testing recognition from python - 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0209fee4-5945-4519-9d69-bbda6c076184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(pose_config, out_filename, frames, data_samples, action_label):\n",
    "    pose_config = mmengine.Config.fromfile(pose_config)\n",
    "    visualizer = VISUALIZERS.build(pose_config.visualizer)\n",
    "    visualizer.set_dataset_meta(data_samples[0].dataset_meta)\n",
    "\n",
    "    vis_frames = []\n",
    "    print('Drawing skeleton for each frame')\n",
    "    for d, f in track_iter_progress(list(zip(data_samples, frames))):\n",
    "        f = mmcv.imconvert(f, 'bgr', 'rgb')\n",
    "        visualizer.add_datasample(\n",
    "            'result',\n",
    "            f,\n",
    "            data_sample=d,\n",
    "            draw_gt=False,\n",
    "            draw_heatmap=False,\n",
    "            draw_bbox=True,\n",
    "            show=False,\n",
    "            wait_time=0,\n",
    "            out_file=None,\n",
    "            kpt_thr=0.3)\n",
    "        vis_frame = visualizer.get_image()\n",
    "        cv2.putText(vis_frame, action_label, (10, 30), FONTFACE, FONTSCALE,\n",
    "                    FONTCOLOR, THICKNESS, LINETYPE)\n",
    "        vis_frames.append(vis_frame)\n",
    "\n",
    "    vid = mpy.ImageSequenceClip(vis_frames, fps=24)\n",
    "    vid.write_videofile(out_filename, remove_temp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a9c591-0fa9-4269-b82b-c34cff2b6470",
   "metadata": {},
   "outputs": [],
   "source": [
    "FONTFACE = cv2.FONT_HERSHEY_DUPLEX\n",
    "FONTSCALE = 0.75\n",
    "FONTCOLOR = (255, 255, 255)  # BGR, white\n",
    "THICKNESS = 1\n",
    "LINETYPE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161763e5-54b7-46d4-a7f6-c2c5bdceb362",
   "metadata": {},
   "outputs": [],
   "source": [
    "video = 'data/30m-50_duduk_drone_bd.mp4'\n",
    "out_filename = 'data/manipulasi6.mp4'\n",
    "\n",
    "# Choose to use an action classification config\n",
    "config = 'mmaction2/configs/skeleton/posec3d/slowonly_r50_8xb16-u48-240e_ntu60-xsub-keypoint.py'\n",
    "# Setup a checkpoint file to load\n",
    "checkpoint = 'https://download.openmmlab.com/mmaction/skeleton/posec3d/slowonly_r50_u48_240e_ntu60_xsub_keypoint/slowonly_r50_u48_240e_ntu60_xsub_keypoint-f3adabf1.pth'\n",
    "\n",
    "# human detection config\n",
    "det_config = 'mmaction2/demo/demo_configs/faster-rcnn_r50_fpn_2x_coco_infer.py'\n",
    "det_checkpoint = 'http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth'\n",
    "det_score_thr = 0.9\n",
    "det_cat_id = 0\n",
    "\n",
    "# pose estimation config\n",
    "pose_config = 'mmaction2/demo/demo_configs/td-hm_hrnet-w32_8xb64-210e_coco-256x192_infer.py'\n",
    "pose_checkpoint = 'https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth'\n",
    "label_map = 'mmaction2/tools/data/skeleton/label_map_ntu60.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692dbfa0-773c-4630-9266-4fdbb1614887",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "short_side = 480"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a88ba2-b35a-4b5c-b4da-edafcddc6789",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dir = tempfile.TemporaryDirectory()\n",
    "frame_paths, frames = frame_extract(video, short_side,\n",
    "                                    tmp_dir.name)\n",
    "\n",
    "h, w, _ = frames[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2300462-6796-4921-a04f-27c4da85f37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Human detection results.\n",
    "det_results, _ = detection_inference(det_config, det_checkpoint,\n",
    "                                     frame_paths, det_score_thr,\n",
    "                                     det_cat_id, device)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b40730-9de2-46b9-b547-6f1821009529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Pose estimation results.\n",
    "pose_results, pose_data_samples = pose_inference(pose_config,\n",
    "                                                 pose_checkpoint,\n",
    "                                                 frame_paths, det_results,\n",
    "                                                 device)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f2d05f-d66c-4ee8-90bb-aa22fb5afea4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5658e56b-b691-4576-ad68-da48b4f9deb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(pose_config, out_filename, frames, pose_data_samples, action_label)\n",
    "\n",
    "tmp_dir.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e67272-bce0-4b5f-bc13-eb7f27c31ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('<video width=50% controls autoplay loop><source src=\"data/manipulasi6.mp4\"></video>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ebca12-2f2e-4bcb-9e10-f23c26706fcb",
   "metadata": {},
   "source": [
    "## Testing recognition + detection from bash - DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d541ce2f-026e-4c1e-aedf-4b41180467c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python mmaction2/demo/demo_video_structuralize.py \\\n",
    "    --video mmaction2/demo/test_video_structuralize.mp4 \\\n",
    "    --out-filename mmaction2/demo/test_stdet_recognition_output.mp4 \\\n",
    "\\\n",
    "    --rgb-stdet-config mmaction2/configs/detection/slowonly/slowonly_kinetics400-pretrained-r101_8xb16-8x8x1-20e_ava21-rgb.py \\\n",
    "    --skeleton-stdet-checkpoint https://download.openmmlab.com/mmaction/skeleton/posec3d/posec3d_ava.pth \\\n",
    "\\\n",
    "    --det-config mmaction2/demo/demo_configs/faster-rcnn_r50_fpn_2x_coco_infer.py \\\n",
    "    --det-checkpoint http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth \\\n",
    "\\\n",
    "    --pose-config mmaction2/demo/demo_configs/td-hm_hrnet-w32_8xb64-210e_coco-256x192_infer.py \\\n",
    "    --pose-checkpoint https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth \\\n",
    "\\\n",
    "    --skeleton-config mmaction2/configs/skeleton/posec3d/slowonly_r50_8xb16-u48-240e_ntu60-xsub-keypoint.py \\\n",
    "    --skeleton-checkpoint https://download.openmmlab.com/mmaction/skeleton/posec3d/posec3d_k400.pth \\\n",
    "\\\n",
    "    --use-skeleton-stdet \\\n",
    "    --use-skeleton-recog \\\n",
    "\\\n",
    "    --label-map-stdet mmaction2/tools/data/ava/label_map.txt \\\n",
    "    --label-map mmaction2/tools/data/kinetics/label_map_k400.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad587218-846b-4b5b-b016-083302936b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('<video width=50% controls autoplay loop><source src=\"mmaction2/demo/test_video_structuralize.mp4\"></video>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f98e148-3cf2-46a8-a265-f8aa7c65649f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "HTML('<video width=50% controls autoplay loop><source src=\"mmaction2/demo/test_stdet_recognition_output.mp4\"></video>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a283883-e7c5-49c7-830a-9b3c0c73b1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python mmaction2/demo/demo_video_structuralize.py \\\n",
    "    --video mmaction2/demo/demo_skeleton.mp4 \\\n",
    "    --out-filename mmaction2/demo/demo_skeleton_vs_out.mp4 \\\n",
    "    --rgb-stdet-config mmaction2/configs/detection/slowonly/slowonly_kinetics400-pretrained-r101_8xb16-8x8x1-20e_ava21-rgb.py \\\n",
    "    --skeleton-stdet-checkpoint https://download.openmmlab.com/mmaction/skeleton/posec3d/posec3d_ava.pth \\\n",
    "    --det-config mmaction2/demo/demo_configs/faster-rcnn_r50_fpn_2x_coco_infer.py \\\n",
    "    --det-checkpoint http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth \\\n",
    "    --pose-config mmaction2/demo/demo_configs/td-hm_hrnet-w32_8xb64-210e_coco-256x192_infer.py \\\n",
    "    --pose-checkpoint https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth \\\n",
    "    --skeleton-config mmaction2/configs/skeleton/posec3d/slowonly_r50_8xb16-u48-240e_ntu60-xsub-keypoint.py \\\n",
    "    --skeleton-checkpoint https://download.openmmlab.com/mmaction/skeleton/posec3d/posec3d_k400.pth \\\n",
    "    --use-skeleton-stdet \\\n",
    "    --use-skeleton-recog \\\n",
    "    --label-map-stdet mmaction2/tools/data/ava/label_map.txt \\\n",
    "    --label-map mmaction2/tools/data/kinetics/label_map_k400.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9339752f-e382-4ca7-baa8-e3f69978157b",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('<video width=50% controls autoplay loop><source src=\"mmaction2/demo/demo_skeleton.mp4\"></video>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3318144-3cc6-40ff-8c40-0d3afca7b00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('<video width=50% controls autoplay loop><source src=\"mmaction2/demo/demo_skeleton_vs_out.mp4\"></video>')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
